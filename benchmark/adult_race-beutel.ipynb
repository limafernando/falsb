{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing file \n",
    "### where we evaluate BEUTEL's models using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "\n",
    "\n",
    "from util.load_data import load_data\n",
    "from util.evaluation import *\n",
    "from models.beutel.models import *\n",
    "from models.beutel.learning import train_loop as beutel_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIR_COEFFS = [1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_seeds = [13, 29, 42, 55, 73]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'adult-race'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, a = load_data(data_name)\n",
    "raw_data = (x, y, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdim = x.shape[1]\n",
    "ydim = y.shape[1]\n",
    "adim = a.shape[1]\n",
    "zdim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"model_name\", \"cv_seed\",\"fair_coeff\", \"clas_acc\", \"dp\", \"deqodds\", \"deqopp\", \"trade_dp\", \"trade_deqodds\", \"trade_deqopp\", \"TN_a0\", \"FP_a0\", \"FN_a0\", \"TP_a0\", \"TN_a1\", \"FP_a1\", \"FN_a1\", \"TP_a1\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing loop\n",
    "#### Each model is evalueted 5 times\n",
    "#### In the end of each iteration we save the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEUTEL for DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Model Loss | Class Loss | Adv Loss | Class Acc | Adv Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 19:45:03.379099: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-06-13 19:45:03.563742: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 1 | 0.7020904421806335 | 0.6811363697052002 | 0.7230445146560669 | 0.726085875331565 | 0.34963527851458887\n",
      "> 2 | 0.6890798211097717 | 0.6643774509429932 | 0.7137821912765503 | 0.7487151856763926 | 0.3324767904509284\n",
      "> 3 | 0.6754788160324097 | 0.6468339562416077 | 0.7041236162185669 | 0.7511190318302388 | 0.3376160477453581\n",
      "> 4 | 0.6607792377471924 | 0.6280919313430786 | 0.6934664845466614 | 0.7572115384615384 | 0.37421253315649866\n",
      "> 5 | 0.6446148157119751 | 0.607866108417511 | 0.6813634634017944 | 0.768691976127321 | 0.5169927055702918\n",
      "> 6 | 0.6267290115356445 | 0.586036205291748 | 0.6674218773841858 | 0.7442390583554377 | 0.6666114058355438\n",
      "> 7 | 0.6070429086685181 | 0.56280118227005 | 0.6512846946716309 | 0.725381299734748 | 0.7135692970822282\n",
      "> 8 | 0.5858393907546997 | 0.538948118686676 | 0.6327306032180786 | 0.7189572281167109 | 0.7277022546419099\n",
      "> 9 | 0.5640205144882202 | 0.5160726308822632 | 0.611968457698822 | 0.71684350132626 | 0.7332145225464191\n",
      "> 10 | 0.5431709289550781 | 0.4962981343269348 | 0.5900436639785767 | 0.7161803713527851 | 0.736032824933687\n",
      "> 11 | 0.5250800251960754 | 0.4813145697116852 | 0.5688454508781433 | 0.7156415782493368 | 0.7371518567639257\n",
      "> 12 | 0.5130524039268494 | 0.4736036956310272 | 0.5525011420249939 | 0.7150198938992043 | 0.737690649867374\n",
      "> 13 | 0.5058006048202515 | 0.4709228277206421 | 0.5406783819198608 | 0.7148541114058355 | 0.7380222148541115\n",
      "> 14 | 0.500912070274353 | 0.47059208154678345 | 0.5312321186065674 | 0.7147712201591512 | 0.7381051061007957\n",
      "> 15 | 0.4967465102672577 | 0.47093266248703003 | 0.5225603580474854 | 0.7148126657824934 | 0.738395225464191\n",
      "> 16 | 0.4930723011493683 | 0.47172731161117554 | 0.514417290687561 | 0.7149370026525199 | 0.7399287135278515\n",
      "> 17 | 0.4897586405277252 | 0.47285062074661255 | 0.5066666603088379 | 0.7143153183023873 | 0.7423740053050398\n",
      "> 18 | 0.48664969205856323 | 0.4741540849208832 | 0.4991452693939209 | 0.7129061671087533 | 0.7461041114058355\n",
      "> 19 | 0.48377302289009094 | 0.4756312072277069 | 0.491914838552475 | 0.7105023209549072 | 0.7504973474801061\n",
      "> 20 | 0.4810980558395386 | 0.4772367477416992 | 0.48495936393737793 | 0.7079741379310345 | 0.7555122679045093\n",
      "> 21 | 0.4785802364349365 | 0.4789127707481384 | 0.478247731924057 | 0.7056531830238727 | 0.7603199602122016\n",
      "> 22 | 0.4761752784252167 | 0.48062342405319214 | 0.4717271327972412 | 0.702420424403183 | 0.7652934350132626\n",
      "> 23 | 0.47395557165145874 | 0.48237431049346924 | 0.46553683280944824 | 0.6985659814323607 | 0.7714688328912467\n",
      "> 24 | 0.471891313791275 | 0.4841410517692566 | 0.45964157581329346 | 0.6958720159151194 | 0.7768153183023873\n",
      "> 25 | 0.4699421525001526 | 0.4858901798725128 | 0.45399412512779236 | 0.6928050397877984 | 0.7806283156498673\n",
      "> 26 | 0.46807703375816345 | 0.48759588599205017 | 0.44855818152427673 | 0.6898209549071618 | 0.7847728779840849\n",
      "> 27 | 0.4662870764732361 | 0.4892461895942688 | 0.44332799315452576 | 0.6866296419098143 | 0.7891246684350133\n",
      "> 28 | 0.4646241068840027 | 0.49085792899131775 | 0.43839025497436523 | 0.6838113395225465 | 0.7931863395225465\n",
      "> 29 | 0.46302810311317444 | 0.49241116642951965 | 0.4336450397968292 | 0.6806200265251989 | 0.7974552387267905\n",
      "> 30 | 0.46152591705322266 | 0.4939069449901581 | 0.4291449189186096 | 0.6778846153846154 | 0.8011024535809018\n",
      "> 31 | 0.46009454131126404 | 0.4953322410583496 | 0.42485684156417847 | 0.6752320954907162 | 0.8040865384615384\n",
      "> 32 | 0.4587368965148926 | 0.49671006202697754 | 0.42076376080513 | 0.672123673740053 | 0.8080238726790451\n",
      "> 33 | 0.45743924379348755 | 0.4980284869670868 | 0.4168500304222107 | 0.6698856100795756 | 0.8110079575596817\n",
      "> 34 | 0.4561919569969177 | 0.499287873506546 | 0.41309601068496704 | 0.6673988726790451 | 0.8143236074270557\n",
      "> 35 | 0.45497506856918335 | 0.5004804134368896 | 0.40946975350379944 | 0.665699602122016 | 0.8166031167108754\n",
      "> 36 | 0.4538501799106598 | 0.5016264915466309 | 0.4060738682746887 | 0.6642075596816976 | 0.8190069628647215\n",
      "> 37 | 0.45277756452560425 | 0.5027232766151428 | 0.4028318524360657 | 0.6625911803713528 | 0.8209549071618037\n",
      "> 38 | 0.4517934024333954 | 0.5038037896156311 | 0.39978301525115967 | 0.6613478116710876 | 0.8228614058355438\n",
      "> 39 | 0.45084697008132935 | 0.5048678517341614 | 0.3968261182308197 | 0.6600629973474801 | 0.8247264588859416\n",
      "> 40 | 0.4499543607234955 | 0.5058994889259338 | 0.39400923252105713 | 0.658612400530504 | 0.8270059681697612\n",
      "> 41 | 0.44905686378479004 | 0.5068663358688354 | 0.39124736189842224 | 0.6572861405835544 | 0.8285809018567639\n",
      "> 42 | 0.44821804761886597 | 0.5077976584434509 | 0.388638436794281 | 0.6557940981432361 | 0.830487400530504\n",
      "> 43 | 0.4473850131034851 | 0.5086904168128967 | 0.3860796391963959 | 0.6539704907161804 | 0.8327254641909815\n",
      "> 44 | 0.44659268856048584 | 0.5095478296279907 | 0.38363754749298096 | 0.6524784482758621 | 0.8347977453580901\n",
      "> 45 | 0.4458756148815155 | 0.5103937387466431 | 0.38135749101638794 | 0.6514837533156499 | 0.8359582228116711\n",
      "> 46 | 0.4451763331890106 | 0.5112069845199585 | 0.37914568185806274 | 0.6503647214854111 | 0.8372430371352785\n",
      "> 47 | 0.444500207901001 | 0.5120077729225159 | 0.3769926130771637 | 0.6491213527851459 | 0.8387350795755968\n",
      "> 48 | 0.4438430666923523 | 0.5127902030944824 | 0.37489593029022217 | 0.6477950928381963 | 0.8402271220159151\n",
      "> 49 | 0.44321420788764954 | 0.5135600566864014 | 0.3728683590888977 | 0.6468418435013262 | 0.8415119363395226\n",
      "> 50 | 0.4426232576370239 | 0.5143089294433594 | 0.37093761563301086 | 0.6457642572944297 | 0.8428381962864722\n",
      "> 51 | 0.4420526921749115 | 0.5150378942489624 | 0.3690674901008606 | 0.6453498010610079 | 0.843501326259947\n",
      "> 52 | 0.4414876699447632 | 0.5157496333122253 | 0.367225706577301 | 0.644645225464191 | 0.844454575596817\n",
      "> 53 | 0.44093191623687744 | 0.5164428949356079 | 0.3654209077358246 | 0.6434433023872679 | 0.8461538461538461\n",
      "> 54 | 0.4404183328151703 | 0.5171216726303101 | 0.3637149930000305 | 0.6426143899204244 | 0.8472314323607427\n",
      "> 55 | 0.4399310350418091 | 0.5178037881851196 | 0.36205825209617615 | 0.6420341511936339 | 0.8478945623342176\n",
      "> 56 | 0.4394514560699463 | 0.5184752941131592 | 0.3604276180267334 | 0.6411637931034483 | 0.8489307029177718\n",
      "> 57 | 0.43900975584983826 | 0.5191346406936646 | 0.35888487100601196 | 0.6405421087533156 | 0.8498010610079576\n",
      "> 58 | 0.4386144280433655 | 0.5197873115539551 | 0.3574415445327759 | 0.6395474137931034 | 0.8508786472148541\n",
      "> 59 | 0.438228040933609 | 0.5204252004623413 | 0.3560308814048767 | 0.6389257294429708 | 0.8514174403183024\n",
      "> 60 | 0.43783676624298096 | 0.52104651927948 | 0.35462698340415955 | 0.6382211538461539 | 0.8524535809018567\n",
      "> 61 | 0.43747347593307495 | 0.5216540098190308 | 0.35329297184944153 | 0.6373922413793104 | 0.8533653846153846\n",
      "> 62 | 0.4371281862258911 | 0.5222406387329102 | 0.35201576352119446 | 0.6368948938992043 | 0.8538627320954907\n",
      "> 63 | 0.4367644190788269 | 0.5228028297424316 | 0.35072600841522217 | 0.6361074270557029 | 0.8548159814323607\n",
      "> 64 | 0.4364336133003235 | 0.5233747959136963 | 0.3494924306869507 | 0.635402851458886 | 0.8556863395225465\n",
      "> 65 | 0.43612098693847656 | 0.5239447951316833 | 0.3482971787452698 | 0.6347811671087533 | 0.8567224801061007\n",
      "> 66 | 0.43582984805107117 | 0.5245047807693481 | 0.3471549153327942 | 0.6344910477453581 | 0.857261273209549\n",
      "> 67 | 0.43555137515068054 | 0.5250450372695923 | 0.3460577130317688 | 0.6337864721485411 | 0.8580487400530504\n",
      "> 68 | 0.43529289960861206 | 0.5255653262138367 | 0.34502047300338745 | 0.633289124668435 | 0.8589605437665783\n",
      "> 69 | 0.4350619316101074 | 0.5260799527168274 | 0.34404388070106506 | 0.6329990053050398 | 0.8594993368700266\n",
      "> 70 | 0.4347793459892273 | 0.5265733003616333 | 0.3429853916168213 | 0.6326259946949602 | 0.8598723474801061\n",
      "> 71 | 0.4344872236251831 | 0.5270573496818542 | 0.34191712737083435 | 0.6322115384615384 | 0.8603696949602122\n",
      "> 72 | 0.4342285692691803 | 0.5275454521179199 | 0.3409116864204407 | 0.631631299734748 | 0.861032824933687\n",
      "> 73 | 0.4339917004108429 | 0.5280170440673828 | 0.339966356754303 | 0.6311339522546419 | 0.8617788461538461\n",
      "> 74 | 0.4337675869464874 | 0.5284834504127502 | 0.3390517234802246 | 0.6308438328912467 | 0.8620689655172413\n",
      "> 75 | 0.43352848291397095 | 0.5289380550384521 | 0.33811888098716736 | 0.6302635941644562 | 0.8626492042440318\n",
      "> 76 | 0.43327951431274414 | 0.5293866991996765 | 0.33717232942581177 | 0.6298076923076923 | 0.8631879973474801\n",
      "> 77 | 0.4330557584762573 | 0.5298328399658203 | 0.3362787067890167 | 0.629268899204244 | 0.8638925729442971\n",
      "> 78 | 0.43285441398620605 | 0.530266523361206 | 0.33544230461120605 | 0.6288958885941645 | 0.864348474801061\n",
      "> 79 | 0.43265289068222046 | 0.5306975245475769 | 0.3346082270145416 | 0.6284814323607427 | 0.8648458222811671\n",
      "> 80 | 0.43248486518859863 | 0.5311239361763 | 0.3338458240032196 | 0.628315649867374 | 0.8652602785145889\n",
      "> 81 | 0.43230676651000977 | 0.5315439701080322 | 0.3330695331096649 | 0.6282327586206896 | 0.8655918435013262\n",
      "> 82 | 0.432129830121994 | 0.531948983669281 | 0.33231067657470703 | 0.6277768567639257 | 0.8663793103448276\n",
      "> 83 | 0.43193918466567993 | 0.5323330163955688 | 0.33154532313346863 | 0.6270722811671088 | 0.8670838859416445\n",
      "> 84 | 0.43177247047424316 | 0.5327154397964478 | 0.33082953095436096 | 0.626657824933687 | 0.867664124668435\n",
      "> 85 | 0.4316115379333496 | 0.5330932140350342 | 0.3301298916339874 | 0.6264091511936339 | 0.8679127984084881\n",
      "> 86 | 0.43145450949668884 | 0.5334621071815491 | 0.3294469118118286 | 0.6260775862068966 | 0.8684101458885941\n",
      "> 87 | 0.4313022494316101 | 0.5338200330734253 | 0.32878443598747253 | 0.6254559018567639 | 0.8690318302387268\n",
      "> 88 | 0.43112248182296753 | 0.5341736674308777 | 0.3280712962150574 | 0.6251657824933687 | 0.8694048408488063\n",
      "> 89 | 0.4309462308883667 | 0.5345178246498108 | 0.3273746073246002 | 0.6247098806366048 | 0.8698607427055703\n",
      "> 90 | 0.43084517121315 | 0.5348519682884216 | 0.3268383741378784 | 0.6241710875331565 | 0.8703995358090185\n",
      "> 91 | 0.43068838119506836 | 0.5351630449295044 | 0.3262137174606323 | 0.6239638594164456 | 0.8706896551724138\n",
      "> 92 | 0.430587500333786 | 0.5354807376861572 | 0.3256942629814148 | 0.6237566312997348 | 0.8711455570291777\n",
      "> 93 | 0.4304615557193756 | 0.5357979536056519 | 0.32512515783309937 | 0.6229691644562334 | 0.8720159151193634\n",
      "> 94 | 0.4303486943244934 | 0.5361056327819824 | 0.3245917558670044 | 0.6226790450928382 | 0.8723060344827587\n",
      "> 95 | 0.43020135164260864 | 0.5363951921463013 | 0.3240075409412384 | 0.6221816976127321 | 0.8728033819628647\n",
      "> 96 | 0.4300813674926758 | 0.5366897583007812 | 0.3234729766845703 | 0.6218501326259946 | 0.8731349469496021\n",
      "> 97 | 0.42995157837867737 | 0.5369800329208374 | 0.32292312383651733 | 0.6214771220159151 | 0.8735079575596817\n",
      "> 98 | 0.42981165647506714 | 0.5372523069381714 | 0.3223710060119629 | 0.6211455570291777 | 0.8738395225464191\n",
      "> 99 | 0.429715633392334 | 0.5375470519065857 | 0.32188424468040466 | 0.6208139920424404 | 0.8741710875331565\n",
      "> 100 | 0.42961567640304565 | 0.5378218293190002 | 0.32140952348709106 | 0.6206482095490716 | 0.8745026525198939\n",
      "> Evaluation\n",
      "> Class Acc = 0.6253324747085571\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.19201380014419556 | 0.14261434879153967 | 0.012728719040751457\n",
      "> Confusion Matrix \n",
      "TN: 2521.0 | FP: 2037.0 \n",
      "FN: 217.0 | TP: 1241.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1758.0 | FP: 0.0 \n",
      "FN: 201.0 | TP: 0.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 763.0 | FP: 2037.0 \n",
      "FN: 16.0 | TP: 1241.0\n"
     ]
    }
   ],
   "source": [
    "fairdef = 'DemPar'\n",
    "\n",
    "for cv_seed in cv_seeds:\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        x, y, a, test_size=0.3, random_state=cv_seed)\n",
    "\n",
    "    train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "    train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    for FAIR_COEFF in FAIR_COEFFS:\n",
    "\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "        model = Beutel(xdim, ydim, adim, zdim, FAIR_COEFF, fairdef)\n",
    "\n",
    "        ret = beutel_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "        Y, A, Y_hat, A_hat = fair_evaluation(model, test_data)\n",
    "        clas_acc, dp, deqodds, deqopp, confusion_matrix = compute_metrics(Y, A, Y_hat, A_hat, adim)\n",
    "\n",
    "        fair_metrics = (dp, deqodds, deqopp)\n",
    "        tradeoff = []\n",
    "        for fair_metric in fair_metrics:\n",
    "            tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "        result = ['BEUTEL4DP', cv_seed, FAIR_COEFF, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]]\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "        del(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEUTEL for Eq Opp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairdef = 'EqOpp'\n",
    "\n",
    "# for FAIR_COEFF in FAIR_COEFFS:\n",
    "#     for i in range(test_loop):\n",
    "\n",
    "#         opt = Adam(learning_rate=learning_rate)\n",
    "\n",
    "#         model = Beutel(xdim, ydim, adim, zdim, hidden_layer_specs, fairdef)\n",
    "\n",
    "#         ret = beutel_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "#         Y, A, Y_hat, A_hat = fair_evaluation(model, valid_data)\n",
    "#         clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1 = compute_metrics(Y, A, Y_hat, A_hat, adim)\n",
    "\n",
    "#         fair_metrics = (dp, deqodds, deqopp)\n",
    "#         tradeoff = []\n",
    "#         for fair_metric in fair_metrics:\n",
    "#             tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "#         result = ['BEUTEL4EqOpp', FAIR_COEFF, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "#         # results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving into DF then CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results, columns=header)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f'{data_name}-result/beutel-{epochs}.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41359ec383f887151a607ad1e28cb7dbc05f61385692c63e2bb2f343bf03f280"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('falsb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
