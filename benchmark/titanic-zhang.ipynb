{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing file \n",
    "### where we evaluate Zhang's models using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 15:44:00.054265: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "\n",
    "from util.load_data import load_data\n",
    "from util.evaluation import *\n",
    "from models.zhang.models import FairLogisticRegression\n",
    "from models.zhang.learning import train_loop as zhang_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_seeds = [13, 29, 42, 55, 73]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'titanic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, a = load_data(data_name)\n",
    "raw_data = (x, y, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdim = x.shape[1]\n",
    "ydim = y.shape[1]\n",
    "adim = a.shape[1]\n",
    "zdim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"model_name\", \"cv_seed\", \"clas_acc\", \"dp\", \"deqodds\", \"deqopp\", \"trade_dp\", \"trade_deqodds\", \"trade_deqopp\", \"TN_a0\", \"FP_a0\", \"FN_a0\", \"TP_a0\", \"TN_a1\", \"FP_a1\", \"FN_a1\", \"TP_a1\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing loop\n",
    "#### Each model is evalueted 5 times\n",
    "#### In the end of each iteration we save the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zhang for DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.537175178527832 | 0.8347809910774231 | 0.6528514588859416 | 0.3241462201591512\n",
      "> 2 | 0.48398250341415405 | 0.7067828178405762 | 0.8170590185676393 | 0.34801889920424406\n",
      "> 3 | 0.46705368161201477 | 0.6304430365562439 | 0.8250994694960212 | 0.6238395225464191\n",
      "> 4 | 0.45943623781204224 | 0.5849467515945435 | 0.83003149867374 | 0.7131548408488063\n",
      "> 5 | 0.4555836617946625 | 0.5569688081741333 | 0.8327669098143236 | 0.6960792440318302\n",
      "> 6 | 0.4535480737686157 | 0.5389748811721802 | 0.8340102785145889 | 0.6819877320954907\n",
      "> 7 | 0.45262983441352844 | 0.5268917083740234 | 0.8359167771883289 | 0.6783819628647215\n",
      "> 8 | 0.45246022939682007 | 0.5182324647903442 | 0.8369529177718833 | 0.6776359416445623\n",
      "> 9 | 0.4527934491634369 | 0.5118634700775146 | 0.8376989389920424 | 0.6775116047745358\n",
      "> 10 | 0.4534529745578766 | 0.5069818496704102 | 0.8389837533156499 | 0.6781747347480106\n",
      "> 11 | 0.45430347323417664 | 0.5031645894050598 | 0.8398541114058355 | 0.678506299734748\n",
      "> 12 | 0.45524275302886963 | 0.5000669956207275 | 0.8401027851458885 | 0.6794181034482759\n",
      "> 13 | 0.45620107650756836 | 0.49770477414131165 | 0.8411389257294429 | 0.6809930371352785\n",
      "> 14 | 0.45713135600090027 | 0.49578142166137695 | 0.8416777188328912 | 0.6816976127320955\n",
      "> 15 | 0.4580039978027344 | 0.49432581663131714 | 0.8421750663129973 | 0.6838113395225465\n",
      "> 16 | 0.4588020145893097 | 0.4931946396827698 | 0.842092175066313 | 0.6858007294429708\n",
      "> 17 | 0.4595186114311218 | 0.49233347177505493 | 0.8427553050397878 | 0.6888262599469496\n",
      "> 18 | 0.46015021204948425 | 0.49176114797592163 | 0.8434184350132626 | 0.6908985411140584\n",
      "> 19 | 0.4607006311416626 | 0.4913688898086548 | 0.8436671087533156 | 0.6937997347480106\n",
      "> 20 | 0.46117210388183594 | 0.49117669463157654 | 0.8439572281167109 | 0.6960792440318302\n",
      "> 21 | 0.4615706205368042 | 0.49105626344680786 | 0.844454575596817 | 0.6981929708222812\n",
      "> 22 | 0.46190202236175537 | 0.4911511540412903 | 0.8449104774535809 | 0.7001823607427056\n",
      "> 23 | 0.46217167377471924 | 0.4912158250808716 | 0.8453663793103449 | 0.7030421087533156\n",
      "> 24 | 0.46238601207733154 | 0.4913797974586487 | 0.84565649867374 | 0.7049900530503979\n",
      "> 25 | 0.4625500440597534 | 0.49159878492355347 | 0.8457808355437666 | 0.7062334217506632\n",
      "> 26 | 0.46267038583755493 | 0.4918217957019806 | 0.8459466180371353 | 0.7074767904509284\n",
      "> 27 | 0.4627510607242584 | 0.49209484457969666 | 0.8458222811671088 | 0.708554376657825\n",
      "> 28 | 0.4627971053123474 | 0.49231743812561035 | 0.846112400530504 | 0.7096319628647215\n",
      "> 29 | 0.4628128409385681 | 0.49258267879486084 | 0.8464439655172413 | 0.711165450928382\n",
      "> 30 | 0.4628022313117981 | 0.4928602874279022 | 0.8463610742705571 | 0.7119529177718833\n",
      "> 31 | 0.4627681374549866 | 0.4931058883666992 | 0.8464854111405835 | 0.7126989389920424\n",
      "> 32 | 0.4627138674259186 | 0.4933362305164337 | 0.8467340848806366 | 0.7127403846153846\n",
      "> 33 | 0.4626415967941284 | 0.493539035320282 | 0.8468998673740054 | 0.7122430371352785\n",
      "> 34 | 0.46255430579185486 | 0.4937790632247925 | 0.846816976127321 | 0.7129890583554377\n",
      "> 35 | 0.4624533951282501 | 0.4939979910850525 | 0.846816976127321 | 0.712574602122016\n",
      "> 36 | 0.4623405933380127 | 0.4941064715385437 | 0.8470242042440318 | 0.7122430371352785\n",
      "> 37 | 0.46221795678138733 | 0.4942944049835205 | 0.8470242042440318 | 0.7122844827586207\n",
      "> 38 | 0.46208637952804565 | 0.4944329261779785 | 0.8474386604774535 | 0.7123259283819628\n",
      "> 39 | 0.4619470238685608 | 0.49464157223701477 | 0.8475629973474801 | 0.7120772546419099\n",
      "> 40 | 0.4618014693260193 | 0.49476706981658936 | 0.8476873342175066 | 0.7119529177718833\n",
      "> 41 | 0.46164998412132263 | 0.4948759078979492 | 0.848018899204244 | 0.7122015915119363\n",
      "> 42 | 0.4614937901496887 | 0.49496376514434814 | 0.848018899204244 | 0.7125331564986738\n",
      "> 43 | 0.46133339405059814 | 0.4950706958770752 | 0.8481846816976127 | 0.7122844827586207\n",
      "> 44 | 0.46116986870765686 | 0.4951642155647278 | 0.8481846816976127 | 0.7124917108753316\n",
      "> 45 | 0.4610036313533783 | 0.4952874779701233 | 0.8484748010610079 | 0.7124088196286472\n",
      "> 46 | 0.46083521842956543 | 0.4955224394798279 | 0.8485162466843501 | 0.7126574933687002\n",
      "> 47 | 0.4606650173664093 | 0.49541175365448 | 0.848723474801061 | 0.7124088196286472\n",
      "> 48 | 0.460493803024292 | 0.49549779295921326 | 0.8488063660477454 | 0.7124088196286472\n",
      "> 49 | 0.46032193303108215 | 0.49554237723350525 | 0.8488892572944297 | 0.7122015915119363\n",
      "> 50 | 0.460149884223938 | 0.4956401288509369 | 0.8490550397877984 | 0.7119529177718833\n",
      "> 51 | 0.45997774600982666 | 0.4956591725349426 | 0.849179376657825 | 0.7119529177718833\n",
      "> 52 | 0.459805965423584 | 0.4956852197647095 | 0.8493451591511937 | 0.7118700265251989\n",
      "> 53 | 0.45963484048843384 | 0.49572718143463135 | 0.8494280503978779 | 0.7117456896551724\n",
      "> 54 | 0.459464430809021 | 0.49576419591903687 | 0.8495523872679045 | 0.7114970159151194\n",
      "> 55 | 0.45929497480392456 | 0.49577945470809937 | 0.8496352785145889 | 0.7112068965517241\n",
      "> 56 | 0.45912712812423706 | 0.4958117604255676 | 0.8497181697612732 | 0.7112483421750663\n",
      "> 57 | 0.4589608907699585 | 0.49584561586380005 | 0.8495938328912467 | 0.7110825596816976\n",
      "> 58 | 0.45879650115966797 | 0.4958791732788086 | 0.8497181697612732 | 0.7108753315649867\n",
      "> 59 | 0.4586339592933655 | 0.49586498737335205 | 0.8497181697612732 | 0.7108338859416445\n",
      "> 60 | 0.4584735333919525 | 0.49587902426719666 | 0.8497181697612732 | 0.710709549071618\n",
      "> 61 | 0.45831578969955444 | 0.495890736579895 | 0.8497596153846154 | 0.7109167771883289\n",
      "> 62 | 0.4581602215766907 | 0.4958895742893219 | 0.8497596153846154 | 0.7109582228116711\n",
      "> 63 | 0.4580075740814209 | 0.4958716928958893 | 0.8494694960212201 | 0.7109996684350133\n",
      "> 64 | 0.4578574001789093 | 0.4959380626678467 | 0.8494280503978779 | 0.7106681034482759\n",
      "> 65 | 0.45771005749702454 | 0.4958776533603668 | 0.8494280503978779 | 0.7106266578249337\n",
      "> 66 | 0.45756587386131287 | 0.49588027596473694 | 0.849676724137931 | 0.7102122015915119\n",
      "> 67 | 0.45742467045783997 | 0.4958834648132324 | 0.8498425066312998 | 0.7099635278514589\n",
      "> 68 | 0.4572865664958954 | 0.4958452582359314 | 0.8498839522546419 | 0.7097148541114059\n",
      "> 69 | 0.457151859998703 | 0.4958480894565582 | 0.8500911803713528 | 0.7093832891246684\n",
      "> 70 | 0.457020103931427 | 0.49580317735671997 | 0.8500082891246684 | 0.7092175066312998\n",
      "> 71 | 0.45689165592193604 | 0.4957960844039917 | 0.850132625994695 | 0.709051724137931\n",
      "> 72 | 0.4567665457725525 | 0.4957953691482544 | 0.8501740716180372 | 0.7088444960212201\n",
      "> 73 | 0.45664483308792114 | 0.4957694113254547 | 0.8502984084880637 | 0.7087201591511937\n",
      "> 74 | 0.45652681589126587 | 0.4957565665245056 | 0.8505470822281167 | 0.708347148541114\n",
      "> 75 | 0.4564122259616852 | 0.4957563579082489 | 0.8506299734748011 | 0.7082642572944297\n",
      "> 76 | 0.4563010632991791 | 0.4957370162010193 | 0.8507543103448276 | 0.7082642572944297\n",
      "> 77 | 0.45619356632232666 | 0.49575871229171753 | 0.8507957559681698 | 0.7082642572944297\n",
      "> 78 | 0.4560897946357727 | 0.49571114778518677 | 0.8510444297082228 | 0.708098474801061\n",
      "> 79 | 0.45598921179771423 | 0.49568599462509155 | 0.8509615384615384 | 0.7082642572944297\n",
      "> 80 | 0.45589208602905273 | 0.49563831090927124 | 0.8510444297082228 | 0.7081399204244032\n",
      "> 81 | 0.45579904317855835 | 0.4957752823829651 | 0.851085875331565 | 0.7079326923076923\n",
      "> 82 | 0.45570939779281616 | 0.495608925819397 | 0.8512516578249337 | 0.7077254641909815\n",
      "> 83 | 0.4556230902671814 | 0.49563032388687134 | 0.851334549071618 | 0.7076840185676393\n",
      "> 84 | 0.4555404782295227 | 0.4956161677837372 | 0.851334549071618 | 0.7076011273209549\n",
      "> 85 | 0.45546096563339233 | 0.495557576417923 | 0.8512516578249337 | 0.7076011273209549\n",
      "> 86 | 0.45538508892059326 | 0.49567127227783203 | 0.851334549071618 | 0.7076011273209549\n",
      "> 87 | 0.4553126096725464 | 0.4955253303050995 | 0.8514588859416445 | 0.7074767904509284\n",
      "> 88 | 0.45524364709854126 | 0.495488703250885 | 0.8515832228116711 | 0.7072281167108754\n",
      "> 89 | 0.45517784357070923 | 0.49549153447151184 | 0.8515417771883289 | 0.7072695623342176\n",
      "> 90 | 0.45511534810066223 | 0.49545952677726746 | 0.8516246684350133 | 0.7072281167108754\n",
      "> 91 | 0.45505616068840027 | 0.4954776465892792 | 0.8518318965517241 | 0.7071866710875332\n",
      "> 92 | 0.45500001311302185 | 0.4954504370689392 | 0.8518733421750663 | 0.7068965517241379\n",
      "> 93 | 0.454947292804718 | 0.49542099237442017 | 0.8519562334217506 | 0.7069379973474801\n",
      "> 94 | 0.45489752292633057 | 0.495413601398468 | 0.852039124668435 | 0.7066064323607427\n",
      "> 95 | 0.4548507332801819 | 0.49538999795913696 | 0.8519976790450928 | 0.7064820954907162\n",
      "> 96 | 0.4548068642616272 | 0.4953846335411072 | 0.8520805702917772 | 0.706440649867374\n",
      "> 97 | 0.45476582646369934 | 0.4953489303588867 | 0.8521634615384616 | 0.7061505305039788\n",
      "> 98 | 0.45472782850265503 | 0.49533534049987793 | 0.8520805702917772 | 0.7062334217506632\n",
      "> 99 | 0.4546924829483032 | 0.4953497350215912 | 0.8520805702917772 | 0.7066893236074271\n",
      "> 100 | 0.4546600878238678 | 0.49533918499946594 | 0.8522049071618037 | 0.7066064323607427\n",
      "> Evaluation\n",
      "> Class Acc = 0.8500664893617021\n",
      "> Adv Acc = 0.8500664893617021\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8116671591997147 | 0.9020376913249493 | 0.8809651136398315\n",
      "> Confusion Matrix \n",
      "TN: 4239.0 | FP: 319.0 \n",
      "FN: 583.0 | TP: 875.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1718.0 | FP: 40.0 \n",
      "FN: 101.0 | TP: 100.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2521.0 | FP: 279.0 \n",
      "FN: 482.0 | TP: 775.0\n"
     ]
    }
   ],
   "source": [
    "fairdef = 'DemPar'\n",
    "\n",
    "for cv_seed in cv_seeds:\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        x, y, a, test_size=0.3, random_state=cv_seed)\n",
    "\n",
    "    train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "    train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # train below\n",
    "\n",
    "    opt = Adam(learning_rate=lr)\n",
    "\n",
    "    model = FairLogisticRegression(xdim, ydim, adim, batch_size, fairdef)\n",
    "    zhang_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "    Y, A, Y_hat, A_hat = fair_evaluation(model, test_data)\n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1 = compute_metrics(Y, A, Y_hat, A_hat, adim)\n",
    "\n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    tradeoff = []\n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "    result = ['Zhang4DP', cv_seed, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    del(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zhang for Eq Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.5167679786682129 | 0.7780622243881226 | 0.6641661140583555 | 0.32418766578249336\n",
      "> 2 | 0.47247856855392456 | 0.6386902928352356 | 0.8193385278514589 | 0.4995440981432361\n",
      "> 3 | 0.46230050921440125 | 0.5677540302276611 | 0.8244777851458885 | 0.7143153183023873\n",
      "> 4 | 0.4566110372543335 | 0.5320210456848145 | 0.8289953580901857 | 0.7214439655172413\n",
      "> 5 | 0.45371437072753906 | 0.5124578475952148 | 0.8313992042440318 | 0.724676724137931\n",
      "> 6 | 0.4522345960140228 | 0.5009664297103882 | 0.8336372679045093 | 0.7225215517241379\n",
      "> 7 | 0.4516741931438446 | 0.4937453866004944 | 0.8351707559681698 | 0.7171750663129973\n",
      "> 8 | 0.451744019985199 | 0.4890083968639374 | 0.8365799071618037 | 0.7148955570291777\n",
      "> 9 | 0.4522421360015869 | 0.4856780171394348 | 0.8384449602122016 | 0.7140251989389921\n",
      "> 10 | 0.453009694814682 | 0.4833061099052429 | 0.8392324270557029 | 0.7140251989389921\n",
      "> 11 | 0.45392122864723206 | 0.4815048575401306 | 0.8396883289124668 | 0.7150613395225465\n",
      "> 12 | 0.45489075779914856 | 0.4801578223705292 | 0.8404757957559682 | 0.7163461538461539\n",
      "> 13 | 0.4558560252189636 | 0.479114830493927 | 0.8407659151193634 | 0.7173408488063661\n",
      "> 14 | 0.4567731022834778 | 0.47831833362579346 | 0.8408073607427056 | 0.7175066312997348\n",
      "> 15 | 0.457622766494751 | 0.47774335741996765 | 0.8414290450928382 | 0.71779675066313\n",
      "> 16 | 0.45839232206344604 | 0.4773045778274536 | 0.8420507294429708 | 0.7186256631299734\n",
      "> 17 | 0.4590773582458496 | 0.47709524631500244 | 0.8423822944297082 | 0.7181283156498673\n",
      "> 18 | 0.45967742800712585 | 0.4769580662250519 | 0.8427138594164456 | 0.7181697612732095\n",
      "> 19 | 0.46019768714904785 | 0.47686731815338135 | 0.8431697612732095 | 0.7175895225464191\n",
      "> 20 | 0.4606407582759857 | 0.47692355513572693 | 0.8437914456233422 | 0.7172579575596817\n",
      "> 21 | 0.4610133171081543 | 0.4770076870918274 | 0.8440815649867374 | 0.7169263925729443\n",
      "> 22 | 0.4613206386566162 | 0.47709667682647705 | 0.8443716843501327 | 0.7169263925729443\n",
      "> 23 | 0.46156811714172363 | 0.4772462546825409 | 0.8446618037135278 | 0.7168020557029178\n",
      "> 24 | 0.4617612957954407 | 0.4774180054664612 | 0.8449104774535809 | 0.7172994031830239\n",
      "> 25 | 0.46190619468688965 | 0.47753405570983887 | 0.8452834880636605 | 0.7171750663129973\n",
      "> 26 | 0.46200835704803467 | 0.4777033030986786 | 0.84565649867374 | 0.716387599469496\n",
      "> 27 | 0.4620724320411682 | 0.47789353132247925 | 0.8456150530503979 | 0.7166777188328912\n",
      "> 28 | 0.46210333704948425 | 0.4781217575073242 | 0.8458222811671088 | 0.7167606100795756\n",
      "> 29 | 0.46210402250289917 | 0.4783027172088623 | 0.8460295092838196 | 0.7170092838196287\n",
      "> 30 | 0.462079256772995 | 0.47849202156066895 | 0.8464439655172413 | 0.7169678381962865\n",
      "> 31 | 0.4620314836502075 | 0.47869330644607544 | 0.8465268567639257 | 0.7167606100795756\n",
      "> 32 | 0.4619640111923218 | 0.47891348600387573 | 0.8466511936339522 | 0.7168020557029178\n",
      "> 33 | 0.46187907457351685 | 0.4790433943271637 | 0.8471070954907162 | 0.7166777188328912\n",
      "> 34 | 0.4617796838283539 | 0.4792656898498535 | 0.8472314323607427 | 0.7166777188328912\n",
      "> 35 | 0.461667001247406 | 0.4793623685836792 | 0.8471899867374005 | 0.7165119363395226\n",
      "> 36 | 0.46154266595840454 | 0.47958827018737793 | 0.8473143236074271 | 0.7164704907161804\n",
      "> 37 | 0.46140843629837036 | 0.47975385189056396 | 0.8475215517241379 | 0.7163461538461539\n",
      "> 38 | 0.46126583218574524 | 0.4798930287361145 | 0.8476458885941645 | 0.7162218169761273\n",
      "> 39 | 0.46111589670181274 | 0.48000773787498474 | 0.8476873342175066 | 0.7160560344827587\n",
      "> 40 | 0.46095961332321167 | 0.4802587330341339 | 0.8479774535809018 | 0.7158902519893899\n",
      "> 41 | 0.460798442363739 | 0.48038339614868164 | 0.8482675729442971 | 0.7155586870026526\n",
      "> 42 | 0.46063297986984253 | 0.48045867681503296 | 0.8482261273209549 | 0.7152685676392573\n",
      "> 43 | 0.4604637622833252 | 0.4806213974952698 | 0.8485576923076923 | 0.7149784482758621\n",
      "> 44 | 0.46029210090637207 | 0.4807785749435425 | 0.8485576923076923 | 0.7145225464190982\n",
      "> 45 | 0.4601174592971802 | 0.48103389143943787 | 0.8486405835543767 | 0.7146468832891246\n",
      "> 46 | 0.45994123816490173 | 0.4809446334838867 | 0.8486405835543767 | 0.714481100795756\n",
      "> 47 | 0.4597635269165039 | 0.4811024069786072 | 0.8488063660477454 | 0.7141495358090185\n",
      "> 48 | 0.459585577249527 | 0.4811633825302124 | 0.8490550397877984 | 0.7141495358090185\n",
      "> 49 | 0.45940712094306946 | 0.4812825322151184 | 0.8492208222811671 | 0.7139837533156499\n",
      "> 50 | 0.45922887325286865 | 0.4814489781856537 | 0.8493866047745358 | 0.7136936339522546\n",
      "> 51 | 0.45905038714408875 | 0.48149389028549194 | 0.8495938328912467 | 0.713320623342175\n",
      "> 52 | 0.4588736891746521 | 0.4816499948501587 | 0.8495938328912467 | 0.7132791777188329\n",
      "> 53 | 0.4586973190307617 | 0.4817275404930115 | 0.8496352785145889 | 0.7131962864721485\n",
      "> 54 | 0.4585224390029907 | 0.48176729679107666 | 0.8495938328912467 | 0.7130719496021221\n",
      "> 55 | 0.4583488702774048 | 0.4818364381790161 | 0.8495523872679045 | 0.7130305039787799\n",
      "> 56 | 0.4581769108772278 | 0.4819489121437073 | 0.8498010610079576 | 0.712823275862069\n",
      "> 57 | 0.4580070972442627 | 0.4819607734680176 | 0.8498425066312998 | 0.7129061671087533\n",
      "> 58 | 0.4578394591808319 | 0.48208433389663696 | 0.8498010610079576 | 0.7129476127320955\n",
      "> 59 | 0.45767420530319214 | 0.4821600914001465 | 0.8498010610079576 | 0.7129476127320955\n",
      "> 60 | 0.457511305809021 | 0.482164591550827 | 0.8497181697612732 | 0.712823275862069\n",
      "> 61 | 0.45735108852386475 | 0.48223066329956055 | 0.8495523872679045 | 0.7129476127320955\n",
      "> 62 | 0.45719367265701294 | 0.4823034703731537 | 0.8498010610079576 | 0.712823275862069\n",
      "> 63 | 0.45703956484794617 | 0.48243552446365356 | 0.8497181697612732 | 0.7126989389920424\n",
      "> 64 | 0.4568882882595062 | 0.4824813902378082 | 0.8497181697612732 | 0.712574602122016\n",
      "> 65 | 0.4567398726940155 | 0.4824784994125366 | 0.8495938328912467 | 0.712574602122016\n",
      "> 66 | 0.45659494400024414 | 0.4825951159000397 | 0.8497596153846154 | 0.7126160477453581\n",
      "> 67 | 0.4564533233642578 | 0.4825808107852936 | 0.8497181697612732 | 0.7124502652519894\n",
      "> 68 | 0.45631498098373413 | 0.48263049125671387 | 0.8497181697612732 | 0.7122015915119363\n",
      "> 69 | 0.4561801552772522 | 0.4827616810798645 | 0.849676724137931 | 0.7124502652519894\n",
      "> 70 | 0.45604872703552246 | 0.4826939105987549 | 0.8499668435013262 | 0.7122844827586207\n",
      "> 71 | 0.4559204578399658 | 0.4828091859817505 | 0.8500911803713528 | 0.712118700265252\n",
      "> 72 | 0.4557962119579315 | 0.4827546775341034 | 0.8503398541114059 | 0.7117871352785146\n",
      "> 73 | 0.45567524433135986 | 0.4828685522079468 | 0.8505885278514589 | 0.7114970159151194\n",
      "> 74 | 0.4555584192276001 | 0.4830411374568939 | 0.8505470822281167 | 0.7112897877984085\n",
      "> 75 | 0.45544514060020447 | 0.48291751742362976 | 0.8506714190981433 | 0.7111240053050398\n",
      "> 76 | 0.45533543825149536 | 0.4829922616481781 | 0.8507128647214854 | 0.7112068965517241\n",
      "> 77 | 0.45522913336753845 | 0.48299044370651245 | 0.8508372015915119 | 0.711165450928382\n",
      "> 78 | 0.45512670278549194 | 0.48300689458847046 | 0.8510029840848806 | 0.7109582228116711\n",
      "> 79 | 0.4550284147262573 | 0.4830895662307739 | 0.851085875331565 | 0.710709549071618\n",
      "> 80 | 0.45493370294570923 | 0.48307904601097107 | 0.8510029840848806 | 0.7105852122015915\n",
      "> 81 | 0.45484232902526855 | 0.48307886719703674 | 0.8510444297082228 | 0.7101707559681698\n",
      "> 82 | 0.4547543525695801 | 0.4830963611602783 | 0.8512102122015915 | 0.7100878647214854\n",
      "> 83 | 0.45467060804367065 | 0.4831884503364563 | 0.8513759946949602 | 0.7098806366047745\n",
      "> 84 | 0.45458984375 | 0.4831981360912323 | 0.8514174403183024 | 0.7093418435013262\n",
      "> 85 | 0.4545127749443054 | 0.48312947154045105 | 0.8516661140583555 | 0.7091760610079576\n",
      "> 86 | 0.45443958044052124 | 0.48315805196762085 | 0.8515832228116711 | 0.7091760610079576\n",
      "> 87 | 0.4543696939945221 | 0.4832006096839905 | 0.8517490053050398 | 0.7092175066312998\n",
      "> 88 | 0.45430296659469604 | 0.4832690358161926 | 0.8517490053050398 | 0.709300397877984\n",
      "> 89 | 0.454240083694458 | 0.4832887649536133 | 0.8516661140583555 | 0.7093832891246684\n",
      "> 90 | 0.45417991280555725 | 0.4832586646080017 | 0.8517075596816976 | 0.7094661803713528\n",
      "> 91 | 0.45412325859069824 | 0.48330867290496826 | 0.8517075596816976 | 0.7093418435013262\n",
      "> 92 | 0.4540698528289795 | 0.4832722246646881 | 0.8518318965517241 | 0.7089688328912467\n",
      "> 93 | 0.4540194869041443 | 0.48329535126686096 | 0.8518318965517241 | 0.7088859416445623\n",
      "> 94 | 0.45397257804870605 | 0.48348018527030945 | 0.851790450928382 | 0.7087616047745358\n",
      "> 95 | 0.45392757654190063 | 0.48335960507392883 | 0.8519147877984085 | 0.7085129310344828\n",
      "> 96 | 0.4538860619068146 | 0.4833686947822571 | 0.852039124668435 | 0.7084714854111406\n",
      "> 97 | 0.4538477659225464 | 0.48339182138442993 | 0.852039124668435 | 0.7082642572944297\n",
      "> 98 | 0.4538120627403259 | 0.4834344983100891 | 0.8519976790450928 | 0.7082642572944297\n",
      "> 99 | 0.453779399394989 | 0.4834620952606201 | 0.8519976790450928 | 0.7081399204244032\n",
      "> 100 | 0.45374903082847595 | 0.4834774136543274 | 0.8519976790450928 | 0.7080155835543767\n",
      "> Evaluation\n",
      "> Class Acc = 0.8503989361702128\n",
      "> Adv Acc = 0.8503989361702128\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8121601194143295 | 0.9023948349058628 | 0.8809651136398315\n",
      "> Confusion Matrix \n",
      "TN: 4241.0 | FP: 317.0 \n",
      "FN: 583.0 | TP: 875.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1718.0 | FP: 40.0 \n",
      "FN: 101.0 | TP: 100.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2523.0 | FP: 277.0 \n",
      "FN: 482.0 | TP: 775.0\n"
     ]
    }
   ],
   "source": [
    "fairdef = 'EqOdds'\n",
    "\n",
    "for cv_seed in cv_seeds:\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        x, y, a, test_size=0.3, random_state=cv_seed)\n",
    "\n",
    "    train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "    train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # train below\n",
    "\n",
    "    opt = Adam(learning_rate=lr)\n",
    "\n",
    "    model = FairLogisticRegression(xdim, ydim, adim, batch_size, fairdef)\n",
    "    zhang_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "    Y, A, Y_hat, A_hat = fair_evaluation(model, test_data)\n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1 = compute_metrics(Y, A, Y_hat, A_hat, adim)\n",
    "\n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    tradeoff = []\n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "    result = ['Zhang4EqOdds', cv_seed, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    del(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zhang for Eq Opp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.5352369546890259 | 0.3454836308956146 | 0.6534316976127321 | 0.3241462201591512\n",
      "> 2 | 0.4807005226612091 | 0.2549954652786255 | 0.8173905835543767 | 0.3957228116710875\n",
      "> 3 | 0.4652402698993683 | 0.21041643619537354 | 0.8256382625994695 | 0.5182775198938993\n",
      "> 4 | 0.45960766077041626 | 0.18977224826812744 | 0.8291196949602122 | 0.5750165782493368\n",
      "> 5 | 0.4571208953857422 | 0.17857737839221954 | 0.8316478779840849 | 0.6185759283819628\n",
      "> 6 | 0.45613378286361694 | 0.1713542342185974 | 0.834051724137931 | 0.6511521883289124\n",
      "> 7 | 0.45593687891960144 | 0.16730940341949463 | 0.8348806366047745 | 0.6744446286472149\n",
      "> 8 | 0.45616504549980164 | 0.16622084379196167 | 0.8359167771883289 | 0.6803713527851459\n",
      "> 9 | 0.45682814717292786 | 0.16606619954109192 | 0.8370358090185677 | 0.6821120689655172\n",
      "> 10 | 0.457761287689209 | 0.1664421409368515 | 0.8385692970822282 | 0.6828580901856764\n",
      "> 11 | 0.4588456451892853 | 0.16705381870269775 | 0.8392738726790451 | 0.6823192970822282\n",
      "> 12 | 0.45998847484588623 | 0.1677386462688446 | 0.8403100132625995 | 0.6809101458885941\n",
      "> 13 | 0.46112555265426636 | 0.16844260692596436 | 0.8409316976127321 | 0.682070623342175\n",
      "> 14 | 0.46221181750297546 | 0.16914495825767517 | 0.8409316976127321 | 0.6815318302387268\n",
      "> 15 | 0.4632232189178467 | 0.1697690784931183 | 0.8417191644562334 | 0.681117374005305\n",
      "> 16 | 0.46414369344711304 | 0.17033636569976807 | 0.8424237400530504 | 0.6804542440318302\n",
      "> 17 | 0.46496695280075073 | 0.17083516716957092 | 0.8427553050397878 | 0.6800397877984085\n",
      "> 18 | 0.4656928777694702 | 0.1712760031223297 | 0.84279675066313 | 0.6795009946949602\n",
      "> 19 | 0.46632471680641174 | 0.17162728309631348 | 0.8431283156498673 | 0.6785891909814323\n",
      "> 20 | 0.4668665826320648 | 0.1719730794429779 | 0.843501326259947 | 0.6786306366047745\n",
      "> 21 | 0.4673260450363159 | 0.1721985638141632 | 0.8439157824933687 | 0.6787964190981433\n",
      "> 22 | 0.4677084684371948 | 0.17242169380187988 | 0.8442473474801061 | 0.6785477453580901\n",
      "> 23 | 0.46802136301994324 | 0.17258980870246887 | 0.8443716843501327 | 0.6786306366047745\n",
      "> 24 | 0.46827036142349243 | 0.17276698350906372 | 0.8449519230769231 | 0.6781332891246684\n",
      "> 25 | 0.4684634506702423 | 0.1728881597518921 | 0.845407824933687 | 0.6787135278514589\n",
      "> 26 | 0.46860644221305847 | 0.17293091118335724 | 0.8454492705570292 | 0.679459549071618\n",
      "> 27 | 0.4687047600746155 | 0.173019140958786 | 0.8456979442970822 | 0.6794181034482759\n",
      "> 28 | 0.4687635600566864 | 0.17306040227413177 | 0.8457808355437666 | 0.6790450928381963\n",
      "> 29 | 0.46878913044929504 | 0.17307260632514954 | 0.846112400530504 | 0.6793352122015915\n",
      "> 30 | 0.4687850773334503 | 0.17303037643432617 | 0.8462367374005305 | 0.679459549071618\n",
      "> 31 | 0.4687550663948059 | 0.17301100492477417 | 0.8465268567639257 | 0.6800812334217506\n",
      "> 32 | 0.4687025249004364 | 0.17297784984111786 | 0.8466097480106101 | 0.6802884615384616\n",
      "> 33 | 0.4686307907104492 | 0.17293596267700195 | 0.8466097480106101 | 0.6803299071618037\n",
      "> 34 | 0.4685423672199249 | 0.17287734150886536 | 0.8466926392572944 | 0.680868700265252\n",
      "> 35 | 0.4684399366378784 | 0.1728278398513794 | 0.8468998673740054 | 0.6810759283819628\n",
      "> 36 | 0.46832501888275146 | 0.17274796962738037 | 0.8471485411140584 | 0.6817390583554377\n",
      "> 37 | 0.4681999981403351 | 0.17266380786895752 | 0.8471899867374005 | 0.6819462864721485\n",
      "> 38 | 0.4680659770965576 | 0.17252710461616516 | 0.847065649867374 | 0.6821120689655172\n",
      "> 39 | 0.4679245948791504 | 0.17243695259094238 | 0.8473143236074271 | 0.6824436339522546\n",
      "> 40 | 0.46777695417404175 | 0.17231163382530212 | 0.8474801061007957 | 0.6826923076923077\n",
      "> 41 | 0.4676244258880615 | 0.17222681641578674 | 0.847770225464191 | 0.6828166445623343\n",
      "> 42 | 0.46746763586997986 | 0.1720949411392212 | 0.8476873342175066 | 0.6829824270557029\n",
      "> 43 | 0.467307448387146 | 0.17195096611976624 | 0.847770225464191 | 0.6832725464190982\n",
      "> 44 | 0.46714502573013306 | 0.17184710502624512 | 0.8478116710875332 | 0.6831896551724138\n",
      "> 45 | 0.46698033809661865 | 0.17169064283370972 | 0.8481432360742706 | 0.683231100795756\n",
      "> 46 | 0.46681472659111023 | 0.17155992984771729 | 0.8482261273209549 | 0.6833139920424404\n",
      "> 47 | 0.4666479825973511 | 0.1714438647031784 | 0.8483504641909815 | 0.6829824270557029\n",
      "> 48 | 0.46648067235946655 | 0.17128592729568481 | 0.8485576923076923 | 0.6831067639257294\n",
      "> 49 | 0.4663136601448059 | 0.17111563682556152 | 0.8486405835543767 | 0.6829409814323607\n",
      "> 50 | 0.46614712476730347 | 0.17099177837371826 | 0.8487649204244032 | 0.6833554376657824\n",
      "> 51 | 0.4659813642501831 | 0.17082497477531433 | 0.8490135941644562 | 0.6837284482758621\n",
      "> 52 | 0.46581655740737915 | 0.17066575586795807 | 0.8493866047745358 | 0.6843086870026526\n",
      "> 53 | 0.46565335988998413 | 0.17054221034049988 | 0.849676724137931 | 0.6843501326259946\n",
      "> 54 | 0.46549177169799805 | 0.17033910751342773 | 0.8496352785145889 | 0.6848889257294429\n",
      "> 55 | 0.46533218026161194 | 0.1702202409505844 | 0.8498425066312998 | 0.6848060344827587\n",
      "> 56 | 0.4651746153831482 | 0.17004156112670898 | 0.849925397877984 | 0.6848474801061007\n",
      "> 57 | 0.4650191068649292 | 0.16986076533794403 | 0.8500911803713528 | 0.6848474801061007\n",
      "> 58 | 0.46486616134643555 | 0.16975191235542297 | 0.8502155172413793 | 0.68559350132626\n",
      "> 59 | 0.46471601724624634 | 0.16953833401203156 | 0.8502569628647215 | 0.6858007294429708\n",
      "> 60 | 0.46456852555274963 | 0.16940495371818542 | 0.8499668435013262 | 0.6861737400530504\n",
      "> 61 | 0.46442386507987976 | 0.1692170947790146 | 0.8500082891246684 | 0.685386273209549\n",
      "> 62 | 0.4642820358276367 | 0.16908380389213562 | 0.8500497347480106 | 0.68559350132626\n",
      "> 63 | 0.4641435742378235 | 0.1688932329416275 | 0.8502984084880637 | 0.6853033819628647\n",
      "> 64 | 0.46400803327560425 | 0.16877694427967072 | 0.8505056366047745 | 0.6848060344827587\n",
      "> 65 | 0.46387559175491333 | 0.16856826841831207 | 0.8506299734748011 | 0.6838942307692307\n",
      "> 66 | 0.463746577501297 | 0.16840426623821259 | 0.8506714190981433 | 0.6835626657824934\n",
      "> 67 | 0.46362096071243286 | 0.16829350590705872 | 0.8506299734748011 | 0.6832725464190982\n",
      "> 68 | 0.4634988605976105 | 0.16808775067329407 | 0.8505056366047745 | 0.6830653183023873\n",
      "> 69 | 0.46338045597076416 | 0.16792362928390503 | 0.8507543103448276 | 0.6825679708222812\n",
      "> 70 | 0.46326541900634766 | 0.16780444979667664 | 0.8508786472148541 | 0.6828166445623343\n",
      "> 71 | 0.46315377950668335 | 0.16762320697307587 | 0.851085875331565 | 0.6833139920424404\n",
      "> 72 | 0.46304595470428467 | 0.16745492815971375 | 0.8511273209549072 | 0.6836041114058355\n",
      "> 73 | 0.46294185519218445 | 0.16732169687747955 | 0.8513759946949602 | 0.6842257957559682\n",
      "> 74 | 0.46284133195877075 | 0.1671428233385086 | 0.8512516578249337 | 0.683479774535809\n",
      "> 75 | 0.4627445936203003 | 0.1670050173997879 | 0.8512931034482759 | 0.6838113395225465\n",
      "> 76 | 0.4626513421535492 | 0.16682186722755432 | 0.8515417771883289 | 0.6837284482758621\n",
      "> 77 | 0.4625619053840637 | 0.1666991412639618 | 0.8515832228116711 | 0.6837698938992043\n",
      "> 78 | 0.4624759256839752 | 0.16655200719833374 | 0.851790450928382 | 0.6840185676392573\n",
      "> 79 | 0.4623938798904419 | 0.1663685292005539 | 0.851790450928382 | 0.6838527851458885\n",
      "> 80 | 0.4623156785964966 | 0.16626609861850739 | 0.8516246684350133 | 0.6836455570291777\n",
      "> 81 | 0.462240993976593 | 0.16610956192016602 | 0.8515832228116711 | 0.6840600132625995\n",
      "> 82 | 0.4621697962284088 | 0.1659226417541504 | 0.8518318965517241 | 0.6840185676392573\n",
      "> 83 | 0.4621022641658783 | 0.1657809317111969 | 0.851790450928382 | 0.6844744694960212\n",
      "> 84 | 0.46203818917274475 | 0.16564249992370605 | 0.8519147877984085 | 0.6842257957559682\n",
      "> 85 | 0.4619775712490082 | 0.16552342474460602 | 0.8518733421750663 | 0.684184350132626\n",
      "> 86 | 0.46192044019699097 | 0.16538205742835999 | 0.8518318965517241 | 0.6841429045092838\n",
      "> 87 | 0.4618667960166931 | 0.1652478277683258 | 0.8519147877984085 | 0.6840185676392573\n",
      "> 88 | 0.4618168771266937 | 0.16509830951690674 | 0.8520805702917772 | 0.6836870026525199\n",
      "> 89 | 0.4617701470851898 | 0.16493317484855652 | 0.8519976790450928 | 0.6833139920424404\n",
      "> 90 | 0.4617266058921814 | 0.16483807563781738 | 0.8519562334217506 | 0.6834383289124668\n",
      "> 91 | 0.4616864323616028 | 0.16470728814601898 | 0.8519976790450928 | 0.683231100795756\n",
      "> 92 | 0.4616495966911316 | 0.16453532874584198 | 0.852039124668435 | 0.683231100795756\n",
      "> 93 | 0.4616154730319977 | 0.1644323766231537 | 0.8522049071618037 | 0.6831482095490716\n",
      "> 94 | 0.4615848958492279 | 0.16429229080677032 | 0.8522463527851459 | 0.6824850795755968\n",
      "> 95 | 0.46155714988708496 | 0.16414135694503784 | 0.8522463527851459 | 0.6828995358090185\n",
      "> 96 | 0.46153247356414795 | 0.16404621303081512 | 0.8522049071618037 | 0.683231100795756\n",
      "> 97 | 0.46151018142700195 | 0.16392233967781067 | 0.8522877984084881 | 0.6836455570291777\n",
      "> 98 | 0.46149080991744995 | 0.16376438736915588 | 0.8522877984084881 | 0.6835626657824934\n",
      "> 99 | 0.4614746570587158 | 0.16367730498313904 | 0.8524121352785146 | 0.683231100795756\n",
      "> 100 | 0.46146076917648315 | 0.16356161236763 | 0.8524121352785146 | 0.6828580901856764\n",
      "> Evaluation\n",
      "> Class Acc = 0.8503989361702128\n",
      "> Adv Acc = 0.8503989361702128\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.81613889336586 | 0.9062025360763073 | 0.8849428594112396\n",
      "> Confusion Matrix \n",
      "TN: 4246.0 | FP: 312.0 \n",
      "FN: 588.0 | TP: 870.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1716.0 | FP: 42.0 \n",
      "FN: 101.0 | TP: 100.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2530.0 | FP: 270.0 \n",
      "FN: 487.0 | TP: 770.0\n"
     ]
    }
   ],
   "source": [
    "fairdef = 'EqOpp'\n",
    "\n",
    "for cv_seed in cv_seeds:\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        x, y, a, test_size=0.3, random_state=cv_seed)\n",
    "\n",
    "    train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "    train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # train below\n",
    "\n",
    "    opt = Adam(learning_rate=lr)\n",
    "\n",
    "    model = FairLogisticRegression(xdim, ydim, adim, batch_size, fairdef)\n",
    "    zhang_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "    Y, A, Y_hat, A_hat = fair_evaluation(model, test_data)\n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1 = compute_metrics(Y, A, Y_hat, A_hat, adim)\n",
    "\n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    tradeoff = []\n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "    result = ['Zhang4EqOpp', cv_seed, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    del(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving into DF then CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>clas_acc</th>\n",
       "      <th>dp</th>\n",
       "      <th>deqodds</th>\n",
       "      <th>deqopp</th>\n",
       "      <th>trade_dp</th>\n",
       "      <th>trade_deqodds</th>\n",
       "      <th>trade_deqopp</th>\n",
       "      <th>TN_a0</th>\n",
       "      <th>FP_a0</th>\n",
       "      <th>FN_a0</th>\n",
       "      <th>TP_a0</th>\n",
       "      <th>TN_a1</th>\n",
       "      <th>FP_a1</th>\n",
       "      <th>FN_a1</th>\n",
       "      <th>TP_a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zhang4DP</td>\n",
       "      <td>0.850066</td>\n",
       "      <td>0.811667</td>\n",
       "      <td>0.902038</td>\n",
       "      <td>0.880965</td>\n",
       "      <td>0.830423</td>\n",
       "      <td>0.875281</td>\n",
       "      <td>0.865240</td>\n",
       "      <td>1718.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2521.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>775.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zhang4EqOdds</td>\n",
       "      <td>0.850399</td>\n",
       "      <td>0.812160</td>\n",
       "      <td>0.902395</td>\n",
       "      <td>0.880965</td>\n",
       "      <td>0.830840</td>\n",
       "      <td>0.875626</td>\n",
       "      <td>0.865412</td>\n",
       "      <td>1718.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2523.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>775.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zhang4EqOpp</td>\n",
       "      <td>0.850399</td>\n",
       "      <td>0.816139</td>\n",
       "      <td>0.906203</td>\n",
       "      <td>0.884943</td>\n",
       "      <td>0.832917</td>\n",
       "      <td>0.877414</td>\n",
       "      <td>0.867327</td>\n",
       "      <td>1716.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2530.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>770.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name  clas_acc        dp   deqodds    deqopp  trade_dp  \\\n",
       "0      Zhang4DP  0.850066  0.811667  0.902038  0.880965  0.830423   \n",
       "1  Zhang4EqOdds  0.850399  0.812160  0.902395  0.880965  0.830840   \n",
       "2   Zhang4EqOpp  0.850399  0.816139  0.906203  0.884943  0.832917   \n",
       "\n",
       "   trade_deqodds  trade_deqopp   TN_a0  FP_a0  FN_a0  TP_a0   TN_a1  FP_a1  \\\n",
       "0       0.875281      0.865240  1718.0   40.0  101.0  100.0  2521.0  279.0   \n",
       "1       0.875626      0.865412  1718.0   40.0  101.0  100.0  2523.0  277.0   \n",
       "2       0.877414      0.867327  1716.0   42.0  101.0  100.0  2530.0  270.0   \n",
       "\n",
       "   FN_a1  TP_a1  \n",
       "0  482.0  775.0  \n",
       "1  482.0  775.0  \n",
       "2  487.0  770.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(results, columns=header)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f'{data_name}-result/zhang-{epochs}.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41359ec383f887151a607ad1e28cb7dbc05f61385692c63e2bb2f343bf03f280"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('falsb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
