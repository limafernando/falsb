{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.data import Dataset\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from models.unfair_lr.models import UnfairLogisticRegression\n",
    "from models.unfair_lr.learning import train_loop\n",
    "from util.evaluation import *\n",
    "from util.load_data import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_seeds = [13, 29, 42, 55, 73]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'titanic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, a = load_data(data_name)\n",
    "raw_data = (x, y, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdim = x.shape[1]\n",
    "ydim = y.shape[1]\n",
    "adim = a.shape[1]\n",
    "zdim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"model_name\", \"cv_seed\", \"clas_acc\", \"dp\", \"deqodds\", \"deqopp\", \"trade_dp\", \"trade_deqodds\", \"trade_deqopp\", \"TN_a0\", \"FP_a0\", \"FN_a0\", \"TP_a0\", \"TN_a1\", \"FP_a1\", \"FN_a1\", \"TP_a1\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.9008842706680298 | 0.3645833333333333\n",
      "> 2 | 0.8942244052886963 | 0.3576388888888889\n",
      "> 3 | 0.8893353939056396 | 0.3576388888888889\n",
      "> 4 | 0.8857583999633789 | 0.3576388888888889\n",
      "> 5 | 0.8834515810012817 | 0.3576388888888889\n",
      "> 6 | 0.880972146987915 | 0.3611111111111111\n",
      "> 7 | 0.8789172172546387 | 0.359375\n",
      "> 8 | 0.8771583437919617 | 0.359375\n",
      "> 9 | 0.8759320974349976 | 0.3576388888888889\n",
      "> 10 | 0.8744905591011047 | 0.359375\n",
      "> 11 | 0.8732051849365234 | 0.359375\n",
      "> 12 | 0.8722895979881287 | 0.359375\n",
      "> 13 | 0.8711854815483093 | 0.359375\n",
      "> 14 | 0.8701767921447754 | 0.359375\n",
      "> 15 | 0.8694486618041992 | 0.359375\n",
      "> 16 | 0.8685561418533325 | 0.359375\n",
      "> 17 | 0.8677276372909546 | 0.359375\n",
      "> 18 | 0.8671241998672485 | 0.359375\n",
      "> 19 | 0.8663761615753174 | 0.359375\n",
      "> 20 | 0.8658287525177002 | 0.359375\n",
      "> 21 | 0.8651478290557861 | 0.359375\n",
      "> 22 | 0.8645058870315552 | 0.359375\n",
      "> 23 | 0.864033579826355 | 0.359375\n",
      "> 24 | 0.8634417057037354 | 0.359375\n",
      "> 25 | 0.8630050420761108 | 0.359375\n",
      "> 26 | 0.8624565601348877 | 0.359375\n",
      "> 27 | 0.8620505928993225 | 0.359375\n",
      "> 28 | 0.8615398406982422 | 0.359375\n",
      "> 29 | 0.8610519170761108 | 0.359375\n",
      "> 30 | 0.8606898784637451 | 0.359375\n",
      "> 31 | 0.8602319359779358 | 0.359375\n",
      "> 32 | 0.8598915338516235 | 0.359375\n",
      "> 33 | 0.8594603538513184 | 0.359375\n",
      "> 34 | 0.8591392040252686 | 0.359375\n",
      "> 35 | 0.8587319850921631 | 0.359375\n",
      "> 36 | 0.8584282994270325 | 0.359375\n",
      "> 37 | 0.8580427169799805 | 0.359375\n",
      "> 38 | 0.8577547073364258 | 0.359375\n",
      "> 39 | 0.8573885560035706 | 0.359375\n",
      "> 40 | 0.857114851474762 | 0.359375\n",
      "> 41 | 0.8567663431167603 | 0.359375\n",
      "> 42 | 0.8565055727958679 | 0.359375\n",
      "> 43 | 0.8561732769012451 | 0.359375\n",
      "> 44 | 0.8559242486953735 | 0.359375\n",
      "> 45 | 0.855606734752655 | 0.359375\n",
      "> 46 | 0.8553686141967773 | 0.359375\n",
      "> 47 | 0.8550646305084229 | 0.359375\n",
      "> 48 | 0.8548365831375122 | 0.359375\n",
      "> 49 | 0.8545455932617188 | 0.359375\n",
      "> 50 | 0.8543267250061035 | 0.359375\n",
      "> 51 | 0.8541104793548584 | 0.359375\n",
      "> 52 | 0.8538341522216797 | 0.359375\n",
      "> 53 | 0.8536263704299927 | 0.359375\n",
      "> 54 | 0.853360652923584 | 0.359375\n",
      "> 55 | 0.8531605005264282 | 0.359375\n",
      "> 56 | 0.8529046177864075 | 0.359375\n",
      "> 57 | 0.8527117967605591 | 0.359375\n",
      "> 58 | 0.8525210618972778 | 0.359375\n",
      "> 59 | 0.8522769808769226 | 0.359375\n",
      "> 60 | 0.8520928621292114 | 0.359375\n",
      "> 61 | 0.8518571853637695 | 0.359375\n",
      "> 62 | 0.8516793251037598 | 0.359375\n",
      "> 63 | 0.8515030741691589 | 0.359375\n",
      "> 64 | 0.8512775897979736 | 0.359375\n",
      "> 65 | 0.8511072993278503 | 0.359375\n",
      "> 66 | 0.8508890867233276 | 0.359375\n",
      "> 67 | 0.8507241010665894 | 0.359375\n",
      "> 68 | 0.8505607843399048 | 0.359375\n",
      "> 69 | 0.8503513336181641 | 0.359375\n",
      "> 70 | 0.8501930236816406 | 0.359375\n",
      "> 71 | 0.8499899506568909 | 0.359375\n",
      "> 72 | 0.8498362898826599 | 0.359375\n",
      "> 73 | 0.8496838808059692 | 0.359375\n",
      "> 74 | 0.8494886755943298 | 0.359375\n",
      "> 75 | 0.8493407964706421 | 0.359375\n",
      "> 76 | 0.8491940498352051 | 0.359375\n",
      "> 77 | 0.849005937576294 | 0.359375\n",
      "> 78 | 0.848863422870636 | 0.359375\n",
      "> 79 | 0.848721981048584 | 0.359375\n",
      "> 80 | 0.8485406041145325 | 0.359375\n",
      "> 81 | 0.8484030961990356 | 0.359375\n",
      "> 82 | 0.8482273817062378 | 0.359375\n",
      "> 83 | 0.8480933904647827 | 0.359375\n",
      "> 84 | 0.8479605913162231 | 0.359375\n",
      "> 85 | 0.8477901220321655 | 0.359375\n",
      "> 86 | 0.8476607203483582 | 0.359375\n",
      "> 87 | 0.8475322723388672 | 0.359375\n",
      "> 88 | 0.8473674058914185 | 0.359375\n",
      "> 89 | 0.8472421765327454 | 0.359375\n",
      "> 90 | 0.8471178412437439 | 0.359375\n",
      "> 91 | 0.8469583988189697 | 0.359375\n",
      "> 92 | 0.846837043762207 | 0.359375\n",
      "> 93 | 0.8467166423797607 | 0.359375\n",
      "> 94 | 0.8465627431869507 | 0.359375\n",
      "> 95 | 0.8464451432228088 | 0.359375\n",
      "> 96 | 0.8463283777236938 | 0.359375\n",
      "> 97 | 0.8462123870849609 | 0.359375\n",
      "> 98 | 0.846063494682312 | 0.359375\n",
      "> 99 | 0.8459502458572388 | 0.359375\n",
      "> 100 | 0.8458378314971924 | 0.359375\n",
      "> Evaluation\n",
      "> Class Acc = 0.41796875\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.957176148891449 | 0.9589743576943874 | 0.9333333298563957\n",
      "> Confusion Matrix \n",
      "TN: 2.0 | FP: 144.0 \n",
      "FN: 5.0 | TP: 105.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 0.0 | FP: 16.0 \n",
      "FN: 5.0 | TP: 70.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2.0 | FP: 128.0 \n",
      "FN: 0.0 | TP: 35.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.9671807289123535 | 0.3767361111111111\n",
      "> 2 | 0.9368497133255005 | 0.3645833333333333\n",
      "> 3 | 0.9239709377288818 | 0.3559027777777778\n",
      "> 4 | 0.9163368344306946 | 0.3541666666666667\n",
      "> 5 | 0.9110317230224609 | 0.3559027777777778\n",
      "> 6 | 0.9070075154304504 | 0.3541666666666667\n",
      "> 7 | 0.9037834405899048 | 0.3541666666666667\n",
      "> 8 | 0.901103138923645 | 0.3559027777777778\n",
      "> 9 | 0.8988146185874939 | 0.3559027777777778\n",
      "> 10 | 0.896821141242981 | 0.3576388888888889\n",
      "> 11 | 0.8950573205947876 | 0.3576388888888889\n",
      "> 12 | 0.8934769630432129 | 0.3576388888888889\n",
      "> 13 | 0.8920465707778931 | 0.359375\n",
      "> 14 | 0.8907407522201538 | 0.3576388888888889\n",
      "> 15 | 0.8895403146743774 | 0.3576388888888889\n",
      "> 16 | 0.8884296417236328 | 0.3576388888888889\n",
      "> 17 | 0.8873966932296753 | 0.3576388888888889\n",
      "> 18 | 0.8864315152168274 | 0.3576388888888889\n",
      "> 19 | 0.8855260014533997 | 0.3576388888888889\n",
      "> 20 | 0.8846733570098877 | 0.3576388888888889\n",
      "> 21 | 0.8838678598403931 | 0.3576388888888889\n",
      "> 22 | 0.8831047415733337 | 0.3576388888888889\n",
      "> 23 | 0.8823797702789307 | 0.3576388888888889\n",
      "> 24 | 0.8816894888877869 | 0.3576388888888889\n",
      "> 25 | 0.8810306787490845 | 0.3576388888888889\n",
      "> 26 | 0.8804007768630981 | 0.3576388888888889\n",
      "> 27 | 0.8797973394393921 | 0.3559027777777778\n",
      "> 28 | 0.879218339920044 | 0.3559027777777778\n",
      "> 29 | 0.8786618113517761 | 0.3559027777777778\n",
      "> 30 | 0.8781262636184692 | 0.3559027777777778\n",
      "> 31 | 0.8776099681854248 | 0.3559027777777778\n",
      "> 32 | 0.8771118521690369 | 0.3559027777777778\n",
      "> 33 | 0.8766305446624756 | 0.3576388888888889\n",
      "> 34 | 0.8761650919914246 | 0.3576388888888889\n",
      "> 35 | 0.8757143616676331 | 0.3576388888888889\n",
      "> 36 | 0.875277578830719 | 0.3576388888888889\n",
      "> 37 | 0.8748538494110107 | 0.3576388888888889\n",
      "> 38 | 0.8744425177574158 | 0.359375\n",
      "> 39 | 0.8740428686141968 | 0.359375\n",
      "> 40 | 0.8736541867256165 | 0.359375\n",
      "> 41 | 0.8732759356498718 | 0.359375\n",
      "> 42 | 0.8729076981544495 | 0.359375\n",
      "> 43 | 0.8725487589836121 | 0.359375\n",
      "> 44 | 0.8721988201141357 | 0.359375\n",
      "> 45 | 0.8718574047088623 | 0.359375\n",
      "> 46 | 0.8715240955352783 | 0.359375\n",
      "> 47 | 0.8711985349655151 | 0.359375\n",
      "> 48 | 0.8708804845809937 | 0.359375\n",
      "> 49 | 0.8705693483352661 | 0.359375\n",
      "> 50 | 0.8702651262283325 | 0.359375\n",
      "> 51 | 0.8699672818183899 | 0.359375\n",
      "> 52 | 0.8696757555007935 | 0.359375\n",
      "> 53 | 0.8693901896476746 | 0.359375\n",
      "> 54 | 0.8691103458404541 | 0.359375\n",
      "> 55 | 0.8688360452651978 | 0.359375\n",
      "> 56 | 0.8685671091079712 | 0.359375\n",
      "> 57 | 0.8683031797409058 | 0.359375\n",
      "> 58 | 0.8680441975593567 | 0.359375\n",
      "> 59 | 0.8677899837493896 | 0.359375\n",
      "> 60 | 0.8675403594970703 | 0.359375\n",
      "> 61 | 0.8672951459884644 | 0.359375\n",
      "> 62 | 0.8670542240142822 | 0.359375\n",
      "> 63 | 0.8668173551559448 | 0.359375\n",
      "> 64 | 0.8665845394134521 | 0.359375\n",
      "> 65 | 0.8663555383682251 | 0.359375\n",
      "> 66 | 0.8661303520202637 | 0.359375\n",
      "> 67 | 0.8659087419509888 | 0.359375\n",
      "> 68 | 0.8656905889511108 | 0.359375\n",
      "> 69 | 0.8654758930206299 | 0.359375\n",
      "> 70 | 0.8652644157409668 | 0.359375\n",
      "> 71 | 0.8650561571121216 | 0.359375\n",
      "> 72 | 0.8648509979248047 | 0.359375\n",
      "> 73 | 0.8646489381790161 | 0.359375\n",
      "> 74 | 0.8644497394561768 | 0.359375\n",
      "> 75 | 0.8642534017562866 | 0.359375\n",
      "> 76 | 0.8640598058700562 | 0.359375\n",
      "> 77 | 0.8638688325881958 | 0.359375\n",
      "> 78 | 0.8636806011199951 | 0.359375\n",
      "> 79 | 0.8634949326515198 | 0.359375\n",
      "> 80 | 0.8633116483688354 | 0.359375\n",
      "> 81 | 0.8631308078765869 | 0.359375\n",
      "> 82 | 0.8629523515701294 | 0.3611111111111111\n",
      "> 83 | 0.8627761602401733 | 0.3611111111111111\n",
      "> 84 | 0.8626022338867188 | 0.3611111111111111\n",
      "> 85 | 0.8624304533004761 | 0.3611111111111111\n",
      "> 86 | 0.8622608184814453 | 0.3611111111111111\n",
      "> 87 | 0.8620932698249817 | 0.3628472222222222\n",
      "> 88 | 0.8619277477264404 | 0.3628472222222222\n",
      "> 89 | 0.8617641925811768 | 0.3628472222222222\n",
      "> 90 | 0.8616025447845459 | 0.3628472222222222\n",
      "> 91 | 0.8614428639411926 | 0.3628472222222222\n",
      "> 92 | 0.8612849712371826 | 0.3628472222222222\n",
      "> 93 | 0.8611289262771606 | 0.3628472222222222\n",
      "> 94 | 0.8609746098518372 | 0.3628472222222222\n",
      "> 95 | 0.8608219623565674 | 0.3645833333333333\n",
      "> 96 | 0.8606711030006409 | 0.3645833333333333\n",
      "> 97 | 0.8605219125747681 | 0.3645833333333333\n",
      "> 98 | 0.8603742718696594 | 0.3645833333333333\n",
      "> 99 | 0.8602281808853149 | 0.3645833333333333\n",
      "> 100 | 0.8600837588310242 | 0.3645833333333333\n",
      "> Evaluation\n",
      "> Class Acc = 0.3125\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8789330124855042 | 0.967212975025177 | 0.9564549326896667\n",
      "> Confusion Matrix \n",
      "TN: 10.0 | FP: 153.0 \n",
      "FN: 23.0 | TP: 70.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 2.0 | FP: 23.0 \n",
      "FN: 16.0 | TP: 45.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 8.0 | FP: 130.0 \n",
      "FN: 7.0 | TP: 25.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.9321149587631226 | 0.3854166666666667\n",
      "> 2 | 0.9231574535369873 | 0.3697916666666667\n",
      "> 3 | 0.9179414510726929 | 0.3836805555555556\n",
      "> 4 | 0.9139279723167419 | 0.3836805555555556\n",
      "> 5 | 0.9106769561767578 | 0.3836805555555556\n",
      "> 6 | 0.9079556465148926 | 0.3836805555555556\n",
      "> 7 | 0.9056211113929749 | 0.3836805555555556\n",
      "> 8 | 0.9035800099372864 | 0.3836805555555556\n",
      "> 9 | 0.9017684459686279 | 0.3836805555555556\n",
      "> 10 | 0.9001411199569702 | 0.3836805555555556\n",
      "> 11 | 0.8986648321151733 | 0.3836805555555556\n",
      "> 12 | 0.897314190864563 | 0.3836805555555556\n",
      "> 13 | 0.8960700631141663 | 0.3836805555555556\n",
      "> 14 | 0.8949170112609863 | 0.3836805555555556\n",
      "> 15 | 0.8938428163528442 | 0.3836805555555556\n",
      "> 16 | 0.892837643623352 | 0.3836805555555556\n",
      "> 17 | 0.8918931484222412 | 0.3836805555555556\n",
      "> 18 | 0.8910025358200073 | 0.3836805555555556\n",
      "> 19 | 0.8901602029800415 | 0.3836805555555556\n",
      "> 20 | 0.8893611431121826 | 0.3836805555555556\n",
      "> 21 | 0.8886012434959412 | 0.3836805555555556\n",
      "> 22 | 0.8878768682479858 | 0.3836805555555556\n",
      "> 23 | 0.8871849775314331 | 0.3836805555555556\n",
      "> 24 | 0.8865227103233337 | 0.3836805555555556\n",
      "> 25 | 0.885887622833252 | 0.3854166666666667\n",
      "> 26 | 0.8852776885032654 | 0.3854166666666667\n",
      "> 27 | 0.884691059589386 | 0.3854166666666667\n",
      "> 28 | 0.8841259479522705 | 0.3854166666666667\n",
      "> 29 | 0.8835809230804443 | 0.3854166666666667\n",
      "> 30 | 0.8830546140670776 | 0.3854166666666667\n",
      "> 31 | 0.8825456500053406 | 0.3854166666666667\n",
      "> 32 | 0.8820531368255615 | 0.3854166666666667\n",
      "> 33 | 0.8815760612487793 | 0.3854166666666667\n",
      "> 34 | 0.8811133503913879 | 0.3854166666666667\n",
      "> 35 | 0.8806642293930054 | 0.3854166666666667\n",
      "> 36 | 0.8802279233932495 | 0.3854166666666667\n",
      "> 37 | 0.8798037767410278 | 0.3854166666666667\n",
      "> 38 | 0.8793911933898926 | 0.3854166666666667\n",
      "> 39 | 0.8789894580841064 | 0.3854166666666667\n",
      "> 40 | 0.8785979747772217 | 0.3854166666666667\n",
      "> 41 | 0.8782163858413696 | 0.3871527777777778\n",
      "> 42 | 0.8778440952301025 | 0.3871527777777778\n",
      "> 43 | 0.8774807453155518 | 0.3871527777777778\n",
      "> 44 | 0.8771259188652039 | 0.3871527777777778\n",
      "> 45 | 0.8767791986465454 | 0.3871527777777778\n",
      "> 46 | 0.876440167427063 | 0.3871527777777778\n",
      "> 47 | 0.8761086463928223 | 0.3871527777777778\n",
      "> 48 | 0.875784158706665 | 0.3871527777777778\n",
      "> 49 | 0.8754665851593018 | 0.3871527777777778\n",
      "> 50 | 0.8751554489135742 | 0.3871527777777778\n",
      "> 51 | 0.8748506307601929 | 0.3871527777777778\n",
      "> 52 | 0.8745517730712891 | 0.3871527777777778\n",
      "> 53 | 0.874258816242218 | 0.3871527777777778\n",
      "> 54 | 0.8739714026451111 | 0.3871527777777778\n",
      "> 55 | 0.8736893534660339 | 0.3871527777777778\n",
      "> 56 | 0.8734124302864075 | 0.3871527777777778\n",
      "> 57 | 0.8731405735015869 | 0.3871527777777778\n",
      "> 58 | 0.8728735446929932 | 0.3871527777777778\n",
      "> 59 | 0.8726111650466919 | 0.3871527777777778\n",
      "> 60 | 0.8723533153533936 | 0.3871527777777778\n",
      "> 61 | 0.8720996975898743 | 0.3871527777777778\n",
      "> 62 | 0.8718503713607788 | 0.3871527777777778\n",
      "> 63 | 0.8716050982475281 | 0.3888888888888889\n",
      "> 64 | 0.8713637590408325 | 0.3888888888888889\n",
      "> 65 | 0.8711262345314026 | 0.3888888888888889\n",
      "> 66 | 0.8708924055099487 | 0.3888888888888889\n",
      "> 67 | 0.8706622123718262 | 0.3888888888888889\n",
      "> 68 | 0.8704354763031006 | 0.3888888888888889\n",
      "> 69 | 0.8702120780944824 | 0.3888888888888889\n",
      "> 70 | 0.8699920177459717 | 0.3888888888888889\n",
      "> 71 | 0.8697751760482788 | 0.3888888888888889\n",
      "> 72 | 0.8695614337921143 | 0.3888888888888889\n",
      "> 73 | 0.8693505525588989 | 0.3888888888888889\n",
      "> 74 | 0.8691427707672119 | 0.3888888888888889\n",
      "> 75 | 0.8689378499984741 | 0.3871527777777778\n",
      "> 76 | 0.8687355518341064 | 0.3871527777777778\n",
      "> 77 | 0.8685359954833984 | 0.3871527777777778\n",
      "> 78 | 0.8683391213417053 | 0.3854166666666667\n",
      "> 79 | 0.8681448101997375 | 0.3854166666666667\n",
      "> 80 | 0.8679530024528503 | 0.3854166666666667\n",
      "> 81 | 0.8677635192871094 | 0.3854166666666667\n",
      "> 82 | 0.867576539516449 | 0.3854166666666667\n",
      "> 83 | 0.8673918843269348 | 0.3854166666666667\n",
      "> 84 | 0.8672094345092773 | 0.3854166666666667\n",
      "> 85 | 0.8670291900634766 | 0.3854166666666667\n",
      "> 86 | 0.8668510913848877 | 0.3854166666666667\n",
      "> 87 | 0.8666751384735107 | 0.3854166666666667\n",
      "> 88 | 0.8665012121200562 | 0.3854166666666667\n",
      "> 89 | 0.8663293123245239 | 0.3836805555555556\n",
      "> 90 | 0.8661593794822693 | 0.3836805555555556\n",
      "> 91 | 0.8659912943840027 | 0.3836805555555556\n",
      "> 92 | 0.8658251762390137 | 0.3836805555555556\n",
      "> 93 | 0.8656609058380127 | 0.3836805555555556\n",
      "> 94 | 0.8654983639717102 | 0.3836805555555556\n",
      "> 95 | 0.865337610244751 | 0.3836805555555556\n",
      "> 96 | 0.8651785850524902 | 0.3836805555555556\n",
      "> 97 | 0.8650212287902832 | 0.3836805555555556\n",
      "> 98 | 0.8648655414581299 | 0.3836805555555556\n",
      "> 99 | 0.8647114038467407 | 0.3836805555555556\n",
      "> 100 | 0.8645589351654053 | 0.3836805555555556\n",
      "> Evaluation\n",
      "> Class Acc = 0.3671875\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9962683916091919 | 0.9870422538369894 | 0.9740845076739788\n",
      "> Confusion Matrix \n",
      "TN: 0.0 | FP: 160.0 \n",
      "FN: 2.0 | TP: 94.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 0.0 | FP: 28.0 \n",
      "FN: 1.0 | TP: 70.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 0.0 | FP: 132.0 \n",
      "FN: 1.0 | TP: 24.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.9255760312080383 | 0.3836805555555556\n",
      "> 2 | 0.9171959161758423 | 0.3732638888888889\n",
      "> 3 | 0.9121403694152832 | 0.3767361111111111\n",
      "> 4 | 0.9083441495895386 | 0.3767361111111111\n",
      "> 5 | 0.905305802822113 | 0.3767361111111111\n",
      "> 6 | 0.9027742147445679 | 0.3767361111111111\n",
      "> 7 | 0.9006050825119019 | 0.3767361111111111\n",
      "> 8 | 0.8987083435058594 | 0.3767361111111111\n",
      "> 9 | 0.8970235586166382 | 0.3767361111111111\n",
      "> 10 | 0.8955086469650269 | 0.3767361111111111\n",
      "> 11 | 0.8941326141357422 | 0.3767361111111111\n",
      "> 12 | 0.8928723335266113 | 0.3767361111111111\n",
      "> 13 | 0.891710102558136 | 0.3767361111111111\n",
      "> 14 | 0.890631914138794 | 0.3767361111111111\n",
      "> 15 | 0.8896264433860779 | 0.3767361111111111\n",
      "> 16 | 0.8886846303939819 | 0.3767361111111111\n",
      "> 17 | 0.8877990245819092 | 0.3767361111111111\n",
      "> 18 | 0.8869632482528687 | 0.3767361111111111\n",
      "> 19 | 0.8861721754074097 | 0.3767361111111111\n",
      "> 20 | 0.8854212760925293 | 0.3767361111111111\n",
      "> 21 | 0.884706974029541 | 0.3767361111111111\n",
      "> 22 | 0.8840258121490479 | 0.3767361111111111\n",
      "> 23 | 0.8833746910095215 | 0.3767361111111111\n",
      "> 24 | 0.8827512264251709 | 0.3767361111111111\n",
      "> 25 | 0.8821530938148499 | 0.3767361111111111\n",
      "> 26 | 0.8815784454345703 | 0.3767361111111111\n",
      "> 27 | 0.8810253143310547 | 0.3767361111111111\n",
      "> 28 | 0.8804923295974731 | 0.3767361111111111\n",
      "> 29 | 0.8799780607223511 | 0.3784722222222222\n",
      "> 30 | 0.8794812560081482 | 0.3784722222222222\n",
      "> 31 | 0.8790007829666138 | 0.3784722222222222\n",
      "> 32 | 0.8785355687141418 | 0.3784722222222222\n",
      "> 33 | 0.8780847787857056 | 0.3784722222222222\n",
      "> 34 | 0.8776475191116333 | 0.3784722222222222\n",
      "> 35 | 0.8772228956222534 | 0.3784722222222222\n",
      "> 36 | 0.8768104314804077 | 0.3784722222222222\n",
      "> 37 | 0.8764091730117798 | 0.3784722222222222\n",
      "> 38 | 0.8760188221931458 | 0.3784722222222222\n",
      "> 39 | 0.8756386041641235 | 0.3784722222222222\n",
      "> 40 | 0.8752681612968445 | 0.3784722222222222\n",
      "> 41 | 0.8749068975448608 | 0.3784722222222222\n",
      "> 42 | 0.8745543956756592 | 0.3784722222222222\n",
      "> 43 | 0.8742102384567261 | 0.3802083333333333\n",
      "> 44 | 0.8738740682601929 | 0.3802083333333333\n",
      "> 45 | 0.8735455274581909 | 0.3802083333333333\n",
      "> 46 | 0.8732243180274963 | 0.3802083333333333\n",
      "> 47 | 0.8729100823402405 | 0.3819444444444444\n",
      "> 48 | 0.8726025223731995 | 0.3819444444444444\n",
      "> 49 | 0.8723013401031494 | 0.3836805555555556\n",
      "> 50 | 0.8720064163208008 | 0.3819444444444444\n",
      "> 51 | 0.8717172741889954 | 0.3819444444444444\n",
      "> 52 | 0.8714338541030884 | 0.3819444444444444\n",
      "> 53 | 0.8711559176445007 | 0.3819444444444444\n",
      "> 54 | 0.8708832263946533 | 0.3819444444444444\n",
      "> 55 | 0.8706156015396118 | 0.3819444444444444\n",
      "> 56 | 0.8703528642654419 | 0.3819444444444444\n",
      "> 57 | 0.8700947761535645 | 0.3819444444444444\n",
      "> 58 | 0.8698412775993347 | 0.3819444444444444\n",
      "> 59 | 0.8695921897888184 | 0.3819444444444444\n",
      "> 60 | 0.869347333908081 | 0.3819444444444444\n",
      "> 61 | 0.869106650352478 | 0.3819444444444444\n",
      "> 62 | 0.8688698410987854 | 0.3819444444444444\n",
      "> 63 | 0.8686369061470032 | 0.3819444444444444\n",
      "> 64 | 0.868407666683197 | 0.3819444444444444\n",
      "> 65 | 0.8681820631027222 | 0.3819444444444444\n",
      "> 66 | 0.8679599761962891 | 0.3819444444444444\n",
      "> 67 | 0.8677412867546082 | 0.3819444444444444\n",
      "> 68 | 0.8675257563591003 | 0.3819444444444444\n",
      "> 69 | 0.8673136234283447 | 0.3819444444444444\n",
      "> 70 | 0.8671044111251831 | 0.3819444444444444\n",
      "> 71 | 0.8668982982635498 | 0.3819444444444444\n",
      "> 72 | 0.8666951656341553 | 0.3819444444444444\n",
      "> 73 | 0.8664947748184204 | 0.3819444444444444\n",
      "> 74 | 0.86629718542099 | 0.3819444444444444\n",
      "> 75 | 0.8661022782325745 | 0.3819444444444444\n",
      "> 76 | 0.8659100532531738 | 0.3819444444444444\n",
      "> 77 | 0.865720272064209 | 0.3802083333333333\n",
      "> 78 | 0.8655331134796143 | 0.3802083333333333\n",
      "> 79 | 0.8653483390808105 | 0.3802083333333333\n",
      "> 80 | 0.8651658892631531 | 0.3802083333333333\n",
      "> 81 | 0.8649857640266418 | 0.3802083333333333\n",
      "> 82 | 0.8648078441619873 | 0.3802083333333333\n",
      "> 83 | 0.8646321892738342 | 0.3802083333333333\n",
      "> 84 | 0.8644586801528931 | 0.3802083333333333\n",
      "> 85 | 0.8642871975898743 | 0.3802083333333333\n",
      "> 86 | 0.8641178011894226 | 0.3802083333333333\n",
      "> 87 | 0.8639503717422485 | 0.3802083333333333\n",
      "> 88 | 0.863784909248352 | 0.3802083333333333\n",
      "> 89 | 0.8636213541030884 | 0.3784722222222222\n",
      "> 90 | 0.8634596467018127 | 0.3784722222222222\n",
      "> 91 | 0.8632997870445251 | 0.3784722222222222\n",
      "> 92 | 0.863141655921936 | 0.3784722222222222\n",
      "> 93 | 0.8629853129386902 | 0.3784722222222222\n",
      "> 94 | 0.862830638885498 | 0.3784722222222222\n",
      "> 95 | 0.8626776337623596 | 0.3784722222222222\n",
      "> 96 | 0.8625262975692749 | 0.3784722222222222\n",
      "> 97 | 0.8623764514923096 | 0.3784722222222222\n",
      "> 98 | 0.862228274345398 | 0.3784722222222222\n",
      "> 99 | 0.8620816469192505 | 0.3784722222222222\n",
      "> 100 | 0.8619364500045776 | 0.3784722222222222\n",
      "> Evaluation\n",
      "> Class Acc = 0.3828125\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9997231364250183 | 0.9849932491779327 | 0.984375\n",
      "> Confusion Matrix \n",
      "TN: 2.0 | FP: 157.0 \n",
      "FN: 1.0 | TP: 96.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 0.0 | FP: 20.0 \n",
      "FN: 1.0 | TP: 63.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2.0 | FP: 137.0 \n",
      "FN: 0.0 | TP: 33.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.8202025294303894 | 0.375\n",
      "> 2 | 0.8089340925216675 | 0.3576388888888889\n",
      "> 3 | 0.8051252365112305 | 0.3541666666666667\n",
      "> 4 | 0.8023490905761719 | 0.3541666666666667\n",
      "> 5 | 0.8001524209976196 | 0.3541666666666667\n",
      "> 6 | 0.7983359098434448 | 0.3541666666666667\n",
      "> 7 | 0.7967883348464966 | 0.3541666666666667\n",
      "> 8 | 0.7954409122467041 | 0.3541666666666667\n",
      "> 9 | 0.7942483425140381 | 0.3541666666666667\n",
      "> 10 | 0.7931791543960571 | 0.3541666666666667\n",
      "> 11 | 0.7922103404998779 | 0.3541666666666667\n",
      "> 12 | 0.7913249731063843 | 0.3541666666666667\n",
      "> 13 | 0.7905100584030151 | 0.3541666666666667\n",
      "> 14 | 0.7897552251815796 | 0.3541666666666667\n",
      "> 15 | 0.789052426815033 | 0.3576388888888889\n",
      "> 16 | 0.7883950471878052 | 0.359375\n",
      "> 17 | 0.7877776622772217 | 0.359375\n",
      "> 18 | 0.7871957421302795 | 0.3611111111111111\n",
      "> 19 | 0.7866455316543579 | 0.3611111111111111\n",
      "> 20 | 0.7861236333847046 | 0.3611111111111111\n",
      "> 21 | 0.7856276035308838 | 0.3611111111111111\n",
      "> 22 | 0.7851547598838806 | 0.3628472222222222\n",
      "> 23 | 0.784703254699707 | 0.3628472222222222\n",
      "> 24 | 0.784271240234375 | 0.3628472222222222\n",
      "> 25 | 0.7838571071624756 | 0.3628472222222222\n",
      "> 26 | 0.7834594249725342 | 0.3628472222222222\n",
      "> 27 | 0.7830770015716553 | 0.3628472222222222\n",
      "> 28 | 0.7827087640762329 | 0.3628472222222222\n",
      "> 29 | 0.7823535799980164 | 0.3628472222222222\n",
      "> 30 | 0.7820107340812683 | 0.3628472222222222\n",
      "> 31 | 0.7816792726516724 | 0.3628472222222222\n",
      "> 32 | 0.7813585996627808 | 0.3628472222222222\n",
      "> 33 | 0.7810479402542114 | 0.3628472222222222\n",
      "> 34 | 0.7807468175888062 | 0.3628472222222222\n",
      "> 35 | 0.7804546356201172 | 0.3628472222222222\n",
      "> 36 | 0.780170738697052 | 0.3628472222222222\n",
      "> 37 | 0.7798948287963867 | 0.3628472222222222\n",
      "> 38 | 0.7796264886856079 | 0.3628472222222222\n",
      "> 39 | 0.7793653011322021 | 0.3611111111111111\n",
      "> 40 | 0.7791107892990112 | 0.3611111111111111\n",
      "> 41 | 0.778862714767456 | 0.3611111111111111\n",
      "> 42 | 0.7786208391189575 | 0.3611111111111111\n",
      "> 43 | 0.778384804725647 | 0.3611111111111111\n",
      "> 44 | 0.7781542539596558 | 0.3611111111111111\n",
      "> 45 | 0.7779290676116943 | 0.3611111111111111\n",
      "> 46 | 0.777708888053894 | 0.3611111111111111\n",
      "> 47 | 0.7774937152862549 | 0.3611111111111111\n",
      "> 48 | 0.7772830128669739 | 0.3611111111111111\n",
      "> 49 | 0.7770769000053406 | 0.3611111111111111\n",
      "> 50 | 0.7768749594688416 | 0.3611111111111111\n",
      "> 51 | 0.7766772508621216 | 0.3611111111111111\n",
      "> 52 | 0.7764833569526672 | 0.3611111111111111\n",
      "> 53 | 0.7762932777404785 | 0.3611111111111111\n",
      "> 54 | 0.7761069536209106 | 0.3611111111111111\n",
      "> 55 | 0.775924026966095 | 0.3611111111111111\n",
      "> 56 | 0.7757445573806763 | 0.3611111111111111\n",
      "> 57 | 0.7755683660507202 | 0.3611111111111111\n",
      "> 58 | 0.7753952741622925 | 0.3611111111111111\n",
      "> 59 | 0.7752252817153931 | 0.3611111111111111\n",
      "> 60 | 0.7750581502914429 | 0.3611111111111111\n",
      "> 61 | 0.7748939990997314 | 0.3611111111111111\n",
      "> 62 | 0.7747324109077454 | 0.3611111111111111\n",
      "> 63 | 0.774573564529419 | 0.3611111111111111\n",
      "> 64 | 0.7744174003601074 | 0.3611111111111111\n",
      "> 65 | 0.7742636203765869 | 0.3611111111111111\n",
      "> 66 | 0.7741122245788574 | 0.3611111111111111\n",
      "> 67 | 0.773963212966919 | 0.3611111111111111\n",
      "> 68 | 0.7738165259361267 | 0.3611111111111111\n",
      "> 69 | 0.7736720442771912 | 0.3611111111111111\n",
      "> 70 | 0.7735296487808228 | 0.3611111111111111\n",
      "> 71 | 0.7733893394470215 | 0.3611111111111111\n",
      "> 72 | 0.7732510566711426 | 0.3611111111111111\n",
      "> 73 | 0.773114800453186 | 0.3611111111111111\n",
      "> 74 | 0.7729803919792175 | 0.3611111111111111\n",
      "> 75 | 0.7728478908538818 | 0.3611111111111111\n",
      "> 76 | 0.7727171182632446 | 0.3611111111111111\n",
      "> 77 | 0.7725881934165955 | 0.3611111111111111\n",
      "> 78 | 0.7724609375 | 0.3611111111111111\n",
      "> 79 | 0.772335410118103 | 0.3611111111111111\n",
      "> 80 | 0.772211492061615 | 0.3611111111111111\n",
      "> 81 | 0.7720891237258911 | 0.3611111111111111\n",
      "> 82 | 0.7719683647155762 | 0.3611111111111111\n",
      "> 83 | 0.7718490362167358 | 0.3611111111111111\n",
      "> 84 | 0.7717312574386597 | 0.3611111111111111\n",
      "> 85 | 0.7716149091720581 | 0.3611111111111111\n",
      "> 86 | 0.7714999914169312 | 0.3611111111111111\n",
      "> 87 | 0.7713863253593445 | 0.3611111111111111\n",
      "> 88 | 0.7712740898132324 | 0.3611111111111111\n",
      "> 89 | 0.7711632251739502 | 0.3611111111111111\n",
      "> 90 | 0.7710535526275635 | 0.3611111111111111\n",
      "> 91 | 0.7709450721740723 | 0.3611111111111111\n",
      "> 92 | 0.7708379626274109 | 0.3611111111111111\n",
      "> 93 | 0.7707319259643555 | 0.3611111111111111\n",
      "> 94 | 0.7706271409988403 | 0.3611111111111111\n",
      "> 95 | 0.7705235481262207 | 0.3611111111111111\n",
      "> 96 | 0.7704209089279175 | 0.3611111111111111\n",
      "> 97 | 0.7703194618225098 | 0.3611111111111111\n",
      "> 98 | 0.7702192068099976 | 0.3611111111111111\n",
      "> 99 | 0.770119845867157 | 0.3611111111111111\n",
      "> 100 | 0.7700215578079224 | 0.3611111111111111\n",
      "> Evaluation\n",
      "> Class Acc = 0.375\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9561688303947449 | 0.9842825755476952 | 0.9873310774564743\n",
      "> Confusion Matrix \n",
      "TN: 4.0 | FP: 151.0 \n",
      "FN: 9.0 | TP: 92.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1.0 | FP: 23.0 \n",
      "FN: 6.0 | TP: 58.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 3.0 | FP: 128.0 \n",
      "FN: 3.0 | TP: 34.0\n"
     ]
    }
   ],
   "source": [
    "for cv_seed in cv_seeds:\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        x, y, a, test_size=0.3, random_state=cv_seed)\n",
    "\n",
    "    train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "    train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # train below\n",
    "\n",
    "    model = UnfairLogisticRegression(xdim, ydim, batch_size)\n",
    "    ret = train_loop(model, train_data, epochs)\n",
    "    Y, A, Y_hat = evaluation(model, test_data)\n",
    "    \n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1  = compute_metrics(Y, A, Y_hat, adim=adim)\n",
    "    \n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    \n",
    "    tradeoff = []\n",
    "    \n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "    \n",
    "    result = ['UnfairLR-decay', cv_seed, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa82d8d9910>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD4CAYAAADSIzzWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAASuUlEQVR4nO3de7BddXXA8e9KQsJDMQmBNCYoQRClVlsGGR4VwSiEhwQKIw/FGGMjghCVlocwpVpwYFBQp2B7W5BYmCCCFqpoy1ApqOURARHCK4UBEoIBQsJbcs9d/eNu4BJy7z335OT+cna+H2bPPee39/mdNcxl3cXav713ZCaSpOE3onQAkrShMgFLUiEmYEkqxAQsSYWYgCWpkFHr/AtGT3aZhd7kpcdvKh2C1kMbTdg21naOVU891HTOacf3rQ0rYEkqZJ1XwJI0rHoapSNomglYUr00uktH0DQTsKRayewpHULTTMCS6qXHBCxJZVgBS1IhnoSTpEKsgCWpjHQVhCQV4kk4SSrEFoQkFdJBJ+G8F4Skesme5rdBRMTFEbEsIu5ew74TIyIjYkL1PiLiuxGxKCLuioidBpvfBCypXhrdzW+DuwSYvvpgRGwN7AM82md4P2D7apsDfG+wyU3Akuqlp6f5bRCZeSOwfA27zgdOAvre+nIG8IPsdTMwNiImDTS/PWBJtZK5bnvAETEDWJKZv4t4w+2EJwOP9Xm/uBpb2t9cJmBJ9TKEVRARMYfedsGrujKza4DjNwW+Sm/7Ya2ZgCXVyxDWAVfJtt+EuwbvAqYCr1a/U4DbI2IXYAmwdZ9jp1Rj/bIHLKle2rgK4k1TZ/4+M7fKzG0ycxt62ww7ZeYTwDXAp6vVELsCKzOz3/YDWAFLqpvGqrZNFRHzgb2ACRGxGDgjMy/q5/Brgf2BRcCLwKzB5jcBS6qXNl6KnJlHDrJ/mz6vEzhuKPObgCXVi5ciS1Ih3oxHkgoxAUtSGdnGk3DrmglYUr3YA5akQmxBSFIhVsCSVIgVsCQVYgUsSYV0+1RkSSrDCliSCrEHLEmFWAFLUiFWwJJUiBWwJBXiKghJKiRz8GPWEyZgSfViD1iSCjEBS1IhnoSTpEIajdIRNG1E6QAkqa16eprfBhERF0fEsoi4u8/YuRFxX0TcFRE/iYixffadGhGLIuL+iNh3sPlNwJLqpY0JGLgEmL7a2HXA+zLz/cADwKkAEbEjcATwp9VnLoyIkQNNbgKWVC/Z0/w22FSZNwLLVxv7r8x8dbHxzcCU6vUM4PLM/GNmPgwsAnYZaH4TsKRayZ5seouIORGxoM82Z4hf91ng59XrycBjffYtrsb65Uk4SfUyhGVomdkFdLXyNRFxGtANXNbK58EELKluhmEVRER8BjgQmJb52qV3S4Ct+xw2pRrrly0ISfXS3pNwbxIR04GTgIMy88U+u64BjoiIMRExFdgeuHWguayAJdVLG6+Ei4j5wF7AhIhYDJxB76qHMcB1EQFwc2Yek5n3RMQVwEJ6WxPHZeaA5bgJeJiMGDGCW27+OY8veYIZh8wsHY6G0enfOI8bf30r48eN5d8v/ScALrjoUq665heMG/s2AOZ+fiZ77r4LK1Y+y5dPO4u773uAg/f7GKedeGzJ0DtTG2/Gk5lHrmH4ogGOPws4q9n5TcDD5ITjP8d99z3I5m99a+lQNMwO3v9jHHXoQXz1H775hvGjDz+YWUcd9oax0aNHc/xfH82DDz3CooceGc4w66OD7gUxaA84It4TESdHxHer7eSIeO9wBFcXkydPYv/9pnHxxfNLh6ICdv7zP+Ntmzf3h3fTTTZmpw+8jzGjR6/jqGqsJ5vfChswAUfEycDlQNDbTL61ej0/Ik5Z9+HVw3nf+hqnnHomPR30l1nr3vyr/oNDPv0FTv/Geax89rnS4dRHo9H8VthgFfBs4IOZeXZmXlptZ9N7dcfs/j7Ud3FzT88L7Yy34xyw/0dZtuwpbr/j96VD0Xrk8EMO4OdXXMxVl1zAlluM59x//JfSIdVG9vQ0vZU2WALuAd6+hvFJ1b41ysyuzNw5M3ceMWKztYmv4+2++858/MB9WPTAzVx26YXsvfcezLvku6XDUmETxo9j5MiRjBgxgsMO2o+7Fz5QOqT66KAWxGAn4b4EXB8RD/L6JXbvALYDvrguA6uL004/m9NOPxuAD++5G1/58jHM/MwJhaNSaU8+tZwtJ4wH4Pr/+Q3bbfvOwhHVSF3uB5yZv4iId9Pbcnj1muYlwG2DrW+T1Otvzzib2+64ixUrnmXawZ/i2NlHc9sdd3H/gw9BwOQ/mcgZJ73+R3mfQ2fy/Asvsqq7m/++6Td0nX8W75pqgm7aelDZNityHT/AbtToyZ3zb0PD5qXHbyodgtZDG03YNtZ2jhf+7oimc85mX798rb9vbbgOWFK91KUFIUkdp4NaECZgSbWyPiwva5YJWFK9WAFLUiEmYEkqZD24xLhZJmBJtZJWwJJUiAlYkgpxFYQkFWIFLEmFmIAlqYxs2IKQpDI6qAIe9JlwktRJsieb3gYTERdHxLKIuLvP2PiIuC4iHqx+jqvGo3pu5qKIuCsidhpsfhOwpHpp7xMxLgGmrzZ2CnB9Zm4PXF+9B9gP2L7a5gDfG2xyE7CkeukZwjaIzLwRWL7a8AxgXvV6HnBwn/EfZK+bgbERMWmg+e0BS6qV7G7+JFxEzKG3Wn1VV2Z2DfKxiZm5tHr9BDCxej2Z1x/dBrC4GltKP0zAkuplCIsgqmQ7WMId6PMZES2f9TMBS6qVYbgXxB8iYlJmLq1aDMuq8SXA1n2Om1KN9csesKR6aWMPuB/XADOr1zOBq/uMf7paDbErsLJPq2KNrIAl1Uo7K+CImA/sBUyIiMXAGcDZwBURMRt4BPhEdfi1wP7AIuBFYNZg85uAJdVLGy+Ey8wj+9k1bQ3HJnDcUOY3AUuqlewuHUHzTMCSaqWDnkpvApZUMyZgSSrDCliSCjEBS1Ih2YjSITTNBCypVqyAJamQ7LEClqQirIAlqZBMK2BJKsIKWJIK6XEVhCSV4Uk4SSrEBCxJheQ6fyBG+5iAJdWKFbAkFeIyNEkqpOEqCEkqwwpYkgrppB6wj6WXVCuZzW+DiYgvR8Q9EXF3RMyPiI0jYmpE3BIRiyLihxExutVYTcCSaiV7oultIBExGTgB2Dkz3weMBI4AzgHOz8ztgGeA2a3GagKWVCuNnhFNb00YBWwSEaOATYGlwEeAK6v984CDW43VBCypVobSgoiIORGxoM825/V5cgnwTeBRehPvSuC3wIrM7K4OWwxMbjVWT8JJqpWeIayCyMwuoGtN+yJiHDADmAqsAH4ETG9DiK8xAUuqlTYuQ/so8HBmPgkQET8G9gDGRsSoqgqeAixp9QtsQUiqlTaugngU2DUiNo2IAKYBC4FfAodVx8wErm411nVeAW+60Zh1/RXqQMsP+2zpELQemnjDDWs9x1BaEAPJzFsi4krgdqAbuIPedsXPgMsj4sxq7KJWv8MWhKRaaXJ1Q1My8wzgjNWGHwJ2acf8JmBJtdJBd6M0AUuql3a1IIaDCVhSrXgzHkkqpIMeimwCllQviRWwJBXRbQtCksqwApakQuwBS1IhVsCSVIgVsCQV0rAClqQyOuiZnCZgSfXSYwUsSWV4Mx5JKsSTcJJUSE/YgpCkIhqlAxgCE7CkWnEVhCQV4ioISSrEVRCSVEgntSDa9/hQSVoP9AxhG0xEjI2IKyPivoi4NyJ2i4jxEXFdRDxY/RzXaqwmYEm10ojmtyZ8B/hFZr4H+ABwL3AKcH1mbg9cX71viQlYUq20qwKOiLcBewIXAWTmK5m5ApgBzKsOmwcc3GqsJmBJtTKUBBwRcyJiQZ9tTp+ppgJPAt+PiDsi4l8jYjNgYmYurY55ApjYaqyehJNUK0N5JFxmdgFd/eweBewEHJ+Zt0TEd1it3ZCZGREtL7ywApZUK208CbcYWJyZt1Tvr6Q3If8hIiYBVD+XtRqrCVhSrTSGsA0kM58AHouIHaqhacBC4BpgZjU2E7i61VhtQUiqlTavAz4euCwiRgMPAbPoLVyviIjZwCPAJ1qd3AQsqVbaeTvKzLwT2HkNu6a1Y34TsKRa8X7AklSI94KQpEI66V4QJmBJteIN2SWpkJ4OakKYgCXViifhJKmQzql/TcCSasYKWJIK6W793jjDzgQsqVY6J/2agCXVjC0ISSrEZWiSVEjnpF8TsKSasQUhSYU0OqgGNgFLqhUrYEkqJK2AJakMK2C9yReO/QwzZx1OEMy75IdceMH3S4ekAjY59FA2PfBAAF762c948corecsxxzBm993JVatoPP44z55zDvn884Uj7VydtAzNpyIPg/fu+G5mzjqcvfc8hN13PYB99/sI2277ztJhaZiNnDqVTQ88kKePOYanP/c5Ru+2GyMnT+aVBQt4etYsls+eTeOxx9jsqKNKh9rRcghbaSbgYbDDDu9iwW2/46WXXqbRaPDrm27h4zP2LR2Whtmod7yDVQsXwh//CI0Gq+68kzEf+hCvLFgAjd7biK9auJARW25ZONLO1k02vTUjIkZGxB0R8dPq/dSIuCUiFkXED6snJrfEBDwMFi58gN13/yDjx49lk002Zp9992LK5Emlw9Iw6374YTZ6//uJzTeHMWMYveuujNxqqzccs8n++/PKrbcWirAecgj/NGkucG+f9+cA52fmdsAzwOxWY225BxwRszJzjY3MiJgDzAEYM3oLRo/avNWvqYUH7v8/zj/vn/nJNfN48YWXuOuue2n0dNKpArVD49FHeWH+fMadey758st0L1pE9vk92OxTnyIbDV6+7rqCUXa+dv6XFRFTgAOAs4CvREQAHwFe7RPNA/4e+F4r869NBfy1/nZkZldm7pyZO2/oyfdV//aDK/jwX85gv32PYMWKlSx68OHSIamAl6+9luWf/zzPzJ1LPvccjcceA2Dj6dMZvdturDzzzMIRdr42V8DfBk7i9by+BbAiM7ur94uBya3GOmAFHBF39bcLmNjql26IJmy5BU89+TRTprydgw7al2l7/1XpkFRAjB1LrljBiK22Ysyee7L82GMZvcsubHbEESyfO7e3P6y1MpQKuO//rVe6MrOr2ncgsCwzfxsRe7UxxNcM1oKYCOxLb5+jrwB+sy4CqqtLL7uQ8ePHsqq7mxO/cgYrVz5XOiQVMPbrX2fE5puT3d089+1vk88/z1vnziU22ohx3/oW0Hsi7rnzziscaedqZPPrG6pk29XP7j2AgyJif2BjYHPgO8DYiBhVVcFTgCWtxjpYAv4p8JbMvHP1HRFxQ6tfuiGavs/hpUPQeuCZE05409jTn/xkgUjqq13rgDPzVOBUgKoC/pvM/GRE/Ag4DLgcmAlc3ep3DNgDzszZmfmrfva5WFHSemcdrIJY3cn0npBbRG9P+KJWJ/JKOEm1si7WF2XmDcAN1euHgF3aMa8JWFKtdNKlyCZgSbXi3dAkqZChrIIozQQsqVZsQUhSIZ10kb8JWFKt2AOWpEJsQUhSIelJOEkqw8fSS1IhtiAkqRBbEJJUiBWwJBXiMjRJKsRLkSWpEFsQklSICViSCnEVhCQVYgUsSYW4CkKSCmlk59yQ0gQsqVY6qQc84GPpJanT9JBNbwOJiK0j4pcRsTAi7omIudX4+Ii4LiIerH6OazVWE7CkWskh/DOIbuDEzNwR2BU4LiJ2BE4Brs/M7YHrq/ctMQFLqpWezKa3gWTm0sy8vXr9HHAvMBmYAcyrDpsHHNxqrCZgSbUylAo4IuZExII+25w1zRkR2wB/AdwCTMzMpdWuJ4CJrcbqSThJtTKUVRCZ2QV0DXRMRLwFuAr4UmY+GxF9P58R0fJZPxOwpFoZrLUwFBGxEb3J97LM/HE1/IeImJSZSyNiErCs1fltQUiqlXadhIveUvci4N7MPK/PrmuAmdXrmcDVrcZqBSypVtpYAe8BHA38PiLurMa+CpwNXBERs4FHgE+0+gUmYEm10q5LkTPzV0D0s3taO77DBCypVhrZKB1C00zAkmqlky5FNgFLqhVvRylJhVgBS1Ih7VwHvK6ZgCXVijdkl6RCvCG7JBViD1iSCrEHLEmFWAFLUiGuA5akQqyAJakQV0FIUiGehJOkQmxBSFIhXgknSYVYAUtSIZ3UA45O+mvR6SJiTvUYbOk1/l5suHwq8vCaUzoArZf8vdhAmYAlqRATsCQVYgIeXvb5tCb+XmygPAknSYVYAUtSISZgSSrEBDxMImJ6RNwfEYsi4pTS8ai8iLg4IpZFxN2lY1EZJuBhEBEjgQuA/YAdgSMjYseyUWk9cAkwvXQQKscEPDx2ARZl5kOZ+QpwOTCjcEwqLDNvBJaXjkPlmICHx2TgsT7vF1djkjZgJmBJKsQEPDyWAFv3eT+lGpO0ATMBD4/bgO0jYmpEjAaOAK4pHJOkwkzAwyAzu4EvAv8J3AtckZn3lI1KpUXEfOB/gR0iYnFEzC4dk4aXlyJLUiFWwJJUiAlYkgoxAUtSISZgSSrEBCxJhZiAJakQE7AkFfL/hYyDo/qNe6oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.9008842706680298 | 0.3645833333333333\n",
      "> 2 | 0.8850905895233154 | 0.34375\n",
      "> 3 | 0.87254798412323 | 0.3420138888888889\n",
      "> 4 | 0.861292839050293 | 0.3454861111111111\n",
      "> 5 | 0.849582314491272 | 0.3576388888888889\n",
      "> 6 | 0.8370593786239624 | 0.3645833333333333\n",
      "> 7 | 0.8250463008880615 | 0.3663194444444444\n",
      "> 8 | 0.8138713836669922 | 0.3663194444444444\n",
      "> 9 | 0.8032193183898926 | 0.3680555555555556\n",
      "> 10 | 0.79282146692276 | 0.375\n",
      "> 11 | 0.7827056646347046 | 0.3767361111111111\n",
      "> 12 | 0.7729858160018921 | 0.3819444444444444\n",
      "> 13 | 0.7636600732803345 | 0.3923611111111111\n",
      "> 14 | 0.754673957824707 | 0.4097222222222222\n",
      "> 15 | 0.7460082173347473 | 0.4114583333333333\n",
      "> 16 | 0.7376683950424194 | 0.4131944444444444\n",
      "> 17 | 0.7296465635299683 | 0.4236111111111111\n",
      "> 18 | 0.7219256162643433 | 0.4253472222222222\n",
      "> 19 | 0.7144930362701416 | 0.4340277777777778\n",
      "> 20 | 0.7073391079902649 | 0.4340277777777778\n",
      "> 21 | 0.7004526853561401 | 0.4513888888888889\n",
      "> 22 | 0.6938217282295227 | 0.4618055555555556\n",
      "> 23 | 0.6874351501464844 | 0.4774305555555556\n",
      "> 24 | 0.6812824010848999 | 0.5121527777777778\n",
      "> 25 | 0.675352931022644 | 0.5642361111111112\n",
      "> 26 | 0.6696364879608154 | 0.6163194444444444\n",
      "> 27 | 0.6641234159469604 | 0.6597222222222222\n",
      "> 28 | 0.6588042974472046 | 0.6753472222222222\n",
      "> 29 | 0.6536701321601868 | 0.6927083333333334\n",
      "> 30 | 0.6487125158309937 | 0.6961805555555556\n",
      "> 31 | 0.6439230442047119 | 0.7100694444444444\n",
      "> 32 | 0.6392939686775208 | 0.7204861111111112\n",
      "> 33 | 0.6348179578781128 | 0.7239583333333334\n",
      "> 34 | 0.6304877996444702 | 0.7361111111111112\n",
      "> 35 | 0.6262969970703125 | 0.7413194444444444\n",
      "> 36 | 0.6222389936447144 | 0.7482638888888888\n",
      "> 37 | 0.6183079481124878 | 0.7413194444444444\n",
      "> 38 | 0.6144981384277344 | 0.7361111111111112\n",
      "> 39 | 0.6108039617538452 | 0.7430555555555556\n",
      "> 40 | 0.6072204113006592 | 0.7465277777777778\n",
      "> 41 | 0.6037425994873047 | 0.7447916666666666\n",
      "> 42 | 0.6003658771514893 | 0.7534722222222222\n",
      "> 43 | 0.5970858931541443 | 0.7534722222222222\n",
      "> 44 | 0.5938984751701355 | 0.7552083333333334\n",
      "> 45 | 0.5907996892929077 | 0.7638888888888888\n",
      "> 46 | 0.5877857208251953 | 0.7673611111111112\n",
      "> 47 | 0.5848531126976013 | 0.7673611111111112\n",
      "> 48 | 0.5819984674453735 | 0.7673611111111112\n",
      "> 49 | 0.5792184472084045 | 0.765625\n",
      "> 50 | 0.5765101909637451 | 0.7673611111111112\n",
      "> 51 | 0.5738707780838013 | 0.7708333333333334\n",
      "> 52 | 0.571297287940979 | 0.7743055555555556\n",
      "> 53 | 0.5687872171401978 | 0.7760416666666666\n",
      "> 54 | 0.5663381814956665 | 0.7777777777777778\n",
      "> 55 | 0.5639475584030151 | 0.7777777777777778\n",
      "> 56 | 0.5616133213043213 | 0.7725694444444444\n",
      "> 57 | 0.5593331456184387 | 0.7708333333333334\n",
      "> 58 | 0.5571050643920898 | 0.7708333333333334\n",
      "> 59 | 0.5549270510673523 | 0.7690972222222222\n",
      "> 60 | 0.5527973175048828 | 0.7690972222222222\n",
      "> 61 | 0.5507140159606934 | 0.7690972222222222\n",
      "> 62 | 0.548675537109375 | 0.7708333333333334\n",
      "> 63 | 0.5466800928115845 | 0.7673611111111112\n",
      "> 64 | 0.5447263717651367 | 0.765625\n",
      "> 65 | 0.542812705039978 | 0.765625\n",
      "> 66 | 0.5409377813339233 | 0.765625\n",
      "> 67 | 0.539100170135498 | 0.7690972222222222\n",
      "> 68 | 0.5372986793518066 | 0.7725694444444444\n",
      "> 69 | 0.5355319976806641 | 0.7743055555555556\n",
      "> 70 | 0.5337989330291748 | 0.7743055555555556\n",
      "> 71 | 0.5320984721183777 | 0.7725694444444444\n",
      "> 72 | 0.5304293632507324 | 0.7725694444444444\n",
      "> 73 | 0.5287907123565674 | 0.7725694444444444\n",
      "> 74 | 0.5271815061569214 | 0.7743055555555556\n",
      "> 75 | 0.5256007313728333 | 0.7760416666666666\n",
      "> 76 | 0.5240474939346313 | 0.7777777777777778\n",
      "> 77 | 0.5225210189819336 | 0.7777777777777778\n",
      "> 78 | 0.5210203528404236 | 0.7777777777777778\n",
      "> 79 | 0.5195447206497192 | 0.78125\n",
      "> 80 | 0.518093466758728 | 0.7795138888888888\n",
      "> 81 | 0.5166656970977783 | 0.78125\n",
      "> 82 | 0.5152608156204224 | 0.7829861111111112\n",
      "> 83 | 0.5138780474662781 | 0.7847222222222222\n",
      "> 84 | 0.5125167369842529 | 0.7847222222222222\n",
      "> 85 | 0.5111764669418335 | 0.7847222222222222\n",
      "> 86 | 0.5098563432693481 | 0.7864583333333334\n",
      "> 87 | 0.5085560083389282 | 0.7899305555555556\n",
      "> 88 | 0.507274866104126 | 0.7899305555555556\n",
      "> 89 | 0.5060123205184937 | 0.7881944444444444\n",
      "> 90 | 0.5047677755355835 | 0.7899305555555556\n",
      "> 91 | 0.5035409927368164 | 0.7881944444444444\n",
      "> 92 | 0.5023313164710999 | 0.7899305555555556\n",
      "> 93 | 0.5011383295059204 | 0.7916666666666666\n",
      "> 94 | 0.4999614953994751 | 0.7916666666666666\n",
      "> 95 | 0.4988005459308624 | 0.7916666666666666\n",
      "> 96 | 0.4976550340652466 | 0.7916666666666666\n",
      "> 97 | 0.496524453163147 | 0.7916666666666666\n",
      "> 98 | 0.4954085946083069 | 0.7934027777777778\n",
      "> 99 | 0.4943069815635681 | 0.7916666666666666\n",
      "> 100 | 0.4932193160057068 | 0.7916666666666666\n",
      "> Evaluation\n",
      "> Class Acc = 0.71484375\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.5611721277236938 | 0.7036492675542831 | 0.5409523844718933\n",
      "> Confusion Matrix \n",
      "TN: 136.0 | FP: 10.0 \n",
      "FN: 63.0 | TP: 47.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 13.0 | FP: 3.0 \n",
      "FN: 32.0 | TP: 43.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 123.0 | FP: 7.0 \n",
      "FN: 31.0 | TP: 4.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.9671807289123535 | 0.3767361111111111\n",
      "> 2 | 0.9240186214447021 | 0.3645833333333333\n",
      "> 3 | 0.9028061628341675 | 0.3559027777777778\n",
      "> 4 | 0.8906269073486328 | 0.3611111111111111\n",
      "> 5 | 0.8806120157241821 | 0.3715277777777778\n",
      "> 6 | 0.869415283203125 | 0.3715277777777778\n",
      "> 7 | 0.8564128875732422 | 0.375\n",
      "> 8 | 0.8425459861755371 | 0.3767361111111111\n",
      "> 9 | 0.8289237022399902 | 0.3767361111111111\n",
      "> 10 | 0.816096842288971 | 0.3819444444444444\n",
      "> 11 | 0.8040799498558044 | 0.3871527777777778\n",
      "> 12 | 0.7926618456840515 | 0.3940972222222222\n",
      "> 13 | 0.781654953956604 | 0.3958333333333333\n",
      "> 14 | 0.7709845304489136 | 0.4027777777777778\n",
      "> 15 | 0.7606589198112488 | 0.4166666666666667\n",
      "> 16 | 0.7507076263427734 | 0.4236111111111111\n",
      "> 17 | 0.7411423921585083 | 0.4322916666666667\n",
      "> 18 | 0.731952428817749 | 0.4357638888888889\n",
      "> 19 | 0.7231169939041138 | 0.4513888888888889\n",
      "> 20 | 0.714617133140564 | 0.4618055555555556\n",
      "> 21 | 0.7064390182495117 | 0.4670138888888889\n",
      "> 22 | 0.6985719799995422 | 0.4861111111111111\n",
      "> 23 | 0.6910058259963989 | 0.5017361111111112\n",
      "> 24 | 0.6837292909622192 | 0.5590277777777778\n",
      "> 25 | 0.6767303347587585 | 0.6006944444444444\n",
      "> 26 | 0.6699970960617065 | 0.6336805555555556\n",
      "> 27 | 0.6635180115699768 | 0.6371527777777778\n",
      "> 28 | 0.6572821140289307 | 0.6631944444444444\n",
      "> 29 | 0.6512787342071533 | 0.6875\n",
      "> 30 | 0.6454975605010986 | 0.7048611111111112\n",
      "> 31 | 0.6399284601211548 | 0.7170138888888888\n",
      "> 32 | 0.6345615386962891 | 0.734375\n",
      "> 33 | 0.6293875575065613 | 0.7430555555555556\n",
      "> 34 | 0.6243973970413208 | 0.7465277777777778\n",
      "> 35 | 0.6195824146270752 | 0.7517361111111112\n",
      "> 36 | 0.6149343252182007 | 0.7552083333333334\n",
      "> 37 | 0.6104452610015869 | 0.7569444444444444\n",
      "> 38 | 0.606107771396637 | 0.7604166666666666\n",
      "> 39 | 0.6019145250320435 | 0.7604166666666666\n",
      "> 40 | 0.5978586673736572 | 0.7586805555555556\n",
      "> 41 | 0.5939339399337769 | 0.7638888888888888\n",
      "> 42 | 0.5901339054107666 | 0.7673611111111112\n",
      "> 43 | 0.586452841758728 | 0.7725694444444444\n",
      "> 44 | 0.5828853249549866 | 0.7760416666666666\n",
      "> 45 | 0.5794259309768677 | 0.7777777777777778\n",
      "> 46 | 0.576069712638855 | 0.78125\n",
      "> 47 | 0.5728119611740112 | 0.7777777777777778\n",
      "> 48 | 0.569648265838623 | 0.7760416666666666\n",
      "> 49 | 0.5665743947029114 | 0.7743055555555556\n",
      "> 50 | 0.5635863542556763 | 0.7777777777777778\n",
      "> 51 | 0.5606803894042969 | 0.7777777777777778\n",
      "> 52 | 0.5578528642654419 | 0.7760416666666666\n",
      "> 53 | 0.5551003217697144 | 0.7777777777777778\n",
      "> 54 | 0.5524198412895203 | 0.7777777777777778\n",
      "> 55 | 0.549808144569397 | 0.78125\n",
      "> 56 | 0.54726243019104 | 0.78125\n",
      "> 57 | 0.5447800159454346 | 0.7829861111111112\n",
      "> 58 | 0.5423582792282104 | 0.7829861111111112\n",
      "> 59 | 0.5399948358535767 | 0.7847222222222222\n",
      "> 60 | 0.5376873016357422 | 0.7847222222222222\n",
      "> 61 | 0.5354336500167847 | 0.7847222222222222\n",
      "> 62 | 0.5332317352294922 | 0.7864583333333334\n",
      "> 63 | 0.5310795307159424 | 0.7864583333333334\n",
      "> 64 | 0.5289751887321472 | 0.7881944444444444\n",
      "> 65 | 0.5269169807434082 | 0.7899305555555556\n",
      "> 66 | 0.5249031782150269 | 0.7916666666666666\n",
      "> 67 | 0.5229322910308838 | 0.7934027777777778\n",
      "> 68 | 0.5210026502609253 | 0.796875\n",
      "> 69 | 0.5191129446029663 | 0.8020833333333334\n",
      "> 70 | 0.5172617435455322 | 0.8038194444444444\n",
      "> 71 | 0.515447735786438 | 0.8055555555555556\n",
      "> 72 | 0.5136697292327881 | 0.8038194444444444\n",
      "> 73 | 0.5119264125823975 | 0.8038194444444444\n",
      "> 74 | 0.5102167725563049 | 0.8072916666666666\n",
      "> 75 | 0.5085397362709045 | 0.8090277777777778\n",
      "> 76 | 0.5068942308425903 | 0.8107638888888888\n",
      "> 77 | 0.5052792429924011 | 0.8107638888888888\n",
      "> 78 | 0.5036939382553101 | 0.8107638888888888\n",
      "> 79 | 0.502137303352356 | 0.8090277777777778\n",
      "> 80 | 0.5006086230278015 | 0.8090277777777778\n",
      "> 81 | 0.49910688400268555 | 0.8090277777777778\n",
      "> 82 | 0.4976314604282379 | 0.8090277777777778\n",
      "> 83 | 0.49618154764175415 | 0.8090277777777778\n",
      "> 84 | 0.49475640058517456 | 0.8107638888888888\n",
      "> 85 | 0.4933554530143738 | 0.8107638888888888\n",
      "> 86 | 0.49197784066200256 | 0.8090277777777778\n",
      "> 87 | 0.4906230568885803 | 0.8090277777777778\n",
      "> 88 | 0.4892904758453369 | 0.8090277777777778\n",
      "> 89 | 0.4879795014858246 | 0.8072916666666666\n",
      "> 90 | 0.48668956756591797 | 0.8072916666666666\n",
      "> 91 | 0.4854201376438141 | 0.8072916666666666\n",
      "> 92 | 0.48417073488235474 | 0.8072916666666666\n",
      "> 93 | 0.4829407036304474 | 0.8090277777777778\n",
      "> 94 | 0.48172974586486816 | 0.8090277777777778\n",
      "> 95 | 0.4805372953414917 | 0.8090277777777778\n",
      "> 96 | 0.4793629050254822 | 0.8090277777777778\n",
      "> 97 | 0.47820615768432617 | 0.8090277777777778\n",
      "> 98 | 0.47706663608551025 | 0.8107638888888888\n",
      "> 99 | 0.47594401240348816 | 0.8142361111111112\n",
      "> 100 | 0.4748377501964569 | 0.8159722222222222\n",
      "> Evaluation\n",
      "> Class Acc = 0.80078125\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.49548566341400146 | 0.683811329305172 | 0.5896516442298889\n",
      "> Confusion Matrix \n",
      "TN: 148.0 | FP: 15.0 \n",
      "FN: 36.0 | TP: 57.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 18.0 | FP: 7.0 \n",
      "FN: 15.0 | TP: 46.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 130.0 | FP: 8.0 \n",
      "FN: 21.0 | TP: 11.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.9321149587631226 | 0.3854166666666667\n",
      "> 2 | 0.9140276312828064 | 0.359375\n",
      "> 3 | 0.8986384868621826 | 0.3697916666666667\n",
      "> 4 | 0.8836816549301147 | 0.3871527777777778\n",
      "> 5 | 0.867021918296814 | 0.3836805555555556\n",
      "> 6 | 0.8510679602622986 | 0.3836805555555556\n",
      "> 7 | 0.8364613056182861 | 0.3940972222222222\n",
      "> 8 | 0.8225511312484741 | 0.3958333333333333\n",
      "> 9 | 0.8088769316673279 | 0.3940972222222222\n",
      "> 10 | 0.7956323027610779 | 0.3958333333333333\n",
      "> 11 | 0.783012866973877 | 0.3958333333333333\n",
      "> 12 | 0.7709341645240784 | 0.3993055555555556\n",
      "> 13 | 0.7592954039573669 | 0.4097222222222222\n",
      "> 14 | 0.7481074333190918 | 0.4166666666666667\n",
      "> 15 | 0.7373883724212646 | 0.4253472222222222\n",
      "> 16 | 0.7271111011505127 | 0.4322916666666667\n",
      "> 17 | 0.7172468900680542 | 0.4392361111111111\n",
      "> 18 | 0.7077847719192505 | 0.4479166666666667\n",
      "> 19 | 0.6987133026123047 | 0.4618055555555556\n",
      "> 20 | 0.6900138854980469 | 0.4652777777777778\n",
      "> 21 | 0.6816695928573608 | 0.4774305555555556\n",
      "> 22 | 0.6736658811569214 | 0.4930555555555556\n",
      "> 23 | 0.6659884452819824 | 0.5208333333333334\n",
      "> 24 | 0.6586219668388367 | 0.5677083333333334\n",
      "> 25 | 0.6515523195266724 | 0.6180555555555556\n",
      "> 26 | 0.6447659730911255 | 0.6631944444444444\n",
      "> 27 | 0.6382496356964111 | 0.6822916666666666\n",
      "> 28 | 0.6319907903671265 | 0.6944444444444444\n",
      "> 29 | 0.6259770393371582 | 0.7152777777777778\n",
      "> 30 | 0.6201969385147095 | 0.7326388888888888\n",
      "> 31 | 0.614639163017273 | 0.7482638888888888\n",
      "> 32 | 0.6092932224273682 | 0.7586805555555556\n",
      "> 33 | 0.6041488647460938 | 0.7621527777777778\n",
      "> 34 | 0.5991963744163513 | 0.7638888888888888\n",
      "> 35 | 0.5944266319274902 | 0.7673611111111112\n",
      "> 36 | 0.5898308157920837 | 0.7725694444444444\n",
      "> 37 | 0.5854005813598633 | 0.765625\n",
      "> 38 | 0.5811280608177185 | 0.765625\n",
      "> 39 | 0.5770056247711182 | 0.7621527777777778\n",
      "> 40 | 0.573026180267334 | 0.765625\n",
      "> 41 | 0.5691829919815063 | 0.765625\n",
      "> 42 | 0.5654693841934204 | 0.7690972222222222\n",
      "> 43 | 0.5618794560432434 | 0.7638888888888888\n",
      "> 44 | 0.5584073066711426 | 0.7638888888888888\n",
      "> 45 | 0.5550473928451538 | 0.7621527777777778\n",
      "> 46 | 0.5517945289611816 | 0.7586805555555556\n",
      "> 47 | 0.5486437082290649 | 0.7604166666666666\n",
      "> 48 | 0.5455902814865112 | 0.7638888888888888\n",
      "> 49 | 0.5426297187805176 | 0.7638888888888888\n",
      "> 50 | 0.5397577285766602 | 0.7638888888888888\n",
      "> 51 | 0.5369704365730286 | 0.7638888888888888\n",
      "> 52 | 0.5342638492584229 | 0.7673611111111112\n",
      "> 53 | 0.5316344499588013 | 0.765625\n",
      "> 54 | 0.5290787220001221 | 0.765625\n",
      "> 55 | 0.5265935063362122 | 0.765625\n",
      "> 56 | 0.5241755247116089 | 0.7690972222222222\n",
      "> 57 | 0.5218220949172974 | 0.7673611111111112\n",
      "> 58 | 0.5195301175117493 | 0.7638888888888888\n",
      "> 59 | 0.517297089099884 | 0.7638888888888888\n",
      "> 60 | 0.5151204466819763 | 0.7638888888888888\n",
      "> 61 | 0.5129978656768799 | 0.7638888888888888\n",
      "> 62 | 0.5109269618988037 | 0.7690972222222222\n",
      "> 63 | 0.5089056491851807 | 0.7690972222222222\n",
      "> 64 | 0.5069317817687988 | 0.7708333333333334\n",
      "> 65 | 0.5050034523010254 | 0.7708333333333334\n",
      "> 66 | 0.5031188726425171 | 0.7725694444444444\n",
      "> 67 | 0.5012761354446411 | 0.7743055555555556\n",
      "> 68 | 0.4994736909866333 | 0.7760416666666666\n",
      "> 69 | 0.49770987033843994 | 0.7760416666666666\n",
      "> 70 | 0.4959830939769745 | 0.7777777777777778\n",
      "> 71 | 0.4942919909954071 | 0.7777777777777778\n",
      "> 72 | 0.49263516068458557 | 0.7777777777777778\n",
      "> 73 | 0.49101126194000244 | 0.7777777777777778\n",
      "> 74 | 0.4894190728664398 | 0.7795138888888888\n",
      "> 75 | 0.4878573715686798 | 0.7795138888888888\n",
      "> 76 | 0.48632505536079407 | 0.7777777777777778\n",
      "> 77 | 0.48482102155685425 | 0.7777777777777778\n",
      "> 78 | 0.4833441972732544 | 0.7777777777777778\n",
      "> 79 | 0.4818936586380005 | 0.7777777777777778\n",
      "> 80 | 0.48046839237213135 | 0.7777777777777778\n",
      "> 81 | 0.4790676236152649 | 0.7777777777777778\n",
      "> 82 | 0.47769033908843994 | 0.7777777777777778\n",
      "> 83 | 0.4763358533382416 | 0.7795138888888888\n",
      "> 84 | 0.47500336170196533 | 0.78125\n",
      "> 85 | 0.47369205951690674 | 0.7795138888888888\n",
      "> 86 | 0.4724012613296509 | 0.7795138888888888\n",
      "> 87 | 0.4711303114891052 | 0.7829861111111112\n",
      "> 88 | 0.46987849473953247 | 0.7847222222222222\n",
      "> 89 | 0.468645304441452 | 0.7881944444444444\n",
      "> 90 | 0.4674299955368042 | 0.7899305555555556\n",
      "> 91 | 0.46623218059539795 | 0.7899305555555556\n",
      "> 92 | 0.4650511145591736 | 0.7881944444444444\n",
      "> 93 | 0.4638864994049072 | 0.7881944444444444\n",
      "> 94 | 0.46273767948150635 | 0.7899305555555556\n",
      "> 95 | 0.46160417795181274 | 0.7899305555555556\n",
      "> 96 | 0.460485577583313 | 0.7899305555555556\n",
      "> 97 | 0.45938146114349365 | 0.7881944444444444\n",
      "> 98 | 0.45829135179519653 | 0.7881944444444444\n",
      "> 99 | 0.45721495151519775 | 0.7899305555555556\n",
      "> 100 | 0.4561518430709839 | 0.7881944444444444\n",
      "> Evaluation\n",
      "> Class Acc = 0.75390625\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.522421658039093 | 0.6504642814397812 | 0.6180281341075897\n",
      "> Confusion Matrix \n",
      "TN: 139.0 | FP: 21.0 \n",
      "FN: 42.0 | TP: 54.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 17.0 | FP: 11.0 \n",
      "FN: 24.0 | TP: 47.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 122.0 | FP: 10.0 \n",
      "FN: 18.0 | TP: 7.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.9255760312080383 | 0.3836805555555556\n",
      "> 2 | 0.9080905914306641 | 0.3697916666666667\n",
      "> 3 | 0.8936542272567749 | 0.375\n",
      "> 4 | 0.8801411390304565 | 0.3767361111111111\n",
      "> 5 | 0.8658671379089355 | 0.3767361111111111\n",
      "> 6 | 0.8511099815368652 | 0.3836805555555556\n",
      "> 7 | 0.8369409441947937 | 0.3923611111111111\n",
      "> 8 | 0.8236734867095947 | 0.3888888888888889\n",
      "> 9 | 0.8110849857330322 | 0.3888888888888889\n",
      "> 10 | 0.7989155650138855 | 0.3923611111111111\n",
      "> 11 | 0.7871004939079285 | 0.3940972222222222\n",
      "> 12 | 0.7757079005241394 | 0.3975694444444444\n",
      "> 13 | 0.7647838592529297 | 0.4045138888888889\n",
      "> 14 | 0.7543095350265503 | 0.4114583333333333\n",
      "> 15 | 0.7442444562911987 | 0.4184027777777778\n",
      "> 16 | 0.7345650792121887 | 0.4236111111111111\n",
      "> 17 | 0.7252629995346069 | 0.4322916666666667\n",
      "> 18 | 0.7163301706314087 | 0.4375\n",
      "> 19 | 0.7077522277832031 | 0.4461805555555556\n",
      "> 20 | 0.6995124220848083 | 0.4565972222222222\n",
      "> 21 | 0.6915957927703857 | 0.4600694444444444\n",
      "> 22 | 0.6839890480041504 | 0.4670138888888889\n",
      "> 23 | 0.6766795516014099 | 0.5104166666666666\n",
      "> 24 | 0.669654130935669 | 0.5677083333333334\n",
      "> 25 | 0.6629001498222351 | 0.6267361111111112\n",
      "> 26 | 0.6564050912857056 | 0.6545138888888888\n",
      "> 27 | 0.6501573324203491 | 0.671875\n",
      "> 28 | 0.6441457271575928 | 0.6788194444444444\n",
      "> 29 | 0.6383594274520874 | 0.7048611111111112\n",
      "> 30 | 0.632787823677063 | 0.7256944444444444\n",
      "> 31 | 0.6274212002754211 | 0.734375\n",
      "> 32 | 0.6222498416900635 | 0.7395833333333334\n",
      "> 33 | 0.6172648668289185 | 0.7430555555555556\n",
      "> 34 | 0.6124574542045593 | 0.7447916666666666\n",
      "> 35 | 0.6078193783760071 | 0.7482638888888888\n",
      "> 36 | 0.6033428311347961 | 0.7517361111111112\n",
      "> 37 | 0.59902024269104 | 0.7552083333333334\n",
      "> 38 | 0.5948444604873657 | 0.7604166666666666\n",
      "> 39 | 0.5908088684082031 | 0.7621527777777778\n",
      "> 40 | 0.5869068503379822 | 0.7673611111111112\n",
      "> 41 | 0.5831323862075806 | 0.7638888888888888\n",
      "> 42 | 0.5794795155525208 | 0.7690972222222222\n",
      "> 43 | 0.575942873954773 | 0.7777777777777778\n",
      "> 44 | 0.5725170969963074 | 0.7795138888888888\n",
      "> 45 | 0.5691972970962524 | 0.78125\n",
      "> 46 | 0.5659786462783813 | 0.7864583333333334\n",
      "> 47 | 0.5628566145896912 | 0.7916666666666666\n",
      "> 48 | 0.5598270893096924 | 0.7934027777777778\n",
      "> 49 | 0.5568859577178955 | 0.7934027777777778\n",
      "> 50 | 0.5540292263031006 | 0.7934027777777778\n",
      "> 51 | 0.5512534379959106 | 0.7951388888888888\n",
      "> 52 | 0.5485550165176392 | 0.7986111111111112\n",
      "> 53 | 0.5459307432174683 | 0.7951388888888888\n",
      "> 54 | 0.5433774590492249 | 0.796875\n",
      "> 55 | 0.5408921241760254 | 0.7951388888888888\n",
      "> 56 | 0.538472056388855 | 0.7934027777777778\n",
      "> 57 | 0.5361144542694092 | 0.7951388888888888\n",
      "> 58 | 0.5338168740272522 | 0.796875\n",
      "> 59 | 0.5315768718719482 | 0.7986111111111112\n",
      "> 60 | 0.5293921232223511 | 0.8003472222222222\n",
      "> 61 | 0.527260422706604 | 0.8020833333333334\n",
      "> 62 | 0.5251798629760742 | 0.8020833333333334\n",
      "> 63 | 0.523148238658905 | 0.8055555555555556\n",
      "> 64 | 0.521163821220398 | 0.8055555555555556\n",
      "> 65 | 0.5192248821258545 | 0.8055555555555556\n",
      "> 66 | 0.5173294544219971 | 0.8055555555555556\n",
      "> 67 | 0.5154762864112854 | 0.8055555555555556\n",
      "> 68 | 0.5136635303497314 | 0.8038194444444444\n",
      "> 69 | 0.5118899345397949 | 0.8038194444444444\n",
      "> 70 | 0.5101540684700012 | 0.8038194444444444\n",
      "> 71 | 0.508454442024231 | 0.8020833333333334\n",
      "> 72 | 0.5067899823188782 | 0.7986111111111112\n",
      "> 73 | 0.5051593780517578 | 0.7986111111111112\n",
      "> 74 | 0.5035615563392639 | 0.8003472222222222\n",
      "> 75 | 0.501995325088501 | 0.8003472222222222\n",
      "> 76 | 0.5004596710205078 | 0.8003472222222222\n",
      "> 77 | 0.49895358085632324 | 0.8003472222222222\n",
      "> 78 | 0.49747616052627563 | 0.8003472222222222\n",
      "> 79 | 0.49602648615837097 | 0.8003472222222222\n",
      "> 80 | 0.49460360407829285 | 0.8003472222222222\n",
      "> 81 | 0.49320676922798157 | 0.7986111111111112\n",
      "> 82 | 0.4918350875377655 | 0.7986111111111112\n",
      "> 83 | 0.4904879331588745 | 0.7986111111111112\n",
      "> 84 | 0.48916441202163696 | 0.8003472222222222\n",
      "> 85 | 0.4878639578819275 | 0.8003472222222222\n",
      "> 86 | 0.486585795879364 | 0.8003472222222222\n",
      "> 87 | 0.48532938957214355 | 0.8038194444444444\n",
      "> 88 | 0.4840940535068512 | 0.8055555555555556\n",
      "> 89 | 0.4828791916370392 | 0.8072916666666666\n",
      "> 90 | 0.48168423771858215 | 0.8072916666666666\n",
      "> 91 | 0.4805086851119995 | 0.8072916666666666\n",
      "> 92 | 0.4793519973754883 | 0.8072916666666666\n",
      "> 93 | 0.4782136082649231 | 0.8072916666666666\n",
      "> 94 | 0.4770931303501129 | 0.8072916666666666\n",
      "> 95 | 0.4759899973869324 | 0.8072916666666666\n",
      "> 96 | 0.4749039113521576 | 0.8055555555555556\n",
      "> 97 | 0.4738343060016632 | 0.8055555555555556\n",
      "> 98 | 0.47278082370758057 | 0.8055555555555556\n",
      "> 99 | 0.47174304723739624 | 0.8072916666666666\n",
      "> 100 | 0.4707205891609192 | 0.8072916666666666\n",
      "> Evaluation\n",
      "> Class Acc = 0.75390625\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.5753045082092285 | 0.735783189535141 | 0.5568181872367859\n",
      "> Confusion Matrix \n",
      "TN: 147.0 | FP: 12.0 \n",
      "FN: 51.0 | TP: 46.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 17.0 | FP: 3.0 \n",
      "FN: 24.0 | TP: 40.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 130.0 | FP: 9.0 \n",
      "FN: 27.0 | TP: 6.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.8202025294303894 | 0.375\n",
      "> 2 | 0.8030803799629211 | 0.3559027777777778\n",
      "> 3 | 0.7923924922943115 | 0.3541666666666667\n",
      "> 4 | 0.7825682163238525 | 0.3628472222222222\n",
      "> 5 | 0.7733221650123596 | 0.3645833333333333\n",
      "> 6 | 0.7640464305877686 | 0.3628472222222222\n",
      "> 7 | 0.7547200918197632 | 0.3645833333333333\n",
      "> 8 | 0.7457659840583801 | 0.3663194444444444\n",
      "> 9 | 0.7373475432395935 | 0.375\n",
      "> 10 | 0.7294071316719055 | 0.3767361111111111\n",
      "> 11 | 0.721863865852356 | 0.3819444444444444\n",
      "> 12 | 0.7146735191345215 | 0.3871527777777778\n",
      "> 13 | 0.7078251838684082 | 0.4010416666666667\n",
      "> 14 | 0.7013162970542908 | 0.4045138888888889\n",
      "> 15 | 0.6951355934143066 | 0.4166666666666667\n",
      "> 16 | 0.6892653703689575 | 0.4201388888888889\n",
      "> 17 | 0.6836881041526794 | 0.4357638888888889\n",
      "> 18 | 0.6783877611160278 | 0.4427083333333333\n",
      "> 19 | 0.6733496189117432 | 0.4548611111111111\n",
      "> 20 | 0.6685591340065002 | 0.4722222222222222\n",
      "> 21 | 0.6640018820762634 | 0.4722222222222222\n",
      "> 22 | 0.6596641540527344 | 0.4826388888888889\n",
      "> 23 | 0.655532956123352 | 0.4982638888888889\n",
      "> 24 | 0.6515960097312927 | 0.5364583333333334\n",
      "> 25 | 0.6478414535522461 | 0.5746527777777778\n",
      "> 26 | 0.6442580223083496 | 0.6128472222222222\n",
      "> 27 | 0.640835165977478 | 0.6388888888888888\n",
      "> 28 | 0.6375628709793091 | 0.6597222222222222\n",
      "> 29 | 0.6344319581985474 | 0.6788194444444444\n",
      "> 30 | 0.631433367729187 | 0.6909722222222222\n",
      "> 31 | 0.6285590529441833 | 0.7013888888888888\n",
      "> 32 | 0.6258010864257812 | 0.7135416666666666\n",
      "> 33 | 0.6231522560119629 | 0.7256944444444444\n",
      "> 34 | 0.6206057667732239 | 0.7378472222222222\n",
      "> 35 | 0.6181552410125732 | 0.7395833333333334\n",
      "> 36 | 0.6157947778701782 | 0.7534722222222222\n",
      "> 37 | 0.6135188341140747 | 0.7586805555555556\n",
      "> 38 | 0.6113221645355225 | 0.7552083333333334\n",
      "> 39 | 0.6092000007629395 | 0.765625\n",
      "> 40 | 0.6071478724479675 | 0.765625\n",
      "> 41 | 0.6051614284515381 | 0.7690972222222222\n",
      "> 42 | 0.6032369136810303 | 0.7690972222222222\n",
      "> 43 | 0.6013705730438232 | 0.7725694444444444\n",
      "> 44 | 0.5995590686798096 | 0.765625\n",
      "> 45 | 0.5977991819381714 | 0.7708333333333334\n",
      "> 46 | 0.5960879325866699 | 0.7725694444444444\n",
      "> 47 | 0.5944225788116455 | 0.7690972222222222\n",
      "> 48 | 0.5928006172180176 | 0.7777777777777778\n",
      "> 49 | 0.5912196636199951 | 0.7795138888888888\n",
      "> 50 | 0.5896773338317871 | 0.7777777777777778\n",
      "> 51 | 0.5881718397140503 | 0.7760416666666666\n",
      "> 52 | 0.5867009162902832 | 0.7760416666666666\n",
      "> 53 | 0.5852630138397217 | 0.7760416666666666\n",
      "> 54 | 0.5838562846183777 | 0.7777777777777778\n",
      "> 55 | 0.5824792385101318 | 0.7795138888888888\n",
      "> 56 | 0.5811304450035095 | 0.7795138888888888\n",
      "> 57 | 0.5798085331916809 | 0.7829861111111112\n",
      "> 58 | 0.5785121917724609 | 0.78125\n",
      "> 59 | 0.5772402286529541 | 0.78125\n",
      "> 60 | 0.5759915113449097 | 0.7829861111111112\n",
      "> 61 | 0.5747650861740112 | 0.7847222222222222\n",
      "> 62 | 0.5735601186752319 | 0.7881944444444444\n",
      "> 63 | 0.5723754167556763 | 0.7881944444444444\n",
      "> 64 | 0.5712102651596069 | 0.7864583333333334\n",
      "> 65 | 0.5700639486312866 | 0.7899305555555556\n",
      "> 66 | 0.5689356327056885 | 0.7881944444444444\n",
      "> 67 | 0.5678246021270752 | 0.7881944444444444\n",
      "> 68 | 0.5667303800582886 | 0.7934027777777778\n",
      "> 69 | 0.5656521320343018 | 0.7934027777777778\n",
      "> 70 | 0.5645894408226013 | 0.7934027777777778\n",
      "> 71 | 0.5635417103767395 | 0.7881944444444444\n",
      "> 72 | 0.5625085234642029 | 0.7881944444444444\n",
      "> 73 | 0.5614893436431885 | 0.7881944444444444\n",
      "> 74 | 0.5604836940765381 | 0.7881944444444444\n",
      "> 75 | 0.5594911575317383 | 0.7881944444444444\n",
      "> 76 | 0.55851149559021 | 0.7916666666666666\n",
      "> 77 | 0.5575440526008606 | 0.7916666666666666\n",
      "> 78 | 0.5565887689590454 | 0.7934027777777778\n",
      "> 79 | 0.5556450486183167 | 0.7934027777777778\n",
      "> 80 | 0.5547128319740295 | 0.7934027777777778\n",
      "> 81 | 0.5537915825843811 | 0.7934027777777778\n",
      "> 82 | 0.5528811812400818 | 0.7934027777777778\n",
      "> 83 | 0.5519812107086182 | 0.7934027777777778\n",
      "> 84 | 0.5510916113853455 | 0.7934027777777778\n",
      "> 85 | 0.5502119660377502 | 0.7934027777777778\n",
      "> 86 | 0.549342155456543 | 0.7934027777777778\n",
      "> 87 | 0.548481822013855 | 0.7934027777777778\n",
      "> 88 | 0.547630786895752 | 0.7934027777777778\n",
      "> 89 | 0.5467890501022339 | 0.7934027777777778\n",
      "> 90 | 0.5459561347961426 | 0.7951388888888888\n",
      "> 91 | 0.5451319813728333 | 0.7951388888888888\n",
      "> 92 | 0.5443164110183716 | 0.7951388888888888\n",
      "> 93 | 0.5435092449188232 | 0.796875\n",
      "> 94 | 0.5427103042602539 | 0.7986111111111112\n",
      "> 95 | 0.541919469833374 | 0.7986111111111112\n",
      "> 96 | 0.5411365628242493 | 0.7986111111111112\n",
      "> 97 | 0.5403614044189453 | 0.7986111111111112\n",
      "> 98 | 0.5395939350128174 | 0.7986111111111112\n",
      "> 99 | 0.5388338565826416 | 0.7986111111111112\n",
      "> 100 | 0.5380812287330627 | 0.8003472222222222\n",
      "> Evaluation\n",
      "> Class Acc = 0.79296875\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.48268401622772217 | 0.6770715042948723 | 0.5430743098258972\n",
      "> Confusion Matrix \n",
      "TN: 141.0 | FP: 14.0 \n",
      "FN: 39.0 | TP: 62.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 18.0 | FP: 6.0 \n",
      "FN: 14.0 | TP: 50.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 123.0 | FP: 8.0 \n",
      "FN: 25.0 | TP: 12.0\n"
     ]
    }
   ],
   "source": [
    "for cv_seed in cv_seeds:\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        x, y, a, test_size=0.3, random_state=cv_seed)\n",
    "\n",
    "    train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "    train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # train below\n",
    "\n",
    "    opt = Adam(learning_rate=lr)\n",
    "\n",
    "    model = UnfairLogisticRegression(xdim, ydim, batch_size)\n",
    "    ret = train_loop(model, train_data, epochs, opt)\n",
    "    Y, A, Y_hat = evaluation(model, test_data)\n",
    "    \n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1  = compute_metrics(Y, A, Y_hat, adim=adim)\n",
    "    \n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    \n",
    "    tradeoff = []\n",
    "    \n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "    \n",
    "    result = ['UnfairLR', cv_seed, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    del(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa8245f3ca0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving into DF then CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>cv_seed</th>\n",
       "      <th>clas_acc</th>\n",
       "      <th>dp</th>\n",
       "      <th>deqodds</th>\n",
       "      <th>deqopp</th>\n",
       "      <th>trade_dp</th>\n",
       "      <th>trade_deqodds</th>\n",
       "      <th>trade_deqopp</th>\n",
       "      <th>TN_a0</th>\n",
       "      <th>FP_a0</th>\n",
       "      <th>FN_a0</th>\n",
       "      <th>TP_a0</th>\n",
       "      <th>TN_a1</th>\n",
       "      <th>FP_a1</th>\n",
       "      <th>FN_a1</th>\n",
       "      <th>TP_a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>13</td>\n",
       "      <td>0.417969</td>\n",
       "      <td>0.957176</td>\n",
       "      <td>0.958974</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.581858</td>\n",
       "      <td>0.582190</td>\n",
       "      <td>0.577375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>29</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.878933</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.956455</td>\n",
       "      <td>0.461069</td>\n",
       "      <td>0.472378</td>\n",
       "      <td>0.471084</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>42</td>\n",
       "      <td>0.367188</td>\n",
       "      <td>0.996268</td>\n",
       "      <td>0.987042</td>\n",
       "      <td>0.974085</td>\n",
       "      <td>0.536603</td>\n",
       "      <td>0.535256</td>\n",
       "      <td>0.533332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>55</td>\n",
       "      <td>0.382812</td>\n",
       "      <td>0.999723</td>\n",
       "      <td>0.984993</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.553630</td>\n",
       "      <td>0.551347</td>\n",
       "      <td>0.551250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>73</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.956169</td>\n",
       "      <td>0.984283</td>\n",
       "      <td>0.987331</td>\n",
       "      <td>0.538720</td>\n",
       "      <td>0.543089</td>\n",
       "      <td>0.543552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>13</td>\n",
       "      <td>0.714844</td>\n",
       "      <td>0.561172</td>\n",
       "      <td>0.703649</td>\n",
       "      <td>0.540952</td>\n",
       "      <td>0.628755</td>\n",
       "      <td>0.709202</td>\n",
       "      <td>0.615859</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>29</td>\n",
       "      <td>0.800781</td>\n",
       "      <td>0.495486</td>\n",
       "      <td>0.683811</td>\n",
       "      <td>0.589652</td>\n",
       "      <td>0.612182</td>\n",
       "      <td>0.737688</td>\n",
       "      <td>0.679187</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>42</td>\n",
       "      <td>0.753906</td>\n",
       "      <td>0.522422</td>\n",
       "      <td>0.650464</td>\n",
       "      <td>0.618028</td>\n",
       "      <td>0.617172</td>\n",
       "      <td>0.698376</td>\n",
       "      <td>0.679238</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>55</td>\n",
       "      <td>0.753906</td>\n",
       "      <td>0.575305</td>\n",
       "      <td>0.735783</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>0.652606</td>\n",
       "      <td>0.744734</td>\n",
       "      <td>0.640545</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>73</td>\n",
       "      <td>0.792969</td>\n",
       "      <td>0.482684</td>\n",
       "      <td>0.677072</td>\n",
       "      <td>0.543074</td>\n",
       "      <td>0.600090</td>\n",
       "      <td>0.730451</td>\n",
       "      <td>0.644651</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name  cv_seed  clas_acc        dp   deqodds    deqopp  trade_dp  \\\n",
       "0  UnfairLR-decay       13  0.417969  0.957176  0.958974  0.933333  0.581858   \n",
       "1  UnfairLR-decay       29  0.312500  0.878933  0.967213  0.956455  0.461069   \n",
       "2  UnfairLR-decay       42  0.367188  0.996268  0.987042  0.974085  0.536603   \n",
       "3  UnfairLR-decay       55  0.382812  0.999723  0.984993  0.984375  0.553630   \n",
       "4  UnfairLR-decay       73  0.375000  0.956169  0.984283  0.987331  0.538720   \n",
       "5        UnfairLR       13  0.714844  0.561172  0.703649  0.540952  0.628755   \n",
       "6        UnfairLR       29  0.800781  0.495486  0.683811  0.589652  0.612182   \n",
       "7        UnfairLR       42  0.753906  0.522422  0.650464  0.618028  0.617172   \n",
       "8        UnfairLR       55  0.753906  0.575305  0.735783  0.556818  0.652606   \n",
       "9        UnfairLR       73  0.792969  0.482684  0.677072  0.543074  0.600090   \n",
       "\n",
       "   trade_deqodds  trade_deqopp  TN_a0  FP_a0  FN_a0  TP_a0  TN_a1  FP_a1  \\\n",
       "0       0.582190      0.577375    0.0   16.0    5.0   70.0    2.0  128.0   \n",
       "1       0.472378      0.471084    2.0   23.0   16.0   45.0    8.0  130.0   \n",
       "2       0.535256      0.533332    0.0   28.0    1.0   70.0    0.0  132.0   \n",
       "3       0.551347      0.551250    0.0   20.0    1.0   63.0    2.0  137.0   \n",
       "4       0.543089      0.543552    1.0   23.0    6.0   58.0    3.0  128.0   \n",
       "5       0.709202      0.615859   13.0    3.0   32.0   43.0  123.0    7.0   \n",
       "6       0.737688      0.679187   18.0    7.0   15.0   46.0  130.0    8.0   \n",
       "7       0.698376      0.679238   17.0   11.0   24.0   47.0  122.0   10.0   \n",
       "8       0.744734      0.640545   17.0    3.0   24.0   40.0  130.0    9.0   \n",
       "9       0.730451      0.644651   18.0    6.0   14.0   50.0  123.0    8.0   \n",
       "\n",
       "   FN_a1  TP_a1  \n",
       "0    0.0   35.0  \n",
       "1    7.0   25.0  \n",
       "2    1.0   24.0  \n",
       "3    0.0   33.0  \n",
       "4    3.0   34.0  \n",
       "5   31.0    4.0  \n",
       "6   21.0   11.0  \n",
       "7   18.0    7.0  \n",
       "8   27.0    6.0  \n",
       "9   25.0   12.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(results, columns=header)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f'{data_name}-result/unfair_lr-{epochs}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('falsb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "34ca74ed6235dfc7dda926bb3adb31e801e3d02679121d5b444ee035e270bd57"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "41359ec383f887151a607ad1e28cb7dbc05f61385692c63e2bb2f343bf03f280"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
