{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing file \n",
    "### where we evaluate BEUTEL's models using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "\n",
    "\n",
    "from util.load_data import load_data\n",
    "from util.evaluation import *\n",
    "from models.beutel.models import *\n",
    "from models.beutel.learning import train_loop as beutel_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIR_COEFFS = [1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_seeds = [13, 29, 42, 55, 73]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'titanic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, a = load_data(data_name)\n",
    "raw_data = (x, y, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdim = x.shape[1]\n",
    "ydim = y.shape[1]\n",
    "adim = a.shape[1]\n",
    "zdim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"model_name\", \"cv_seed\",\"fair_coeff\", \"clas_acc\", \"dp\", \"deqodds\", \"deqopp\", \"trade_dp\", \"trade_deqodds\", \"trade_deqopp\", \"TN_a0\", \"FP_a0\", \"FN_a0\", \"TP_a0\", \"TN_a1\", \"FP_a1\", \"FN_a1\", \"TP_a1\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing loop\n",
    "#### Each model is evalueted 5 times\n",
    "#### In the end of each iteration we save the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEUTEL for DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Model Loss | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.7102263569831848 | 0.6909472942352295 | 0.7295054197311401 | 0.49506578947368424 | 0.5\n",
      "> 2 | 0.6612179279327393 | 0.6458163261413574 | 0.6766194701194763 | 0.4407894736842105 | 0.6233552631578947\n",
      "> 3 | 0.6898583769798279 | 0.701541543006897 | 0.6781753301620483 | 0.5016447368421053 | 0.6118421052631579\n",
      "> 4 | 0.6602238416671753 | 0.6735606789588928 | 0.6468870639801025 | 0.40460526315789475 | 0.6595394736842105\n",
      "> 5 | 0.6551238298416138 | 0.6453511118888855 | 0.6648964881896973 | 0.47039473684210525 | 0.6825657894736842\n",
      "> 6 | 0.6663339734077454 | 0.6609464287757874 | 0.6717214584350586 | 0.5230263157894737 | 0.6200657894736842\n",
      "> 7 | 0.675331711769104 | 0.6444687843322754 | 0.7061946392059326 | 0.4342105263157895 | 0.7680921052631579\n",
      "> 8 | 0.6501919627189636 | 0.6754915714263916 | 0.6248923540115356 | 0.3815789473684211 | 0.8207236842105263\n",
      "> 9 | 0.6491979360580444 | 0.6547811031341553 | 0.643614649772644 | 0.4654605263157895 | 0.7105263157894737\n",
      "> 10 | 0.6473277807235718 | 0.6857103705406189 | 0.6089452505111694 | 0.3881578947368421 | 0.8273026315789473\n",
      "> 11 | 0.6624088883399963 | 0.5921356081962585 | 0.7326821088790894 | 0.45230263157894735 | 0.7467105263157895\n",
      "> 12 | 0.6528822183609009 | 0.66139155626297 | 0.6443728804588318 | 0.4276315789473684 | 0.7845394736842105\n",
      "> 13 | 0.6598864793777466 | 0.7563456296920776 | 0.5634273886680603 | 0.3782894736842105 | 0.837171052631579\n",
      "> 14 | 0.6806656122207642 | 0.7729448676109314 | 0.5883862972259521 | 0.40131578947368424 | 0.7976973684210527\n",
      "> 15 | 0.6463803052902222 | 0.6936630010604858 | 0.599097490310669 | 0.40789473684210525 | 0.7944078947368421\n",
      "> 16 | 0.6380871534347534 | 0.7115034461021423 | 0.5646708607673645 | 0.43585526315789475 | 0.756578947368421\n",
      "> 17 | 0.6343202590942383 | 0.6851403713226318 | 0.5835001468658447 | 0.4144736842105263 | 0.7944078947368421\n",
      "> 18 | 0.6352056264877319 | 0.6914364099502563 | 0.5789748430252075 | 0.4144736842105263 | 0.7911184210526315\n",
      "> 19 | 0.6330274343490601 | 0.6629252433776855 | 0.6031296253204346 | 0.4375 | 0.774671052631579\n",
      "> 20 | 0.6274535059928894 | 0.6493800282478333 | 0.6055269837379456 | 0.4309210526315789 | 0.78125\n",
      "> 21 | 0.6357381343841553 | 0.6753500699996948 | 0.5961260795593262 | 0.4276315789473684 | 0.7845394736842105\n",
      "> 22 | 0.6368868350982666 | 0.6731250286102295 | 0.6006486415863037 | 0.4029605263157895 | 0.7927631578947368\n",
      "> 23 | 0.6292740106582642 | 0.6774295568466187 | 0.5811184048652649 | 0.4276315789473684 | 0.7845394736842105\n",
      "> 24 | 0.6332734823226929 | 0.6691035628318787 | 0.5974433422088623 | 0.43914473684210525 | 0.7598684210526315\n",
      "> 25 | 0.6415795087814331 | 0.6651964783668518 | 0.6179624795913696 | 0.4309210526315789 | 0.7648026315789473\n",
      "> 26 | 0.6525945663452148 | 0.6693626642227173 | 0.6358264684677124 | 0.42598684210526316 | 0.75\n",
      "> 27 | 0.6335949897766113 | 0.6626579761505127 | 0.60453200340271 | 0.4243421052631579 | 0.7680921052631579\n",
      "> 28 | 0.6279678344726562 | 0.6734782457351685 | 0.582457423210144 | 0.4309210526315789 | 0.78125\n",
      "> 29 | 0.6269568204879761 | 0.6763361692428589 | 0.5775774717330933 | 0.4276315789473684 | 0.7845394736842105\n",
      "> 30 | 0.6241449117660522 | 0.6557772159576416 | 0.5925124883651733 | 0.42105263157894735 | 0.7944078947368421\n",
      "> 31 | 0.6223640441894531 | 0.656069278717041 | 0.5886589288711548 | 0.42598684210526316 | 0.7960526315789473\n",
      "> 32 | 0.6292997598648071 | 0.6772283911705017 | 0.5813711881637573 | 0.3980263157894737 | 0.819078947368421\n",
      "> 33 | 0.6224613189697266 | 0.6661851406097412 | 0.5787374973297119 | 0.4194078947368421 | 0.7993421052631579\n",
      "> 34 | 0.6225077509880066 | 0.6646264791488647 | 0.5803890228271484 | 0.41776315789473684 | 0.8075657894736842\n",
      "> 35 | 0.6207433938980103 | 0.6633071899414062 | 0.5781797170639038 | 0.42105263157894735 | 0.8092105263157895\n",
      "> 36 | 0.6204933524131775 | 0.6700336933135986 | 0.5709531307220459 | 0.4276315789473684 | 0.7993421052631579\n",
      "> 37 | 0.6225669384002686 | 0.6475023031234741 | 0.597631573677063 | 0.4506578947368421 | 0.7796052631578947\n",
      "> 38 | 0.6215351819992065 | 0.645190954208374 | 0.5978795289993286 | 0.41776315789473684 | 0.805921052631579\n",
      "> 39 | 0.6227574348449707 | 0.6445494294166565 | 0.6009653806686401 | 0.44901315789473684 | 0.78125\n",
      "> 40 | 0.6382650136947632 | 0.6689645051956177 | 0.6075654625892639 | 0.43585526315789475 | 0.7549342105263158\n",
      "> 41 | 0.6600891947746277 | 0.6905238032341003 | 0.629654586315155 | 0.4407894736842105 | 0.7598684210526315\n",
      "> 42 | 0.6428450345993042 | 0.6580636501312256 | 0.6276264190673828 | 0.4375 | 0.743421052631579\n",
      "> 43 | 0.6241270899772644 | 0.6410242319107056 | 0.6072299480438232 | 0.45394736842105265 | 0.756578947368421\n",
      "> 44 | 0.6269383430480957 | 0.6809759140014648 | 0.5729008913040161 | 0.4440789473684211 | 0.7796052631578947\n",
      "> 45 | 0.6235367059707642 | 0.6717167496681213 | 0.5753567814826965 | 0.4440789473684211 | 0.7763157894736842\n",
      "> 46 | 0.651862621307373 | 0.778628945350647 | 0.5250963568687439 | 0.42269736842105265 | 0.7680921052631579\n",
      "> 47 | 0.6231865882873535 | 0.6651573181152344 | 0.5812158584594727 | 0.42105263157894735 | 0.7828947368421053\n",
      "> 48 | 0.6226755380630493 | 0.6606284379959106 | 0.5847225189208984 | 0.42105263157894735 | 0.805921052631579\n",
      "> 49 | 0.6212233304977417 | 0.6490696668624878 | 0.5933769941329956 | 0.4128289473684211 | 0.8174342105263158\n",
      "> 50 | 0.6285507678985596 | 0.6687794923782349 | 0.5883221626281738 | 0.43585526315789475 | 0.7944078947368421\n",
      "> 51 | 0.6279562711715698 | 0.6466356515884399 | 0.6092768907546997 | 0.43914473684210525 | 0.7911184210526315\n",
      "> 52 | 0.6233558654785156 | 0.6710143685340881 | 0.5756974220275879 | 0.4243421052631579 | 0.7976973684210527\n",
      "> 53 | 0.6269237995147705 | 0.6805371642112732 | 0.5733104944229126 | 0.43914473684210525 | 0.787828947368421\n",
      "> 54 | 0.6234798431396484 | 0.6535670757293701 | 0.5933926105499268 | 0.4309210526315789 | 0.7993421052631579\n",
      "> 55 | 0.6233686208724976 | 0.6555026769638062 | 0.5912346243858337 | 0.43256578947368424 | 0.7976973684210527\n",
      "> 56 | 0.6231063008308411 | 0.6587196588516235 | 0.5874930024147034 | 0.43585526315789475 | 0.7944078947368421\n",
      "> 57 | 0.6222386360168457 | 0.6580289602279663 | 0.5864482522010803 | 0.4276315789473684 | 0.8026315789473685\n",
      "> 58 | 0.6211279630661011 | 0.6585279703140259 | 0.5837279558181763 | 0.4161184210526316 | 0.8075657894736842\n",
      "> 59 | 0.6224160194396973 | 0.659744381904602 | 0.5850875377655029 | 0.4194078947368421 | 0.8108552631578947\n",
      "> 60 | 0.6377516984939575 | 0.6668868064880371 | 0.6086165904998779 | 0.41776315789473684 | 0.8092105263157895\n",
      "> 61 | 0.6245927810668945 | 0.6538944244384766 | 0.5952911972999573 | 0.4292763157894737 | 0.7944078947368421\n",
      "> 62 | 0.6252790093421936 | 0.672199010848999 | 0.5783589482307434 | 0.43914473684210525 | 0.7944078947368421\n",
      "> 63 | 0.627740740776062 | 0.6720334887504578 | 0.583448052406311 | 0.4292763157894737 | 0.8009868421052632\n",
      "> 64 | 0.6239615082740784 | 0.661746621131897 | 0.5861763954162598 | 0.41776315789473684 | 0.805921052631579\n",
      "> 65 | 0.6253824234008789 | 0.6667200922966003 | 0.5840446949005127 | 0.4161184210526316 | 0.8141447368421053\n",
      "> 66 | 0.6287132501602173 | 0.6519432067871094 | 0.6054832935333252 | 0.4375 | 0.7828947368421053\n",
      "> 67 | 0.627948522567749 | 0.6646595001220703 | 0.5912375450134277 | 0.43914473684210525 | 0.7911184210526315\n",
      "> 68 | 0.6214670538902283 | 0.6602205038070679 | 0.5827137231826782 | 0.44243421052631576 | 0.787828947368421\n",
      "> 69 | 0.6200264692306519 | 0.660185694694519 | 0.5798672437667847 | 0.43914473684210525 | 0.787828947368421\n",
      "> 70 | 0.6265140771865845 | 0.6525222063064575 | 0.600506067276001 | 0.44243421052631576 | 0.7845394736842105\n",
      "> 71 | 0.6270357370376587 | 0.6617061495780945 | 0.5923653244972229 | 0.4457236842105263 | 0.7845394736842105\n",
      "> 72 | 0.631084680557251 | 0.675107479095459 | 0.587061882019043 | 0.42105263157894735 | 0.7631578947368421\n",
      "> 73 | 0.6272326111793518 | 0.6685519218444824 | 0.5859133005142212 | 0.4473684210526316 | 0.7763157894736842\n",
      "> 74 | 0.6203360557556152 | 0.6556137800216675 | 0.585058331489563 | 0.4440789473684211 | 0.7796052631578947\n",
      "> 75 | 0.6205830574035645 | 0.6563473343849182 | 0.5848187804222107 | 0.4440789473684211 | 0.7861842105263158\n",
      "> 76 | 0.6374348402023315 | 0.6850552558898926 | 0.5898145437240601 | 0.4457236842105263 | 0.7648026315789473\n",
      "> 77 | 0.6674827933311462 | 0.7112001776695251 | 0.6237654685974121 | 0.46381578947368424 | 0.7335526315789473\n",
      "> 78 | 0.6289757490158081 | 0.6380102038383484 | 0.6199411749839783 | 0.4407894736842105 | 0.7220394736842105\n",
      "> 79 | 0.6228863596916199 | 0.6509217023849487 | 0.5948510766029358 | 0.4555921052631579 | 0.7171052631578947\n",
      "> 80 | 0.6269221305847168 | 0.6373534798622131 | 0.6164907813072205 | 0.4407894736842105 | 0.7713815789473685\n",
      "> 81 | 0.6343929171562195 | 0.6400654315948486 | 0.6287204027175903 | 0.45394736842105265 | 0.7549342105263158\n",
      "> 82 | 0.6245959997177124 | 0.6383908987045288 | 0.610801100730896 | 0.44901315789473684 | 0.7598684210526315\n",
      "> 83 | 0.6247011423110962 | 0.6393566727638245 | 0.6100456714630127 | 0.4342105263157895 | 0.7779605263157895\n",
      "> 84 | 0.6234022378921509 | 0.6495215892791748 | 0.597282886505127 | 0.43585526315789475 | 0.7796052631578947\n",
      "> 85 | 0.6217562556266785 | 0.644754946231842 | 0.5987576246261597 | 0.43585526315789475 | 0.7796052631578947\n",
      "> 86 | 0.6210407018661499 | 0.6468715071678162 | 0.5952098965644836 | 0.45230263157894735 | 0.7598684210526315\n",
      "> 87 | 0.6201990842819214 | 0.6643406748771667 | 0.5760574340820312 | 0.44243421052631576 | 0.774671052631579\n",
      "> 88 | 0.6174300312995911 | 0.6449394226074219 | 0.5899206399917603 | 0.4375 | 0.7779605263157895\n",
      "> 89 | 0.6166525483131409 | 0.642013430595398 | 0.5912916660308838 | 0.45394736842105265 | 0.7697368421052632\n",
      "> 90 | 0.6159167289733887 | 0.6436039209365845 | 0.5882295370101929 | 0.45230263157894735 | 0.7713815789473685\n",
      "> 91 | 0.6180251836776733 | 0.6395020484924316 | 0.596548318862915 | 0.4555921052631579 | 0.7713815789473685\n",
      "> 92 | 0.6155458688735962 | 0.6478594541549683 | 0.5832322239875793 | 0.45394736842105265 | 0.7730263157894737\n",
      "> 93 | 0.6171256303787231 | 0.6439540386199951 | 0.5902972221374512 | 0.45723684210526316 | 0.7730263157894737\n",
      "> 94 | 0.6162887811660767 | 0.6416642665863037 | 0.5909132957458496 | 0.4555921052631579 | 0.7779605263157895\n",
      "> 95 | 0.616809606552124 | 0.6373926997184753 | 0.5962264537811279 | 0.4506578947368421 | 0.7763157894736842\n",
      "> 96 | 0.6166014671325684 | 0.6397284269332886 | 0.5934745073318481 | 0.45394736842105265 | 0.7763157894736842\n",
      "> 97 | 0.6154884099960327 | 0.6381853818893433 | 0.5927915573120117 | 0.45394736842105265 | 0.7730263157894737\n",
      "> 98 | 0.6157679557800293 | 0.6384168863296509 | 0.5931190848350525 | 0.4555921052631579 | 0.7713815789473685\n",
      "> 99 | 0.6160358786582947 | 0.6391060948371887 | 0.5929656624794006 | 0.45723684210526316 | 0.7697368421052632\n",
      "> 100 | 0.6159446835517883 | 0.6542099118232727 | 0.5776795148849487 | 0.4654605263157895 | 0.7648026315789473\n",
      "> Evaluation\n",
      "> Class Acc = 0.42578125\n",
      "> Adv Acc = 0.42578125\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.6236430108547211 | 0.499610785394907 | 0.5809523537755013\n",
      "> Confusion Matrix \n",
      "TN: 43.0 | FP: 103.0 \n",
      "FN: 44.0 | TP: 66.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 13.0 | FP: 3.0 \n",
      "FN: 40.0 | TP: 35.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 30.0 | FP: 100.0 \n",
      "FN: 4.0 | TP: 31.0\n",
      "> Epoch | Model Loss | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.7534443140029907 | 0.7720768451690674 | 0.7348117828369141 | 0.43585526315789475 | 0.6052631578947368\n",
      "> 2 | 0.7330083847045898 | 0.7526731491088867 | 0.7133437395095825 | 0.3881578947368421 | 0.6398026315789473\n",
      "> 3 | 0.7170603275299072 | 0.7375463843345642 | 0.6965742707252502 | 0.3881578947368421 | 0.6398026315789473\n",
      "> 4 | 0.7054811120033264 | 0.7265444397926331 | 0.6844178438186646 | 0.3881578947368421 | 0.6398026315789473\n",
      "> 5 | 0.6973907351493835 | 0.7187553644180298 | 0.6760262250900269 | 0.3881578947368421 | 0.6398026315789473\n",
      "> 6 | 0.6919276118278503 | 0.7133200764656067 | 0.6705350875854492 | 0.3881578947368421 | 0.6398026315789473\n",
      "> 7 | 0.688369870185852 | 0.7095469236373901 | 0.667192816734314 | 0.3881578947368421 | 0.6398026315789473\n",
      "> 8 | 0.6861503720283508 | 0.7069146037101746 | 0.6653860807418823 | 0.3881578947368421 | 0.6398026315789473\n",
      "> 9 | 0.6848430633544922 | 0.7050497531890869 | 0.6646363735198975 | 0.3881578947368421 | 0.6398026315789473\n",
      "> 10 | 0.6841398477554321 | 0.7036951184272766 | 0.6645845770835876 | 0.3963815789473684 | 0.6414473684210527\n",
      "> 11 | 0.6838243007659912 | 0.7026790380477905 | 0.6649695634841919 | 0.4243421052631579 | 0.6266447368421053\n",
      "> 12 | 0.683748722076416 | 0.7018900513648987 | 0.6656074523925781 | 0.4407894736842105 | 0.6200657894736842\n",
      "> 13 | 0.6838146448135376 | 0.7012566328048706 | 0.6663725972175598 | 0.44901315789473684 | 0.6118421052631579\n",
      "> 14 | 0.6839576959609985 | 0.7007336616516113 | 0.6671816110610962 | 0.45723684210526316 | 0.6101973684210527\n",
      "> 15 | 0.6841370463371277 | 0.7002924680709839 | 0.6679816842079163 | 0.4720394736842105 | 0.6019736842105263\n",
      "> 16 | 0.6843276619911194 | 0.6999143958091736 | 0.6687408685684204 | 0.48848684210526316 | 0.5921052631578947\n",
      "> 17 | 0.6845146417617798 | 0.6995874047279358 | 0.669441819190979 | 0.48848684210526316 | 0.5921052631578947\n",
      "> 18 | 0.6846897602081299 | 0.6993029117584229 | 0.6700765490531921 | 0.48848684210526316 | 0.5921052631578947\n",
      "> 19 | 0.6848490238189697 | 0.6990547180175781 | 0.6706433296203613 | 0.48848684210526316 | 0.5921052631578947\n",
      "> 20 | 0.6849911212921143 | 0.6988382339477539 | 0.6711440086364746 | 0.4868421052631579 | 0.5904605263157895\n",
      "> 21 | 0.6851160526275635 | 0.6986494064331055 | 0.6715826988220215 | 0.4868421052631579 | 0.587171052631579\n",
      "> 22 | 0.6852248907089233 | 0.6984848380088806 | 0.6719648838043213 | 0.4934210526315789 | 0.5805921052631579\n",
      "> 23 | 0.6853188872337341 | 0.6983417868614197 | 0.6722959876060486 | 0.4967105263157895 | 0.5773026315789473\n",
      "> 24 | 0.6853997707366943 | 0.6982176899909973 | 0.6725818514823914 | 0.49506578947368424 | 0.5723684210526315\n",
      "> 25 | 0.6854689717292786 | 0.6981101036071777 | 0.6728277206420898 | 0.5016447368421053 | 0.5657894736842105\n",
      "> 26 | 0.685528039932251 | 0.6980171203613281 | 0.6730389595031738 | 0.5 | 0.5674342105263158\n",
      "> 27 | 0.6855783462524414 | 0.6979368329048157 | 0.6732197999954224 | 0.5032894736842105 | 0.5641447368421053\n",
      "> 28 | 0.6856210231781006 | 0.6978676319122314 | 0.6733744740486145 | 0.49835526315789475 | 0.569078947368421\n",
      "> 29 | 0.6856573224067688 | 0.6978080868721008 | 0.6735065579414368 | 0.5016447368421053 | 0.5592105263157895\n",
      "> 30 | 0.6856880784034729 | 0.6977569460868835 | 0.6736193299293518 | 0.4934210526315789 | 0.5641447368421053\n",
      "> 31 | 0.6857141852378845 | 0.6977129578590393 | 0.6737154126167297 | 0.4934210526315789 | 0.5608552631578947\n",
      "> 32 | 0.6857362985610962 | 0.6976752281188965 | 0.6737972497940063 | 0.4917763157894737 | 0.5625\n",
      "> 33 | 0.6857550144195557 | 0.6976429224014282 | 0.6738669872283936 | 0.49506578947368424 | 0.5526315789473685\n",
      "> 34 | 0.6857708096504211 | 0.6976152658462524 | 0.6739263534545898 | 0.4967105263157895 | 0.5509868421052632\n",
      "> 35 | 0.6857842206954956 | 0.6975916028022766 | 0.6739767789840698 | 0.4917763157894737 | 0.5493421052631579\n",
      "> 36 | 0.6857955455780029 | 0.697571337223053 | 0.6740198135375977 | 0.4901315789473684 | 0.5509868421052632\n",
      "> 37 | 0.6858052015304565 | 0.6975540518760681 | 0.6740562915802002 | 0.4901315789473684 | 0.5476973684210527\n",
      "> 38 | 0.685813307762146 | 0.6975392699241638 | 0.674087405204773 | 0.4901315789473684 | 0.5476973684210527\n",
      "> 39 | 0.6858202219009399 | 0.6975265741348267 | 0.6741138100624084 | 0.4934210526315789 | 0.5444078947368421\n",
      "> 40 | 0.6858260631561279 | 0.6975158452987671 | 0.6741363406181335 | 0.4917763157894737 | 0.5460526315789473\n",
      "> 41 | 0.6858310699462891 | 0.6975066661834717 | 0.6741555333137512 | 0.4901315789473684 | 0.5476973684210527\n",
      "> 42 | 0.6858353018760681 | 0.6974987983703613 | 0.6741718053817749 | 0.4901315789473684 | 0.5476973684210527\n",
      "> 43 | 0.6858389377593994 | 0.6974921226501465 | 0.6741857528686523 | 0.4917763157894737 | 0.5493421052631579\n",
      "> 44 | 0.685841977596283 | 0.697486400604248 | 0.6741976141929626 | 0.4917763157894737 | 0.5493421052631579\n",
      "> 45 | 0.6858446002006531 | 0.6974815130233765 | 0.6742076873779297 | 0.4917763157894737 | 0.5493421052631579\n",
      "> 46 | 0.6858469247817993 | 0.697477400302887 | 0.6742163896560669 | 0.4917763157894737 | 0.5493421052631579\n",
      "> 47 | 0.6858488321304321 | 0.6974738836288452 | 0.6742237210273743 | 0.4917763157894737 | 0.5493421052631579\n",
      "> 48 | 0.6858505010604858 | 0.6974709033966064 | 0.6742300391197205 | 0.4934210526315789 | 0.5476973684210527\n",
      "> 49 | 0.6858518719673157 | 0.6974682807922363 | 0.674235463142395 | 0.4934210526315789 | 0.5476973684210527\n",
      "> 50 | 0.685853123664856 | 0.6974661350250244 | 0.6742401123046875 | 0.4934210526315789 | 0.5476973684210527\n",
      "> 51 | 0.6858541965484619 | 0.6974642872810364 | 0.6742440462112427 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 52 | 0.6858550906181335 | 0.6974626779556274 | 0.6742475032806396 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 53 | 0.6858559250831604 | 0.6974613666534424 | 0.6742504835128784 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 54 | 0.6858566403388977 | 0.6974602937698364 | 0.6742530465126038 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 55 | 0.6858572959899902 | 0.69745934009552 | 0.6742552518844604 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 56 | 0.6858577728271484 | 0.6974585056304932 | 0.6742571592330933 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 57 | 0.6858583092689514 | 0.6974577903747559 | 0.674258828163147 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 58 | 0.6858587861061096 | 0.6974571943283081 | 0.6742603182792664 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 59 | 0.6858591437339783 | 0.6974567770957947 | 0.6742615699768066 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 60 | 0.6858595013618469 | 0.6974563598632812 | 0.6742627024650574 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 61 | 0.6858599185943604 | 0.6974560022354126 | 0.6742637157440186 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 62 | 0.6858601570129395 | 0.697455644607544 | 0.6742645502090454 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 63 | 0.6858603954315186 | 0.6974554061889648 | 0.6742653846740723 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 64 | 0.6858606338500977 | 0.6974552273750305 | 0.6742660999298096 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 65 | 0.6858608722686768 | 0.6974550485610962 | 0.6742666959762573 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 66 | 0.6858611106872559 | 0.6974549293518066 | 0.6742672920227051 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 67 | 0.685861349105835 | 0.6974548101425171 | 0.6742678284645081 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 68 | 0.6858614683151245 | 0.6974547505378723 | 0.6742682456970215 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 69 | 0.6858617067337036 | 0.6974546909332275 | 0.6742687225341797 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 70 | 0.6858618259429932 | 0.6974546313285828 | 0.6742690801620483 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 71 | 0.6858619451522827 | 0.697454571723938 | 0.674269437789917 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 72 | 0.6858621835708618 | 0.697454571723938 | 0.6742697954177856 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 73 | 0.6858623027801514 | 0.6974545121192932 | 0.6742700338363647 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 74 | 0.6858624219894409 | 0.6974544525146484 | 0.6742703914642334 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 75 | 0.6858625411987305 | 0.6974544525146484 | 0.6742706298828125 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 76 | 0.68586266040802 | 0.6974545121192932 | 0.6742708683013916 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 77 | 0.6858627796173096 | 0.6974544525146484 | 0.6742711067199707 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 78 | 0.6858628988265991 | 0.6974544525146484 | 0.6742713451385498 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 79 | 0.6858630180358887 | 0.6974545121192932 | 0.6742715835571289 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 80 | 0.6858631372451782 | 0.6974545121192932 | 0.6742717623710632 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 81 | 0.6858632564544678 | 0.6974545121192932 | 0.6742719411849976 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 82 | 0.6858633756637573 | 0.697454571723938 | 0.6742721199989319 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 83 | 0.6858634352684021 | 0.697454571723938 | 0.6742722988128662 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 84 | 0.6858634948730469 | 0.697454571723938 | 0.6742725372314453 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 85 | 0.6858636140823364 | 0.697454571723938 | 0.6742726564407349 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 86 | 0.685863733291626 | 0.6974546313285828 | 0.6742728352546692 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 87 | 0.6858637928962708 | 0.697454571723938 | 0.6742730140686035 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 88 | 0.6858638525009155 | 0.6974546313285828 | 0.6742731332778931 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 89 | 0.6858639717102051 | 0.6974546909332275 | 0.6742733120918274 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 90 | 0.6858640909194946 | 0.6974546909332275 | 0.6742734909057617 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 91 | 0.6858641505241394 | 0.6974546909332275 | 0.6742735505104065 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 92 | 0.6858642101287842 | 0.6974546909332275 | 0.6742737293243408 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 93 | 0.685864269733429 | 0.6974547505378723 | 0.6742738485336304 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 94 | 0.6858643293380737 | 0.6974547505378723 | 0.6742739081382751 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 95 | 0.6858644485473633 | 0.6974547505378723 | 0.6742740273475647 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 96 | 0.6858644485473633 | 0.6974547505378723 | 0.6742741465568542 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 97 | 0.6858645081520081 | 0.6974548101425171 | 0.6742743253707886 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 98 | 0.6858645677566528 | 0.6974548101425171 | 0.6742743849754333 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 99 | 0.6858646869659424 | 0.6974548697471619 | 0.6742745041847229 | 0.49506578947368424 | 0.5460526315789473\n",
      "> 100 | 0.6858646869659424 | 0.6974548101425171 | 0.6742746233940125 | 0.49506578947368424 | 0.5460526315789473\n",
      "> Evaluation\n",
      "> Class Acc = 0.51953125\n",
      "> Adv Acc = 0.51953125\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9069767594337463 | 0.9165227860212326 | 0.8457991778850555\n",
      "> Confusion Matrix \n",
      "TN: 80.0 | FP: 83.0 \n",
      "FN: 40.0 | TP: 53.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 12.0 | FP: 13.0 \n",
      "FN: 23.0 | TP: 38.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 68.0 | FP: 70.0 \n",
      "FN: 17.0 | TP: 15.0\n",
      "> Epoch | Model Loss | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.7177175283432007 | 0.6716611385345459 | 0.7637737989425659 | 0.48355263157894735 | 0.5361842105263158\n",
      "> 2 | 0.7160466909408569 | 0.6920926570892334 | 0.7400007247924805 | 0.4934210526315789 | 0.5361842105263158\n",
      "> 3 | 0.6948080062866211 | 0.7349239587783813 | 0.6546921133995056 | 0.506578947368421 | 0.555921052631579\n",
      "> 4 | 0.6975939273834229 | 0.7179782390594482 | 0.6772094964981079 | 0.49506578947368424 | 0.5707236842105263\n",
      "> 5 | 0.7023890018463135 | 0.7148577570915222 | 0.68992018699646 | 0.4901315789473684 | 0.5953947368421053\n",
      "> 6 | 0.7335011959075928 | 0.7879374027252197 | 0.6790649890899658 | 0.4868421052631579 | 0.6118421052631579\n",
      "> 7 | 0.6974755525588989 | 0.8632662892341614 | 0.5316849946975708 | 0.41776315789473684 | 0.7269736842105263\n",
      "> 8 | 0.7000204920768738 | 0.8910520672798157 | 0.5089888572692871 | 0.3963815789473684 | 0.7944078947368421\n",
      "> 9 | 0.707314670085907 | 0.8693684339523315 | 0.5452607870101929 | 0.41776315789473684 | 0.756578947368421\n",
      "> 10 | 0.7246474027633667 | 0.9106783270835876 | 0.5386165380477905 | 0.3815789473684211 | 0.8026315789473685\n",
      "> 11 | 0.7090569138526917 | 0.9080499410629272 | 0.5100639462471008 | 0.4194078947368421 | 0.7417763157894737\n",
      "> 12 | 0.7191001772880554 | 0.8382647037506104 | 0.5999355316162109 | 0.4473684210526316 | 0.6677631578947368\n",
      "> 13 | 0.701352596282959 | 0.9459811449050903 | 0.45672407746315 | 0.4407894736842105 | 0.680921052631579\n",
      "> 14 | 0.7133243083953857 | 0.8511236310005188 | 0.5755249261856079 | 0.4276315789473684 | 0.7236842105263158\n",
      "> 15 | 0.7028812170028687 | 0.8105775117874146 | 0.595184862613678 | 0.4161184210526316 | 0.7648026315789473\n",
      "> 16 | 0.6984657049179077 | 0.815830409526825 | 0.5811009407043457 | 0.42105263157894735 | 0.7796052631578947\n",
      "> 17 | 0.6902133226394653 | 0.7747308015823364 | 0.6056958436965942 | 0.3963815789473684 | 0.7582236842105263\n",
      "> 18 | 0.6908748149871826 | 0.7619196176528931 | 0.6198299527168274 | 0.40625 | 0.7023026315789473\n",
      "> 19 | 0.6903905272483826 | 0.7827515602111816 | 0.598029613494873 | 0.48026315789473684 | 0.569078947368421\n",
      "> 20 | 0.6836966276168823 | 0.8152655363082886 | 0.5521277189254761 | 0.4473684210526316 | 0.7006578947368421\n",
      "> 21 | 0.7174972891807556 | 0.8664045333862305 | 0.5685900449752808 | 0.3963815789473684 | 0.8042763157894737\n",
      "> 22 | 0.6884814500808716 | 0.749990701675415 | 0.6269721388816833 | 0.4786184210526316 | 0.6101973684210527\n",
      "> 23 | 0.6853022575378418 | 0.8391433954238892 | 0.5314610004425049 | 0.45230263157894735 | 0.7023026315789473\n",
      "> 24 | 0.6970705986022949 | 0.8414992094039917 | 0.5526419281959534 | 0.3963815789473684 | 0.8009868421052632\n",
      "> 25 | 0.6869505643844604 | 0.753955602645874 | 0.6199454665184021 | 0.4753289473684211 | 0.6332236842105263\n",
      "> 26 | 0.6874381303787231 | 0.8392977118492126 | 0.5355784296989441 | 0.4555921052631579 | 0.7154605263157895\n",
      "> 27 | 0.735354483127594 | 0.8613858222961426 | 0.6093232035636902 | 0.4029605263157895 | 0.7911184210526315\n",
      "> 28 | 0.6847208738327026 | 0.8725708723068237 | 0.49687090516090393 | 0.45394736842105265 | 0.680921052631579\n",
      "> 29 | 0.6980897784233093 | 0.9323123097419739 | 0.4638672471046448 | 0.44243421052631576 | 0.7286184210526315\n",
      "> 30 | 0.6987091898918152 | 0.8993786573410034 | 0.4980396628379822 | 0.40460526315789475 | 0.7861842105263158\n",
      "> 31 | 0.6941185593605042 | 0.9184615612030029 | 0.46977555751800537 | 0.4457236842105263 | 0.7286184210526315\n",
      "> 32 | 0.6981122493743896 | 0.8839476704597473 | 0.5122768878936768 | 0.3980263157894737 | 0.8026315789473685\n",
      "> 33 | 0.6760942935943604 | 0.8360462188720703 | 0.5161423683166504 | 0.4654605263157895 | 0.7023026315789473\n",
      "> 34 | 0.6981661319732666 | 0.9143715500831604 | 0.4819607734680176 | 0.43914473684210525 | 0.7483552631578947\n",
      "> 35 | 0.6854146718978882 | 0.861502468585968 | 0.5093268156051636 | 0.4161184210526316 | 0.7779605263157895\n",
      "> 36 | 0.6859264969825745 | 0.8664013147354126 | 0.5054516792297363 | 0.4161184210526316 | 0.774671052631579\n",
      "> 37 | 0.6836726665496826 | 0.8577537536621094 | 0.5095916986465454 | 0.4161184210526316 | 0.78125\n",
      "> 38 | 0.682209849357605 | 0.8496100306510925 | 0.5148097276687622 | 0.4144736842105263 | 0.7861842105263158\n",
      "> 39 | 0.6799728870391846 | 0.8366165161132812 | 0.5233292579650879 | 0.4095394736842105 | 0.7944078947368421\n",
      "> 40 | 0.6785371899604797 | 0.8230488300323486 | 0.5340255498886108 | 0.40789473684210525 | 0.7960526315789473\n",
      "> 41 | 0.6790726184844971 | 0.8258888125419617 | 0.5322564840316772 | 0.4161184210526316 | 0.7845394736842105\n",
      "> 42 | 0.6781941652297974 | 0.8237267136573792 | 0.5326617360115051 | 0.42598684210526316 | 0.7779605263157895\n",
      "> 43 | 0.6754683256149292 | 0.812468409538269 | 0.5384681224822998 | 0.42105263157894735 | 0.7861842105263158\n",
      "> 44 | 0.6765077114105225 | 0.8206363320350647 | 0.5323790311813354 | 0.42269736842105265 | 0.78125\n",
      "> 45 | 0.674915075302124 | 0.8127068281173706 | 0.5371233224868774 | 0.4194078947368421 | 0.787828947368421\n",
      "> 46 | 0.6744008660316467 | 0.8127214908599854 | 0.5360802412033081 | 0.4095394736842105 | 0.7976973684210527\n",
      "> 47 | 0.6736457347869873 | 0.8058830499649048 | 0.5414083003997803 | 0.4144736842105263 | 0.7927631578947368\n",
      "> 48 | 0.6738835573196411 | 0.8083137273788452 | 0.5394535064697266 | 0.4128289473684211 | 0.7911184210526315\n",
      "> 49 | 0.6728125810623169 | 0.8054900169372559 | 0.5401351451873779 | 0.41118421052631576 | 0.7960526315789473\n",
      "> 50 | 0.6725796461105347 | 0.8009000420570374 | 0.5442591905593872 | 0.4144736842105263 | 0.7927631578947368\n",
      "> 51 | 0.6737784743309021 | 0.8073328137397766 | 0.5402241349220276 | 0.4128289473684211 | 0.7944078947368421\n",
      "> 52 | 0.6728743314743042 | 0.8006551265716553 | 0.5450935363769531 | 0.4161184210526316 | 0.7911184210526315\n",
      "> 53 | 0.6716859340667725 | 0.7957873344421387 | 0.5475845336914062 | 0.4161184210526316 | 0.7911184210526315\n",
      "> 54 | 0.6727449893951416 | 0.8003549575805664 | 0.5451351404190063 | 0.4161184210526316 | 0.7911184210526315\n",
      "> 55 | 0.6731244325637817 | 0.8014353513717651 | 0.5448133945465088 | 0.4161184210526316 | 0.7911184210526315\n",
      "> 56 | 0.671504020690918 | 0.7959233522415161 | 0.5470848083496094 | 0.4128289473684211 | 0.7944078947368421\n",
      "> 57 | 0.6708441376686096 | 0.7921135425567627 | 0.549574613571167 | 0.4243421052631579 | 0.7828947368421053\n",
      "> 58 | 0.6712452173233032 | 0.798481822013855 | 0.5440084934234619 | 0.4161184210526316 | 0.787828947368421\n",
      "> 59 | 0.6702834367752075 | 0.7939890623092651 | 0.5465777516365051 | 0.4144736842105263 | 0.7927631578947368\n",
      "> 60 | 0.6694379448890686 | 0.7943850755691528 | 0.5444907546043396 | 0.41118421052631576 | 0.7960526315789473\n",
      "> 61 | 0.6702673435211182 | 0.7941272258758545 | 0.5464074611663818 | 0.4144736842105263 | 0.7927631578947368\n",
      "> 62 | 0.6704422831535339 | 0.7940014004707336 | 0.546883225440979 | 0.41776315789473684 | 0.7861842105263158\n",
      "> 63 | 0.6694813370704651 | 0.7894668579101562 | 0.5494958758354187 | 0.42105263157894735 | 0.7861842105263158\n",
      "> 64 | 0.6702815294265747 | 0.7873184680938721 | 0.5532445907592773 | 0.4194078947368421 | 0.7845394736842105\n",
      "> 65 | 0.6696577668190002 | 0.7887754440307617 | 0.5505400896072388 | 0.4194078947368421 | 0.7845394736842105\n",
      "> 66 | 0.6691741943359375 | 0.7914879322052002 | 0.5468604564666748 | 0.41776315789473684 | 0.7894736842105263\n",
      "> 67 | 0.6686577796936035 | 0.7866418957710266 | 0.5506736040115356 | 0.4128289473684211 | 0.7944078947368421\n",
      "> 68 | 0.6692985892295837 | 0.7842992544174194 | 0.5542978644371033 | 0.4128289473684211 | 0.7944078947368421\n",
      "> 69 | 0.6696397066116333 | 0.7886279821395874 | 0.5506513118743896 | 0.41776315789473684 | 0.7828947368421053\n",
      "> 70 | 0.6701735258102417 | 0.7877451181411743 | 0.5526019334793091 | 0.4161184210526316 | 0.787828947368421\n",
      "> 71 | 0.6704882383346558 | 0.7781496047973633 | 0.5628268122673035 | 0.4144736842105263 | 0.7894736842105263\n",
      "> 72 | 0.6700236797332764 | 0.7684851884841919 | 0.5715622901916504 | 0.42269736842105265 | 0.78125\n",
      "> 73 | 0.6689136028289795 | 0.7691407203674316 | 0.5686864852905273 | 0.4161184210526316 | 0.787828947368421\n",
      "> 74 | 0.670365035533905 | 0.7876836061477661 | 0.553046464920044 | 0.4128289473684211 | 0.7911184210526315\n",
      "> 75 | 0.6711705923080444 | 0.7838712930679321 | 0.5584697723388672 | 0.4095394736842105 | 0.7976973684210527\n",
      "> 76 | 0.669062614440918 | 0.7658843398094177 | 0.5722408890724182 | 0.4029605263157895 | 0.8042763157894737\n",
      "> 77 | 0.6687428951263428 | 0.764597475528717 | 0.5728882551193237 | 0.4095394736842105 | 0.7944078947368421\n",
      "> 78 | 0.6706733703613281 | 0.7740926742553711 | 0.5672540664672852 | 0.4309210526315789 | 0.7697368421052632\n",
      "> 79 | 0.6705682277679443 | 0.7608335614204407 | 0.5803029537200928 | 0.42269736842105265 | 0.78125\n",
      "> 80 | 0.6714068651199341 | 0.7472068667411804 | 0.5956068634986877 | 0.4161184210526316 | 0.787828947368421\n",
      "> 81 | 0.6700807809829712 | 0.7470781803131104 | 0.5930832624435425 | 0.4144736842105263 | 0.7894736842105263\n",
      "> 82 | 0.6731456518173218 | 0.7747470140457153 | 0.5715444684028625 | 0.4243421052631579 | 0.7697368421052632\n",
      "> 83 | 0.6689357161521912 | 0.7478488683700562 | 0.590022623538971 | 0.4029605263157895 | 0.8042763157894737\n",
      "> 84 | 0.6700413227081299 | 0.7377015948295593 | 0.6023809909820557 | 0.40789473684210525 | 0.7960526315789473\n",
      "> 85 | 0.6714537143707275 | 0.7432699799537659 | 0.5996374487876892 | 0.40789473684210525 | 0.7894736842105263\n",
      "> 86 | 0.6726188063621521 | 0.7518100142478943 | 0.5934275388717651 | 0.4144736842105263 | 0.7828947368421053\n",
      "> 87 | 0.6672916412353516 | 0.7565205097198486 | 0.5780627727508545 | 0.40131578947368424 | 0.7993421052631579\n",
      "> 88 | 0.6642458438873291 | 0.7519385814666748 | 0.5765531659126282 | 0.40460526315789475 | 0.7960526315789473\n",
      "> 89 | 0.6686502695083618 | 0.7499687671661377 | 0.5873316526412964 | 0.4029605263157895 | 0.7845394736842105\n",
      "> 90 | 0.674366295337677 | 0.7529633045196533 | 0.5957692861557007 | 0.4161184210526316 | 0.7680921052631579\n",
      "> 91 | 0.6737215518951416 | 0.7402787208557129 | 0.6071642637252808 | 0.4309210526315789 | 0.7664473684210527\n",
      "> 92 | 0.6664419174194336 | 0.7432844638824463 | 0.5895993113517761 | 0.40789473684210525 | 0.7828947368421053\n",
      "> 93 | 0.6686028838157654 | 0.7441621422767639 | 0.5930436849594116 | 0.40131578947368424 | 0.7763157894736842\n",
      "> 94 | 0.6745423674583435 | 0.7597141265869141 | 0.5893706679344177 | 0.41118421052631576 | 0.7730263157894737\n",
      "> 95 | 0.6689116954803467 | 0.7587668299674988 | 0.5790566205978394 | 0.4161184210526316 | 0.78125\n",
      "> 96 | 0.6670215129852295 | 0.7434998750686646 | 0.5905432105064392 | 0.4095394736842105 | 0.787828947368421\n",
      "> 97 | 0.6699748039245605 | 0.7377090454101562 | 0.6022405624389648 | 0.40131578947368424 | 0.7828947368421053\n",
      "> 98 | 0.6704478859901428 | 0.7526240348815918 | 0.5882718563079834 | 0.4144736842105263 | 0.7664473684210527\n",
      "> 99 | 0.6690395474433899 | 0.7571697235107422 | 0.5809093713760376 | 0.4161184210526316 | 0.774671052631579\n",
      "> 100 | 0.6685764789581299 | 0.7565137147903442 | 0.5806392431259155 | 0.40789473684210525 | 0.7894736842105263\n",
      "> Evaluation\n",
      "> Class Acc = 0.4375\n",
      "> Adv Acc = 0.4375\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.6061249077320099 | 0.5222696885466576 | 0.6343661993741989\n",
      "> Confusion Matrix \n",
      "TN: 65.0 | FP: 95.0 \n",
      "FN: 49.0 | TP: 47.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 25.0 | FP: 3.0 \n",
      "FN: 43.0 | TP: 28.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 40.0 | FP: 92.0 \n",
      "FN: 6.0 | TP: 19.0\n",
      "> Epoch | Model Loss | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.6914708614349365 | 0.7024741768836975 | 0.6804676055908203 | 0.5148026315789473 | 0.5131578947368421\n",
      "> 2 | 0.6880348920822144 | 0.7411582469940186 | 0.6349115371704102 | 0.5197368421052632 | 0.5049342105263158\n",
      "> 3 | 0.673255443572998 | 0.7331476211547852 | 0.6133631467819214 | 0.5296052631578947 | 0.6233552631578947\n",
      "> 4 | 0.6822811365127563 | 0.7630290389060974 | 0.6015332937240601 | 0.47039473684210525 | 0.6858552631578947\n",
      "> 5 | 0.6832597255706787 | 0.7909733653068542 | 0.5755459666252136 | 0.4276315789473684 | 0.7450657894736842\n",
      "> 6 | 0.6668233871459961 | 0.7867240905761719 | 0.5469227433204651 | 0.4375 | 0.7384868421052632\n",
      "> 7 | 0.663915753364563 | 0.788392186164856 | 0.5394394397735596 | 0.4276315789473684 | 0.7549342105263158\n",
      "> 8 | 0.6636965274810791 | 0.8029395937919617 | 0.5244535207748413 | 0.4276315789473684 | 0.7713815789473685\n",
      "> 9 | 0.6640822887420654 | 0.8330646753311157 | 0.49509990215301514 | 0.4457236842105263 | 0.756578947368421\n",
      "> 10 | 0.6694004535675049 | 0.8556075692176819 | 0.4831933379173279 | 0.45230263157894735 | 0.7368421052631579\n",
      "> 11 | 0.6596614718437195 | 0.7459028959274292 | 0.5734200477600098 | 0.4292763157894737 | 0.756578947368421\n",
      "> 12 | 0.6615437269210815 | 0.7377474308013916 | 0.5853399038314819 | 0.41118421052631576 | 0.7779605263157895\n",
      "> 13 | 0.6631457805633545 | 0.7427988052368164 | 0.5834928750991821 | 0.41118421052631576 | 0.78125\n",
      "> 14 | 0.669503927230835 | 0.7507385015487671 | 0.5882693529129028 | 0.40131578947368424 | 0.787828947368421\n",
      "> 15 | 0.6824163198471069 | 0.7598379850387573 | 0.6049945950508118 | 0.4128289473684211 | 0.7763157894736842\n",
      "> 16 | 0.679693341255188 | 0.8008837103843689 | 0.5585029125213623 | 0.42105263157894735 | 0.7516447368421053\n",
      "> 17 | 0.6908683776855469 | 0.8119739294052124 | 0.5697627663612366 | 0.46875 | 0.7269736842105263\n",
      "> 18 | 0.6715919971466064 | 0.7778290510177612 | 0.5653548836708069 | 0.4457236842105263 | 0.7171052631578947\n",
      "> 19 | 0.6636912226676941 | 0.7715481519699097 | 0.5558342933654785 | 0.45230263157894735 | 0.7401315789473685\n",
      "> 20 | 0.6546365022659302 | 0.7592288255691528 | 0.5500441789627075 | 0.4605263157894737 | 0.71875\n",
      "> 21 | 0.6547662615776062 | 0.7751880884170532 | 0.5343444347381592 | 0.45394736842105265 | 0.7286184210526315\n",
      "> 22 | 0.655349612236023 | 0.777548611164093 | 0.5331507325172424 | 0.4555921052631579 | 0.743421052631579\n",
      "> 23 | 0.6514812707901001 | 0.7760655879974365 | 0.5268970131874084 | 0.45394736842105265 | 0.7286184210526315\n",
      "> 24 | 0.655058741569519 | 0.7863093614578247 | 0.5238081812858582 | 0.4506578947368421 | 0.7351973684210527\n",
      "> 25 | 0.6548266410827637 | 0.734745979309082 | 0.5749073028564453 | 0.44243421052631576 | 0.756578947368421\n",
      "> 26 | 0.666297197341919 | 0.7378929257392883 | 0.5947015285491943 | 0.4161184210526316 | 0.7861842105263158\n",
      "> 27 | 0.6597679853439331 | 0.7473756074905396 | 0.5721604228019714 | 0.44243421052631576 | 0.756578947368421\n",
      "> 28 | 0.65503990650177 | 0.7805706262588501 | 0.5295091867446899 | 0.4588815789473684 | 0.6973684210526315\n",
      "> 29 | 0.6551611423492432 | 0.7656821012496948 | 0.544640064239502 | 0.45230263157894735 | 0.7236842105263158\n",
      "> 30 | 0.6488613486289978 | 0.752678632736206 | 0.5450440645217896 | 0.47039473684210525 | 0.7055921052631579\n",
      "> 31 | 0.6604288816452026 | 0.718663215637207 | 0.602194607257843 | 0.42598684210526316 | 0.7828947368421053\n",
      "> 32 | 0.6541293859481812 | 0.7317062020301819 | 0.5765525698661804 | 0.4473684210526316 | 0.7450657894736842\n",
      "> 33 | 0.6529304385185242 | 0.763805091381073 | 0.5420557856559753 | 0.44901315789473684 | 0.7072368421052632\n",
      "> 34 | 0.6513128280639648 | 0.7544151544570923 | 0.5482104420661926 | 0.4440789473684211 | 0.7286184210526315\n",
      "> 35 | 0.6826291084289551 | 0.7152057886123657 | 0.6500523090362549 | 0.43256578947368424 | 0.7532894736842105\n",
      "> 36 | 0.6887471079826355 | 0.696813702583313 | 0.6806805729866028 | 0.5197368421052632 | 0.5148026315789473\n",
      "> 37 | 0.6923580169677734 | 0.6829633712768555 | 0.701752781867981 | 0.5148026315789473 | 0.5197368421052632\n",
      "> 38 | 0.677169919013977 | 0.7483627796173096 | 0.6059769988059998 | 0.49835526315789475 | 0.5921052631578947\n",
      "> 39 | 0.6579737067222595 | 0.7559467554092407 | 0.5600006580352783 | 0.4276315789473684 | 0.7615131578947368\n",
      "> 40 | 0.6609597206115723 | 0.7221257090568542 | 0.5997937321662903 | 0.4161184210526316 | 0.7927631578947368\n",
      "> 41 | 0.655410647392273 | 0.7383586168289185 | 0.5724625587463379 | 0.4440789473684211 | 0.7286184210526315\n",
      "> 42 | 0.6519564986228943 | 0.7474266886711121 | 0.5564863085746765 | 0.45723684210526316 | 0.71875\n",
      "> 43 | 0.6564514636993408 | 0.7047150731086731 | 0.6081878542900085 | 0.4243421052631579 | 0.774671052631579\n",
      "> 44 | 0.6504871845245361 | 0.7500094175338745 | 0.5509649515151978 | 0.4588815789473684 | 0.7203947368421053\n",
      "> 45 | 0.6548410654067993 | 0.7074726819992065 | 0.6022095084190369 | 0.42598684210526316 | 0.7828947368421053\n",
      "> 46 | 0.6507155299186707 | 0.7387987375259399 | 0.5626323223114014 | 0.4407894736842105 | 0.7450657894736842\n",
      "> 47 | 0.6575841903686523 | 0.7128708362579346 | 0.6022976040840149 | 0.45723684210526316 | 0.7351973684210527\n",
      "> 48 | 0.648357093334198 | 0.7399125099182129 | 0.5568016767501831 | 0.4506578947368421 | 0.7154605263157895\n",
      "> 49 | 0.6558264493942261 | 0.70487380027771 | 0.6067790985107422 | 0.3996710526315789 | 0.8125\n",
      "> 50 | 0.660222053527832 | 0.6963289976119995 | 0.6241152286529541 | 0.4292763157894737 | 0.7598684210526315\n",
      "> 51 | 0.6532070636749268 | 0.7592588663101196 | 0.5471552014350891 | 0.4868421052631579 | 0.6463815789473685\n",
      "> 52 | 0.6543787121772766 | 0.7555615305900574 | 0.5531958341598511 | 0.47368421052631576 | 0.7088815789473685\n",
      "> 53 | 0.6507753133773804 | 0.7218024730682373 | 0.5797482132911682 | 0.4440789473684211 | 0.7483552631578947\n",
      "> 54 | 0.6514159440994263 | 0.7623258829116821 | 0.54050612449646 | 0.4654605263157895 | 0.6710526315789473\n",
      "> 55 | 0.6596552133560181 | 0.7010133862495422 | 0.6182970404624939 | 0.40625 | 0.8026315789473685\n",
      "> 56 | 0.650095522403717 | 0.7176625728607178 | 0.5825285911560059 | 0.4407894736842105 | 0.7483552631578947\n",
      "> 57 | 0.6641519069671631 | 0.7250635027885437 | 0.603240430355072 | 0.4506578947368421 | 0.725328947368421\n",
      "> 58 | 0.6718887090682983 | 0.7365830540657043 | 0.6071944236755371 | 0.4144736842105263 | 0.7680921052631579\n",
      "> 59 | 0.6508445739746094 | 0.7337563037872314 | 0.5679328441619873 | 0.4588815789473684 | 0.7269736842105263\n",
      "> 60 | 0.6507678627967834 | 0.720442533493042 | 0.5810931921005249 | 0.4440789473684211 | 0.7450657894736842\n",
      "> 61 | 0.6506538391113281 | 0.7490352988243103 | 0.5522724986076355 | 0.46875 | 0.6842105263157895\n",
      "> 62 | 0.6520916223526001 | 0.6870696544647217 | 0.617113471031189 | 0.43256578947368424 | 0.7697368421052632\n",
      "> 63 | 0.6479007601737976 | 0.7096009850502014 | 0.586200475692749 | 0.41776315789473684 | 0.7549342105263158\n",
      "> 64 | 0.6579069495201111 | 0.6879929900169373 | 0.6278209686279297 | 0.3963815789473684 | 0.7960526315789473\n",
      "> 65 | 0.6802361011505127 | 0.6529248952865601 | 0.7075474262237549 | 0.4753289473684211 | 0.5723684210526315\n",
      "> 66 | 0.7095965147018433 | 0.7858773469924927 | 0.6333156824111938 | 0.4654605263157895 | 0.6480263157894737\n",
      "> 67 | 0.7055147290229797 | 0.7791345119476318 | 0.6318950653076172 | 0.38651315789473684 | 0.6348684210526315\n",
      "> 68 | 0.6983702182769775 | 0.7595372796058655 | 0.6372031569480896 | 0.38651315789473684 | 0.6348684210526315\n",
      "> 69 | 0.6939184665679932 | 0.7440306544303894 | 0.6438062191009521 | 0.3848684210526316 | 0.6365131578947368\n",
      "> 70 | 0.691475510597229 | 0.7324997186660767 | 0.6504514217376709 | 0.43585526315789475 | 0.6052631578947368\n",
      "> 71 | 0.6902209520339966 | 0.7239387631416321 | 0.6565030217170715 | 0.4506578947368421 | 0.6036184210526315\n",
      "> 72 | 0.6896336078643799 | 0.7175381183624268 | 0.661729097366333 | 0.4506578947368421 | 0.6036184210526315\n",
      "> 73 | 0.6894108057022095 | 0.7127190232276917 | 0.6661026477813721 | 0.4506578947368421 | 0.5970394736842105\n",
      "> 74 | 0.6893795728683472 | 0.7090693712234497 | 0.6696897745132446 | 0.46875 | 0.5888157894736842\n",
      "> 75 | 0.6894420385360718 | 0.7062921524047852 | 0.6725919246673584 | 0.5032894736842105 | 0.5641447368421053\n",
      "> 76 | 0.6895439624786377 | 0.7041706442832947 | 0.6749172210693359 | 0.5016447368421053 | 0.5625\n",
      "> 77 | 0.6896560192108154 | 0.7025449275970459 | 0.6767671704292297 | 0.5032894736842105 | 0.5575657894736842\n",
      "> 78 | 0.6897634267807007 | 0.701295793056488 | 0.6782310009002686 | 0.5098684210526315 | 0.5476973684210527\n",
      "> 79 | 0.6898593306541443 | 0.7003340721130371 | 0.6793845891952515 | 0.5098684210526315 | 0.5476973684210527\n",
      "> 80 | 0.6899415254592896 | 0.6995924711227417 | 0.6802905797958374 | 0.5115131578947368 | 0.5493421052631579\n",
      "> 81 | 0.6900100708007812 | 0.6990197896957397 | 0.6810002326965332 | 0.5131578947368421 | 0.5476973684210527\n",
      "> 82 | 0.6900659799575806 | 0.6985770463943481 | 0.681554913520813 | 0.5115131578947368 | 0.5460526315789473\n",
      "> 83 | 0.690110981464386 | 0.698234498500824 | 0.6819874048233032 | 0.5131578947368421 | 0.5411184210526315\n",
      "> 84 | 0.6901467442512512 | 0.6979693174362183 | 0.6823241710662842 | 0.5098684210526315 | 0.5411184210526315\n",
      "> 85 | 0.69017493724823 | 0.6977639198303223 | 0.6825859546661377 | 0.5082236842105263 | 0.5394736842105263\n",
      "> 86 | 0.6901968717575073 | 0.6976048350334167 | 0.6827890872955322 | 0.506578947368421 | 0.5345394736842105\n",
      "> 87 | 0.6902139782905579 | 0.6974815130233765 | 0.6829463243484497 | 0.5082236842105263 | 0.5328947368421053\n",
      "> 88 | 0.6902270317077637 | 0.6973860263824463 | 0.6830679774284363 | 0.506578947368421 | 0.524671052631579\n",
      "> 89 | 0.6902369260787964 | 0.6973119974136353 | 0.6831618547439575 | 0.5115131578947368 | 0.5197368421052632\n",
      "> 90 | 0.6902444362640381 | 0.6972546577453613 | 0.6832342147827148 | 0.5115131578947368 | 0.5197368421052632\n",
      "> 91 | 0.690250039100647 | 0.6972102522850037 | 0.6832898259162903 | 0.5131578947368421 | 0.5180921052631579\n",
      "> 92 | 0.6902540922164917 | 0.697175920009613 | 0.6833323240280151 | 0.5148026315789473 | 0.5164473684210527\n",
      "> 93 | 0.6902570724487305 | 0.6971492767333984 | 0.6833648681640625 | 0.5148026315789473 | 0.5164473684210527\n",
      "> 94 | 0.6902591586112976 | 0.6971286535263062 | 0.6833896636962891 | 0.5164473684210527 | 0.5148026315789473\n",
      "> 95 | 0.6902605295181274 | 0.6971127986907959 | 0.6834083795547485 | 0.5180921052631579 | 0.5164473684210527\n",
      "> 96 | 0.6902614831924438 | 0.6971005201339722 | 0.6834225058555603 | 0.5180921052631579 | 0.5164473684210527\n",
      "> 97 | 0.6902620196342468 | 0.6970909833908081 | 0.6834330558776855 | 0.5213815789473685 | 0.5131578947368421\n",
      "> 98 | 0.6902622580528259 | 0.6970836520195007 | 0.6834408640861511 | 0.5213815789473685 | 0.5131578947368421\n",
      "> 99 | 0.6902623176574707 | 0.6970779895782471 | 0.6834466457366943 | 0.5213815789473685 | 0.5131578947368421\n",
      "> 100 | 0.6902621984481812 | 0.6970735788345337 | 0.6834507584571838 | 0.5230263157894737 | 0.5148026315789473\n",
      "> Evaluation\n",
      "> Class Acc = 0.49609375\n",
      "> Adv Acc = 0.49609375\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9822812676429749 | 0.9656256884336472 | 0.9848484992980957\n",
      "> Confusion Matrix \n",
      "TN: 79.0 | FP: 80.0 \n",
      "FN: 49.0 | TP: 48.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 9.0 | FP: 11.0 \n",
      "FN: 32.0 | TP: 32.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 70.0 | FP: 69.0 \n",
      "FN: 17.0 | TP: 16.0\n",
      "> Epoch | Model Loss | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.6985660791397095 | 0.6878074407577515 | 0.7093247175216675 | 0.47368421052631576 | 0.555921052631579\n",
      "> 2 | 0.6835100054740906 | 0.7155262231826782 | 0.6514937281608582 | 0.5328947368421053 | 0.5131578947368421\n",
      "> 3 | 0.6839907765388489 | 0.7395827770233154 | 0.6283987760543823 | 0.5361842105263158 | 0.506578947368421\n",
      "> 4 | 0.6851487159729004 | 0.7300559282302856 | 0.6402415633201599 | 0.5493421052631579 | 0.5361842105263158\n",
      "> 5 | 0.6841399669647217 | 0.74269700050354 | 0.6255828142166138 | 0.5411184210526315 | 0.5279605263157895\n",
      "> 6 | 0.6840566396713257 | 0.7505309581756592 | 0.6175823211669922 | 0.5263157894736842 | 0.5592105263157895\n",
      "> 7 | 0.6787102222442627 | 0.7483958601951599 | 0.6090246438980103 | 0.5098684210526315 | 0.569078947368421\n",
      "> 8 | 0.6789298057556152 | 0.7467676401138306 | 0.6110919713973999 | 0.4917763157894737 | 0.600328947368421\n",
      "> 9 | 0.6711065769195557 | 0.7512880563735962 | 0.5909250378608704 | 0.48355263157894735 | 0.6085526315789473\n",
      "> 10 | 0.6872782111167908 | 0.727431058883667 | 0.6471253633499146 | 0.45394736842105265 | 0.6282894736842105\n",
      "> 11 | 0.7339760065078735 | 0.7878944277763367 | 0.6800577640533447 | 0.49506578947368424 | 0.6529605263157895\n",
      "> 12 | 0.6812362670898438 | 0.7402019500732422 | 0.6222705841064453 | 0.48848684210526316 | 0.6036184210526315\n",
      "> 13 | 0.6844701170921326 | 0.8302223682403564 | 0.5387178659439087 | 0.4292763157894737 | 0.71875\n",
      "> 14 | 0.686903715133667 | 0.749298632144928 | 0.6245087385177612 | 0.5 | 0.5789473684210527\n",
      "> 15 | 0.6809513568878174 | 0.748867392539978 | 0.6130353212356567 | 0.5296052631578947 | 0.5657894736842105\n",
      "> 16 | 0.6795041561126709 | 0.7482506036758423 | 0.6107577085494995 | 0.5049342105263158 | 0.5970394736842105\n",
      "> 17 | 0.6792852282524109 | 0.7397531867027283 | 0.6188172101974487 | 0.48848684210526316 | 0.6069078947368421\n",
      "> 18 | 0.6737834215164185 | 0.7408925890922546 | 0.6066741943359375 | 0.4917763157894737 | 0.6167763157894737\n",
      "> 19 | 0.7165032029151917 | 0.7400458455085754 | 0.6929605603218079 | 0.4555921052631579 | 0.6595394736842105\n",
      "> 20 | 0.7070413827896118 | 0.8560060262680054 | 0.5580767393112183 | 0.4868421052631579 | 0.625\n",
      "> 21 | 0.6931911706924438 | 0.8782200813293457 | 0.508162260055542 | 0.42598684210526316 | 0.7384868421052632\n",
      "> 22 | 0.686262309551239 | 0.78077232837677 | 0.5917524099349976 | 0.3980263157894737 | 0.7730263157894737\n",
      "> 23 | 0.6798676252365112 | 0.8682342767715454 | 0.49150100350379944 | 0.40789473684210525 | 0.7697368421052632\n",
      "> 24 | 0.7068998217582703 | 0.84962397813797 | 0.5641756653785706 | 0.3651315789473684 | 0.7976973684210527\n",
      "> 25 | 0.6797530651092529 | 0.8474684953689575 | 0.5120376944541931 | 0.4621710526315789 | 0.7088815789473685\n",
      "> 26 | 0.6791743040084839 | 0.831864058971405 | 0.526484489440918 | 0.3832236842105263 | 0.8042763157894737\n",
      "> 27 | 0.6711511611938477 | 0.8353109359741211 | 0.5069913268089294 | 0.3651315789473684 | 0.805921052631579\n",
      "> 28 | 0.6760612726211548 | 0.8339554071426392 | 0.5181671380996704 | 0.38980263157894735 | 0.7845394736842105\n",
      "> 29 | 0.6898196339607239 | 0.8575639724731445 | 0.5220752954483032 | 0.39473684210526316 | 0.7927631578947368\n",
      "> 30 | 0.6750343441963196 | 0.7806969881057739 | 0.56937175989151 | 0.4309210526315789 | 0.7072368421052632\n",
      "> 31 | 0.6806327700614929 | 0.781238853931427 | 0.5800266265869141 | 0.42269736842105265 | 0.7384868421052632\n",
      "> 32 | 0.6834781169891357 | 0.8356058597564697 | 0.5313504338264465 | 0.375 | 0.7894736842105263\n",
      "> 33 | 0.7044507265090942 | 0.8754972815513611 | 0.5334041118621826 | 0.37006578947368424 | 0.7911184210526315\n",
      "> 34 | 0.6967575550079346 | 0.8765714168548584 | 0.5169436931610107 | 0.3782894736842105 | 0.8125\n",
      "> 35 | 0.6948771476745605 | 0.8708721399307251 | 0.5188822746276855 | 0.3815789473684211 | 0.805921052631579\n",
      "> 36 | 0.7241296172142029 | 0.9357640147209167 | 0.5124951601028442 | 0.3782894736842105 | 0.7960526315789473\n",
      "> 37 | 0.7247442007064819 | 0.9276118874549866 | 0.5218765735626221 | 0.39144736842105265 | 0.7796052631578947\n",
      "> 38 | 0.6940999627113342 | 0.8541406393051147 | 0.5340592861175537 | 0.4095394736842105 | 0.7680921052631579\n",
      "> 39 | 0.724643349647522 | 0.9210222959518433 | 0.5282644033432007 | 0.38980263157894735 | 0.7779605263157895\n",
      "> 40 | 0.698277473449707 | 0.8477979898452759 | 0.5487570762634277 | 0.4276315789473684 | 0.75\n",
      "> 41 | 0.7092900276184082 | 0.889574408531189 | 0.5290056467056274 | 0.3848684210526316 | 0.7861842105263158\n",
      "> 42 | 0.7123973369598389 | 0.8967263698577881 | 0.5280683636665344 | 0.3667763157894737 | 0.8042763157894737\n",
      "> 43 | 0.6938142776489258 | 0.8406918048858643 | 0.5469367504119873 | 0.3930921052631579 | 0.7976973684210527\n",
      "> 44 | 0.7018914222717285 | 0.8646736145019531 | 0.5391091704368591 | 0.40460526315789475 | 0.7763157894736842\n",
      "> 45 | 0.7221437692642212 | 0.8685047030448914 | 0.5757828950881958 | 0.40789473684210525 | 0.7697368421052632\n",
      "> 46 | 0.7136427760124207 | 0.8631162643432617 | 0.5641692876815796 | 0.40789473684210525 | 0.7697368421052632\n",
      "> 47 | 0.7129085659980774 | 0.8603204488754272 | 0.5654966831207275 | 0.42269736842105265 | 0.7516447368421053\n",
      "> 48 | 0.698555588722229 | 0.8592077493667603 | 0.5379034280776978 | 0.41118421052631576 | 0.7697368421052632\n",
      "> 49 | 0.7261701226234436 | 0.8822656869888306 | 0.5700746178627014 | 0.41118421052631576 | 0.7532894736842105\n",
      "> 50 | 0.7079364061355591 | 0.8734748959541321 | 0.5423979759216309 | 0.4128289473684211 | 0.7615131578947368\n",
      "> 51 | 0.7115777730941772 | 0.8873203992843628 | 0.5358352065086365 | 0.40789473684210525 | 0.7631578947368421\n",
      "> 52 | 0.7138065099716187 | 0.8630397319793701 | 0.5645734071731567 | 0.4161184210526316 | 0.7549342105263158\n",
      "> 53 | 0.7207395434379578 | 0.8750113248825073 | 0.5664677619934082 | 0.42105263157894735 | 0.75\n",
      "> 54 | 0.7036432027816772 | 0.8733296394348145 | 0.53395676612854 | 0.4161184210526316 | 0.7648026315789473\n",
      "> 55 | 0.7253115177154541 | 0.8983107805252075 | 0.5523121953010559 | 0.4194078947368421 | 0.7516447368421053\n",
      "> 56 | 0.714057445526123 | 0.8634059429168701 | 0.564708948135376 | 0.4243421052631579 | 0.7532894736842105\n",
      "> 57 | 0.693112850189209 | 0.8380358219146729 | 0.5481898784637451 | 0.4095394736842105 | 0.7680921052631579\n",
      "> 58 | 0.7022168040275574 | 0.8432530164718628 | 0.561180591583252 | 0.3980263157894737 | 0.7730263157894737\n",
      "> 59 | 0.7117831110954285 | 0.8582257032394409 | 0.565340518951416 | 0.3980263157894737 | 0.7664473684210527\n",
      "> 60 | 0.7001866698265076 | 0.839894711971283 | 0.560478687286377 | 0.43256578947368424 | 0.725328947368421\n",
      "> 61 | 0.7217695713043213 | 0.8474327325820923 | 0.5961065888404846 | 0.4276315789473684 | 0.7335526315789473\n",
      "> 62 | 0.6969404220581055 | 0.8490390777587891 | 0.5448417663574219 | 0.4144736842105263 | 0.7269736842105263\n",
      "> 63 | 0.6866980791091919 | 0.7544018626213074 | 0.6189942955970764 | 0.41776315789473684 | 0.7006578947368421\n",
      "> 64 | 0.6827705502510071 | 0.739539623260498 | 0.6260013580322266 | 0.46381578947368424 | 0.6348684210526315\n",
      "> 65 | 0.7051990628242493 | 0.8354854583740234 | 0.5749126076698303 | 0.4457236842105263 | 0.712171052631579\n",
      "> 66 | 0.686102032661438 | 0.7393267154693604 | 0.6328773498535156 | 0.42598684210526316 | 0.6299342105263158\n",
      "> 67 | 0.6851601600646973 | 0.733377993106842 | 0.6369423866271973 | 0.4128289473684211 | 0.6200657894736842\n",
      "> 68 | 0.6848994493484497 | 0.7285982370376587 | 0.6412007212638855 | 0.4473684210526316 | 0.5953947368421053\n",
      "> 69 | 0.6851519346237183 | 0.7253671884536743 | 0.6449368000030518 | 0.46381578947368424 | 0.5921052631578947\n",
      "> 70 | 0.6856374740600586 | 0.7232494354248047 | 0.648025393486023 | 0.48355263157894735 | 0.5789473684210527\n",
      "> 71 | 0.6861907839775085 | 0.7218642234802246 | 0.6505173444747925 | 0.4901315789473684 | 0.5756578947368421\n",
      "> 72 | 0.6867291331291199 | 0.7209538817405701 | 0.6525044441223145 | 0.5180921052631579 | 0.5575657894736842\n",
      "> 73 | 0.6872155666351318 | 0.720352292060852 | 0.6540789008140564 | 0.5197368421052632 | 0.5592105263157895\n",
      "> 74 | 0.6876373291015625 | 0.7199529409408569 | 0.6553215980529785 | 0.5197368421052632 | 0.5592105263157895\n",
      "> 75 | 0.6879935264587402 | 0.7196870446205139 | 0.6563000679016113 | 0.5197368421052632 | 0.5592105263157895\n",
      "> 76 | 0.6882894039154053 | 0.7195097208023071 | 0.6570690870285034 | 0.5197368421052632 | 0.5592105263157895\n",
      "> 77 | 0.6885321736335754 | 0.7193914651870728 | 0.6576728820800781 | 0.5197368421052632 | 0.5592105263157895\n",
      "> 78 | 0.6887297630310059 | 0.7193130254745483 | 0.6581465005874634 | 0.5180921052631579 | 0.5575657894736842\n",
      "> 79 | 0.6888895630836487 | 0.7192612886428833 | 0.6585178971290588 | 0.5164473684210527 | 0.555921052631579\n",
      "> 80 | 0.6890182495117188 | 0.7192275524139404 | 0.6588089466094971 | 0.5164473684210527 | 0.555921052631579\n",
      "> 81 | 0.689121425151825 | 0.7192057967185974 | 0.6590370535850525 | 0.5148026315789473 | 0.5575657894736842\n",
      "> 82 | 0.6892039775848389 | 0.7191921472549438 | 0.6592158079147339 | 0.5148026315789473 | 0.5575657894736842\n",
      "> 83 | 0.6892699003219604 | 0.7191838622093201 | 0.659355878829956 | 0.5131578947368421 | 0.555921052631579\n",
      "> 84 | 0.6893223524093628 | 0.7191790342330933 | 0.6594657301902771 | 0.5164473684210527 | 0.5526315789473685\n",
      "> 85 | 0.6893641948699951 | 0.7191765308380127 | 0.6595517992973328 | 0.5213815789473685 | 0.5476973684210527\n",
      "> 86 | 0.6893973350524902 | 0.7191754579544067 | 0.6596193313598633 | 0.5180921052631579 | 0.5444078947368421\n",
      "> 87 | 0.689423680305481 | 0.7191752791404724 | 0.6596721410751343 | 0.5148026315789473 | 0.5476973684210527\n",
      "> 88 | 0.6894445419311523 | 0.7191755175590515 | 0.659713625907898 | 0.5131578947368421 | 0.5493421052631579\n",
      "> 89 | 0.6894612312316895 | 0.7191760540008545 | 0.6597462296485901 | 0.5131578947368421 | 0.5493421052631579\n",
      "> 90 | 0.6894742250442505 | 0.719176709651947 | 0.659771740436554 | 0.5131578947368421 | 0.5460526315789473\n",
      "> 91 | 0.6894845962524414 | 0.7191773056983948 | 0.6597918272018433 | 0.5131578947368421 | 0.5460526315789473\n",
      "> 92 | 0.6894927024841309 | 0.7191779017448425 | 0.659807562828064 | 0.5115131578947368 | 0.5476973684210527\n",
      "> 93 | 0.6894991397857666 | 0.7191784381866455 | 0.6598198413848877 | 0.5115131578947368 | 0.5476973684210527\n",
      "> 94 | 0.6895042061805725 | 0.7191789150238037 | 0.6598294973373413 | 0.5115131578947368 | 0.5476973684210527\n",
      "> 95 | 0.6895081400871277 | 0.7191791534423828 | 0.6598371267318726 | 0.5131578947368421 | 0.5493421052631579\n",
      "> 96 | 0.6895112991333008 | 0.7191795110702515 | 0.6598429679870605 | 0.5131578947368421 | 0.5493421052631579\n",
      "> 97 | 0.6895136833190918 | 0.7191796898841858 | 0.659847617149353 | 0.5131578947368421 | 0.5493421052631579\n",
      "> 98 | 0.6895155310630798 | 0.7191798090934753 | 0.6598511934280396 | 0.5131578947368421 | 0.5493421052631579\n",
      "> 99 | 0.6895169615745544 | 0.7191798686981201 | 0.6598540544509888 | 0.5148026315789473 | 0.5476973684210527\n",
      "> 100 | 0.6895180940628052 | 0.7191799879074097 | 0.6598562598228455 | 0.5148026315789473 | 0.5476973684210527\n",
      "> Evaluation\n",
      "> Class Acc = 0.48046875\n",
      "> Adv Acc = 0.48046875\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9870129823684692 | 0.9270768910646439 | 0.9155405461788177\n",
      "> Confusion Matrix \n",
      "TN: 63.0 | FP: 92.0 \n",
      "FN: 41.0 | TP: 60.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 11.0 | FP: 13.0 \n",
      "FN: 24.0 | TP: 40.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 52.0 | FP: 79.0 \n",
      "FN: 17.0 | TP: 20.0\n"
     ]
    }
   ],
   "source": [
    "fairdef = 'DemPar'\n",
    "\n",
    "for cv_seed in cv_seeds:\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        x, y, a, test_size=0.3, random_state=cv_seed)\n",
    "\n",
    "    train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "    train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    for FAIR_COEFF in FAIR_COEFFS:\n",
    "\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "        model = Beutel(xdim, ydim, adim, zdim, FAIR_COEFF, fairdef)\n",
    "\n",
    "        ret = beutel_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "        Y, A, Y_hat, A_hat = fair_evaluation(model, test_data)\n",
    "        clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1 = compute_metrics(Y, A, Y_hat, A_hat, adim)\n",
    "\n",
    "        fair_metrics = (dp, deqodds, deqopp)\n",
    "        tradeoff = []\n",
    "        for fair_metric in fair_metrics:\n",
    "            tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "        result = ['BEUTEL4DP', cv_seed, FAIR_COEFF, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "        del(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEUTEL for Eq Opp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairdef = 'EqOpp'\n",
    "\n",
    "# for FAIR_COEFF in FAIR_COEFFS:\n",
    "#     for i in range(test_loop):\n",
    "\n",
    "#         opt = Adam(learning_rate=learning_rate)\n",
    "\n",
    "#         model = Beutel(xdim, ydim, adim, zdim, hidden_layer_specs, fairdef)\n",
    "\n",
    "#         ret = beutel_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "#         Y, A, Y_hat, A_hat = fair_evaluation(model, valid_data)\n",
    "#         clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1 = compute_metrics(Y, A, Y_hat, A_hat, adim)\n",
    "\n",
    "#         fair_metrics = (dp, deqodds, deqopp)\n",
    "#         tradeoff = []\n",
    "#         for fair_metric in fair_metrics:\n",
    "#             tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "#         result = ['BEUTEL4EqOpp', FAIR_COEFF, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "#         # results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving into DF then CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>cv_seed</th>\n",
       "      <th>fair_coeff</th>\n",
       "      <th>clas_acc</th>\n",
       "      <th>dp</th>\n",
       "      <th>deqodds</th>\n",
       "      <th>deqopp</th>\n",
       "      <th>trade_dp</th>\n",
       "      <th>trade_deqodds</th>\n",
       "      <th>trade_deqopp</th>\n",
       "      <th>TN_a0</th>\n",
       "      <th>FP_a0</th>\n",
       "      <th>FN_a0</th>\n",
       "      <th>TP_a0</th>\n",
       "      <th>TN_a1</th>\n",
       "      <th>FP_a1</th>\n",
       "      <th>FN_a1</th>\n",
       "      <th>TP_a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEUTEL4DP</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.425781</td>\n",
       "      <td>0.623643</td>\n",
       "      <td>0.499611</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.506059</td>\n",
       "      <td>0.459751</td>\n",
       "      <td>0.491408</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BEUTEL4DP</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.916523</td>\n",
       "      <td>0.845799</td>\n",
       "      <td>0.660638</td>\n",
       "      <td>0.663154</td>\n",
       "      <td>0.643682</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BEUTEL4DP</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.606125</td>\n",
       "      <td>0.522270</td>\n",
       "      <td>0.634366</td>\n",
       "      <td>0.508190</td>\n",
       "      <td>0.476141</td>\n",
       "      <td>0.517854</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BEUTEL4DP</td>\n",
       "      <td>55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496094</td>\n",
       "      <td>0.982281</td>\n",
       "      <td>0.965626</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.659242</td>\n",
       "      <td>0.655448</td>\n",
       "      <td>0.659819</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BEUTEL4DP</td>\n",
       "      <td>73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.480469</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.927077</td>\n",
       "      <td>0.915541</td>\n",
       "      <td>0.646317</td>\n",
       "      <td>0.632919</td>\n",
       "      <td>0.630209</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name  cv_seed  fair_coeff  clas_acc        dp   deqodds    deqopp  \\\n",
       "0  BEUTEL4DP       13         1.0  0.425781  0.623643  0.499611  0.580952   \n",
       "1  BEUTEL4DP       29         1.0  0.519531  0.906977  0.916523  0.845799   \n",
       "2  BEUTEL4DP       42         1.0  0.437500  0.606125  0.522270  0.634366   \n",
       "3  BEUTEL4DP       55         1.0  0.496094  0.982281  0.965626  0.984848   \n",
       "4  BEUTEL4DP       73         1.0  0.480469  0.987013  0.927077  0.915541   \n",
       "\n",
       "   trade_dp  trade_deqodds  trade_deqopp  TN_a0  FP_a0  FN_a0  TP_a0  TN_a1  \\\n",
       "0  0.506059       0.459751      0.491408   13.0    3.0   40.0   35.0   30.0   \n",
       "1  0.660638       0.663154      0.643682   12.0   13.0   23.0   38.0   68.0   \n",
       "2  0.508190       0.476141      0.517854   25.0    3.0   43.0   28.0   40.0   \n",
       "3  0.659242       0.655448      0.659819    9.0   11.0   32.0   32.0   70.0   \n",
       "4  0.646317       0.632919      0.630209   11.0   13.0   24.0   40.0   52.0   \n",
       "\n",
       "   FP_a1  FN_a1  TP_a1  \n",
       "0  100.0    4.0   31.0  \n",
       "1   70.0   17.0   15.0  \n",
       "2   92.0    6.0   19.0  \n",
       "3   69.0   17.0   16.0  \n",
       "4   79.0   17.0   20.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(results, columns=header)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f'{data_name}-result/beutel-{epochs}.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41359ec383f887151a607ad1e28cb7dbc05f61385692c63e2bb2f343bf03f280"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('falsb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
