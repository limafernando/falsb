{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing file \n",
    "### where we evaluate BEUTEL's models using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "\n",
    "\n",
    "from util.load_data import load_data\n",
    "from util.evaluation import *\n",
    "from models.beutel.models import *\n",
    "from models.beutel.learning import train_loop as beutel_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIR_COEFFS = [1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_seeds = [13, 29, 42, 55, 73]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'adult'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, a = load_data(data_name)\n",
    "raw_data = (x, y, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdim = x.shape[1]\n",
    "ydim = y.shape[1]\n",
    "adim = a.shape[1]\n",
    "zdim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"model_name\", \"cv_seed\",\"fair_coeff\", \"clas_acc\", \"dp\", \"deqodds\", \"deqopp\", \"trade_dp\", \"trade_deqodds\", \"trade_deqopp\", \"TN_a0\", \"FP_a0\", \"FN_a0\", \"TP_a0\", \"TN_a1\", \"FP_a1\", \"FN_a1\", \"TP_a1\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing loop\n",
    "#### Each model is evalueted 5 times\n",
    "#### In the end of each iteration we save the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEUTEL for DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Model Loss | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.7048986554145813 | 0.7214328050613403 | 0.6883646249771118 | 0.49064711830131447 | 0.8546195652173914\n",
      "> 2 | 0.6708391308784485 | 0.6342356204986572 | 0.7074426412582397 | 0.6901857937310415 | 0.376453488372093\n",
      "> 3 | 0.6708784103393555 | 0.6341137290000916 | 0.7076429724693298 | 0.7339800303336703 | 0.3375884732052578\n",
      "> 4 | 0.6708954572677612 | 0.6340923309326172 | 0.7076985239982605 | 0.7352439332659252 | 0.3372724974721941\n",
      "> 5 | 0.6709012985229492 | 0.6340850591659546 | 0.7077175378799438 | 0.7354967138523761 | 0.33701971688574317\n",
      "> 6 | 0.6709034442901611 | 0.6340824365615845 | 0.7077245116233826 | 0.7355283114256825 | 0.33698811931243683\n",
      "> 7 | 0.6709042191505432 | 0.6340814828872681 | 0.7077270150184631 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 8 | 0.6709045171737671 | 0.6340811252593994 | 0.7077279686927795 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 9 | 0.6709046363830566 | 0.6340809464454651 | 0.7077282667160034 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 10 | 0.6709046363830566 | 0.6340809464454651 | 0.7077284455299377 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 11 | 0.6709046363830566 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 12 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 13 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 14 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 15 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 16 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 17 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 18 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 19 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 20 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 21 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 22 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 23 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 24 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 25 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 26 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 27 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 28 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 29 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 30 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 31 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 32 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 33 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 34 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 35 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 36 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 37 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 38 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 39 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 40 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 41 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 42 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 43 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 44 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 45 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 46 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 47 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 48 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 49 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 50 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 51 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 52 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 53 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 54 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 55 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 56 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 57 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 58 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 59 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 60 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 61 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 62 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 63 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 64 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 65 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 66 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 67 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 68 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 69 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 70 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 71 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 72 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 73 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 74 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 75 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 76 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 77 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 78 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 79 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 80 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 81 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 82 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 83 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 84 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 85 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 86 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 87 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 88 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 89 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 90 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 91 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 92 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 93 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 94 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 95 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 96 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 97 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 98 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 99 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> 100 | 0.6709047555923462 | 0.6340808868408203 | 0.7077285051345825 | 0.7356231041456016 | 0.33683013144590496\n",
      "> Evaluation\n",
      "> Class Acc = 0.7381057739257812\n",
      "> Adv Acc = 0.7381057739257812\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9982305001467466 | 0.9919432252645493 | 0.9873731732368469\n",
      "> Confusion Matrix \n",
      "TN: 9876.0 | FP: 308.0 \n",
      "FN: 3237.0 | TP: 115.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3772.0 | FP: 109.0 \n",
      "FN: 488.0 | TP: 23.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 6104.0 | FP: 199.0 \n",
      "FN: 2749.0 | TP: 92.0\n",
      "> Epoch | Model Loss | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.613736629486084 | 0.6677052974700928 | 0.5597678422927856 | 0.4397118301314459 | 0.800461324570273\n",
      "> 2 | 0.7830792665481567 | 1.1848690509796143 | 0.38128939270973206 | 0.4684024266936299 | 0.8829309908998989\n",
      "> 3 | 0.7794238328933716 | 1.1475911140441895 | 0.41125649213790894 | 0.4552262386248736 | 0.9301693629929222\n",
      "> 4 | 0.9694415330886841 | 1.1507394313812256 | 0.7881437540054321 | 0.5051188068756319 | 0.9579436299292214\n",
      "> 5 | 0.7489144802093506 | 1.14509916305542 | 0.3527298867702484 | 0.49857810920121337 | 0.9653058645096056\n",
      "> 6 | 0.7511546611785889 | 1.168154001235962 | 0.33415526151657104 | 0.4929537411526795 | 0.9790192113245703\n",
      "> 7 | 0.806100606918335 | 1.1838587522506714 | 0.4283426105976105 | 0.43892189079878663 | 0.9095045500505561\n",
      "> 8 | 0.8570151925086975 | 1.174849510192871 | 0.5391808748245239 | 0.4538675429726997 | 0.9359517189079879\n",
      "> 9 | 0.8578308820724487 | 1.1707937717437744 | 0.544867992401123 | 0.40577603640040444 | 0.8503854903943378\n",
      "> 10 | 0.8577689528465271 | 1.1706503629684448 | 0.5448876619338989 | 0.4058392315470172 | 0.8508910515672397\n",
      "> 11 | 0.8577485680580139 | 1.170600414276123 | 0.5448968410491943 | 0.4058708291203236 | 0.8509858442871587\n",
      "> 12 | 0.8577413558959961 | 1.1705822944641113 | 0.5449005365371704 | 0.4058708291203236 | 0.8510490394337715\n",
      "> 13 | 0.857738733291626 | 1.1705756187438965 | 0.5449019074440002 | 0.4058708291203236 | 0.8511122345803842\n",
      "> 14 | 0.8577377796173096 | 1.1705732345581055 | 0.5449024438858032 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 15 | 0.8577374219894409 | 1.1705721616744995 | 0.5449026226997375 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 16 | 0.8577372431755066 | 1.1705719232559204 | 0.5449026823043823 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 17 | 0.8577372431755066 | 1.1705718040466309 | 0.5449026823043823 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 18 | 0.8577373027801514 | 1.1705716848373413 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 19 | 0.8577373027801514 | 1.1705718040466309 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 20 | 0.8577372431755066 | 1.1705716848373413 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 21 | 0.8577372431755066 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 22 | 0.8577372431755066 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 23 | 0.8577372431755066 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 24 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 25 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 26 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 27 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 28 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 29 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 30 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 31 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 32 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 33 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 34 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 35 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 36 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 37 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 38 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 39 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 40 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 41 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 42 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 43 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 44 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 45 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 46 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 47 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 48 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 49 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 50 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 51 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 52 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 53 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 54 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 55 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 56 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 57 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 58 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 59 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 60 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 61 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 62 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 63 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 64 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 65 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 66 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 67 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 68 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 69 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 70 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 71 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 72 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 73 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 74 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 75 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 76 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 77 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 78 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 79 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 80 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 81 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 82 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 83 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 84 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 85 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 86 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 87 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 88 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 89 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 90 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 91 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 92 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 93 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 94 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 95 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 96 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 97 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 98 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 99 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> 100 | 0.8577371835708618 | 1.1705715656280518 | 0.5449027419090271 | 0.4058392315470172 | 0.8510806370070778\n",
      "> Evaluation\n",
      "> Class Acc = 0.4208776652812958\n",
      "> Adv Acc = 0.4208776652812958\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.44564205408096313 | 0.4465881995856762 | 0.4454881176352501\n",
      "> Confusion Matrix \n",
      "TN: 2811.0 | FP: 7370.0 \n",
      "FN: 469.0 | TP: 2886.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 2400.0 | FP: 1486.0 \n",
      "FN: 309.0 | TP: 197.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 411.0 | FP: 5884.0 \n",
      "FN: 160.0 | TP: 2689.0\n",
      "> Epoch | Model Loss | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.5142484903335571 | 0.6090326905250549 | 0.4194643497467041 | 0.4629360465116279 | 0.8727565722952477\n",
      "> 2 | 0.6513216495513916 | 0.6521584391593933 | 0.6504848599433899 | 0.5368111729019212 | 0.5557065217391305\n",
      "> 3 | 0.6609948873519897 | 0.6317139863967896 | 0.6902757883071899 | 0.6581142568250758 | 0.4090621840242669\n",
      "> 4 | 0.6771367788314819 | 0.6433176398277283 | 0.7109559774398804 | 0.7217517694641051 | 0.34675176946410513\n",
      "> 5 | 0.677110493183136 | 0.6431736350059509 | 0.711047351360321 | 0.7289560161779576 | 0.3420121334681496\n",
      "> 6 | 0.6771044731140137 | 0.6431311964988708 | 0.7110776901245117 | 0.7294615773508595 | 0.341443377148635\n",
      "> 7 | 0.6771022081375122 | 0.6431156396865845 | 0.7110887169837952 | 0.7297143579373104 | 0.34134858442871585\n",
      "> 8 | 0.6771013736724854 | 0.643109917640686 | 0.7110928297042847 | 0.7297143579373104 | 0.3413169868554095\n",
      "> 9 | 0.6771010756492615 | 0.6431078314781189 | 0.7110943794250488 | 0.7297459555106168 | 0.34134858442871585\n",
      "> 10 | 0.6771009564399719 | 0.6431070566177368 | 0.711094856262207 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 11 | 0.6771008968353271 | 0.6431067585945129 | 0.7110950946807861 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 12 | 0.6771008968353271 | 0.6431065797805786 | 0.7110951542854309 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 13 | 0.6771008968353271 | 0.6431066393852234 | 0.7110951542854309 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 14 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 15 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 16 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 17 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 18 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 19 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 20 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 21 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 22 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 23 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 24 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 25 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 26 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 27 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 28 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 29 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 30 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 31 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 32 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 33 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 34 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 35 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 36 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 37 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 38 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 39 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 40 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 41 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 42 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 43 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 44 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 45 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 46 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 47 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 48 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 49 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 50 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 51 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 52 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 53 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 54 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 55 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 56 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 57 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 58 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 59 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 60 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 61 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 62 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 63 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 64 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 65 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 66 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 67 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 68 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 69 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 70 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 71 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 72 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 73 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 74 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 75 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 76 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 77 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 78 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 79 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 80 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 81 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 82 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 83 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 84 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 85 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 86 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 87 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 88 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 89 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 90 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 91 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 92 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 93 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 94 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 95 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 96 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 97 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 98 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 99 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> 100 | 0.6771008968353271 | 0.6431065797805786 | 0.7110952138900757 | 0.7297775530839231 | 0.34138018200202225\n",
      "> Evaluation\n",
      "> Class Acc = 0.7202275395393372\n",
      "> Adv Acc = 0.7202275395393372\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9993528462946415 | 0.9878062512725592 | 0.9785569906234741\n",
      "> Confusion Matrix \n",
      "TN: 9541.0 | FP: 626.0 \n",
      "FN: 3161.0 | TP: 208.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3697.0 | FP: 235.0 \n",
      "FN: 460.0 | TP: 40.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 5844.0 | FP: 391.0 \n",
      "FN: 2701.0 | TP: 168.0\n",
      "> Epoch | Model Loss | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.6745715141296387 | 0.6504734754562378 | 0.6986694931983948 | 0.5673028311425683 | 0.4576908493427705\n",
      "> 2 | 0.6745102405548096 | 0.6502208113670349 | 0.6987996697425842 | 0.7393516177957533 | 0.3348078867542973\n",
      "> 3 | 0.674513578414917 | 0.6502237319946289 | 0.6988033056259155 | 0.7392252275025278 | 0.335123862487361\n",
      "> 4 | 0.6745147705078125 | 0.6502248644828796 | 0.6988046169281006 | 0.7390356420626896 | 0.335123862487361\n",
      "> 5 | 0.6745151877403259 | 0.6502252221107483 | 0.6988050937652588 | 0.7389724469160769 | 0.3351870576339737\n",
      "> 6 | 0.6745153665542603 | 0.6502254009246826 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 7 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 8 | 0.674515426158905 | 0.6502255201339722 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 9 | 0.6745153665542603 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 10 | 0.6745154857635498 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 11 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 12 | 0.6745154857635498 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 13 | 0.6745154857635498 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 14 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 15 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 16 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 17 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 18 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 19 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 20 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 21 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 22 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 23 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 24 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 25 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 26 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 27 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 28 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 29 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 30 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 31 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 32 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 33 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 34 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 35 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 36 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 37 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 38 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 39 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 40 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 41 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 42 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 43 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 44 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 45 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 46 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 47 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 48 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 49 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 50 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 51 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 52 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 53 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 54 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 55 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 56 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 57 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 58 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 59 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 60 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 61 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 62 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 63 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 64 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 65 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 66 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 67 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 68 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 69 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 70 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 71 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 72 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 73 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 74 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 75 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 76 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 77 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 78 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 79 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 80 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 81 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 82 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 83 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 84 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 85 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 86 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 87 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 88 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 89 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 90 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 91 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 92 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 93 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 94 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 95 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 96 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 97 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 98 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 99 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> 100 | 0.674515426158905 | 0.6502254605293274 | 0.6988053321838379 | 0.7390040444893832 | 0.33521865520728006\n",
      "> Evaluation\n",
      "> Class Acc = 0.7329344153404236\n",
      "> Adv Acc = 0.7329344153404236\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9989526346325874 | 0.996212899684906 | 0.9946262836456299\n",
      "> Confusion Matrix \n",
      "TN: 9814.0 | FP: 316.0 \n",
      "FN: 3299.0 | TP: 107.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3740.0 | FP: 115.0 \n",
      "FN: 482.0 | TP: 18.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 6074.0 | FP: 201.0 \n",
      "FN: 2817.0 | TP: 89.0\n",
      "> Epoch | Model Loss | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 1.1437907218933105 | 1.1216928958892822 | 1.1658883094787598 | 0.5677451971688574 | 0.590937815975733\n",
      "> 2 | 0.6788239479064941 | 0.6560888290405273 | 0.7015589475631714 | 0.7403943377148635 | 0.33556622851365014\n",
      "> 3 | 0.6787852644920349 | 0.6559731960296631 | 0.7015973329544067 | 0.7426377654196158 | 0.33452350859453994\n",
      "> 4 | 0.6787732839584351 | 0.6559370160102844 | 0.7016095519065857 | 0.7430485338725986 | 0.3340495449949444\n",
      "> 5 | 0.6787691116333008 | 0.6559243202209473 | 0.7016139030456543 | 0.7431117290192113 | 0.3339231547017189\n",
      "> 6 | 0.6787675619125366 | 0.6559196710586548 | 0.7016154527664185 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 7 | 0.6787669658660889 | 0.6559180021286011 | 0.7016159892082214 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 8 | 0.6787667870521545 | 0.6559172868728638 | 0.7016162872314453 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 9 | 0.6787667274475098 | 0.6559171080589294 | 0.7016162872314453 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 10 | 0.6787667274475098 | 0.6559170484542847 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 11 | 0.6787667274475098 | 0.6559170484542847 | 0.7016164064407349 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 12 | 0.678766667842865 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 13 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 14 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 15 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 16 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 17 | 0.678766667842865 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 18 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 19 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 20 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 21 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 22 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 23 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 24 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 25 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 26 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 27 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 28 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 29 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 30 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 31 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 32 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 33 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 34 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 35 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 36 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 37 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 38 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 39 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 40 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 41 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 42 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 43 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 44 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 45 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 46 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 47 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 48 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 49 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 50 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 51 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 52 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 53 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 54 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 55 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 56 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 57 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 58 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 59 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 60 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 61 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 62 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 63 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 64 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 65 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 66 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 67 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 68 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 69 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 70 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 71 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 72 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 73 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 74 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 75 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 76 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 77 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 78 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 79 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 80 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 81 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 82 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 83 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 84 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 85 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 86 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 87 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 88 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 89 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 90 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 91 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 92 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 93 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 94 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 95 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 96 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 97 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 98 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 99 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> 100 | 0.6787667274475098 | 0.6559169888496399 | 0.7016163468360901 | 0.7432065217391305 | 0.33382836198179977\n",
      "> Evaluation\n",
      "> Class Acc = 0.6871306300163269\n",
      "> Adv Acc = 0.6871306300163269\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9936961755156517 | 0.994319248944521 | 0.995961606502533\n",
      "> Confusion Matrix \n",
      "TN: 8862.0 | FP: 1253.0 \n",
      "FN: 2982.0 | TP: 439.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3416.0 | FP: 463.0 \n",
      "FN: 415.0 | TP: 63.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 5446.0 | FP: 790.0 \n",
      "FN: 2567.0 | TP: 376.0\n"
     ]
    }
   ],
   "source": [
    "fairdef = 'DemPar'\n",
    "\n",
    "for cv_seed in cv_seeds:\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        x, y, a, test_size=0.3, random_state=cv_seed)\n",
    "\n",
    "    train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "    train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    for FAIR_COEFF in FAIR_COEFFS:\n",
    "\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "        model = Beutel(xdim, ydim, adim, zdim, FAIR_COEFF, fairdef)\n",
    "\n",
    "        ret = beutel_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "        Y, A, Y_hat, A_hat = fair_evaluation(model, test_data)\n",
    "        clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1 = compute_metrics(Y, A, Y_hat, A_hat, adim)\n",
    "\n",
    "        fair_metrics = (dp, deqodds, deqopp)\n",
    "        tradeoff = []\n",
    "        for fair_metric in fair_metrics:\n",
    "            tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "        result = ['BEUTEL4DP', cv_seed, FAIR_COEFF, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "        del(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEUTEL for Eq Opp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairdef = 'EqOpp'\n",
    "\n",
    "# for FAIR_COEFF in FAIR_COEFFS:\n",
    "#     for i in range(test_loop):\n",
    "\n",
    "#         opt = Adam(learning_rate=learning_rate)\n",
    "\n",
    "#         model = Beutel(xdim, ydim, adim, zdim, hidden_layer_specs, fairdef)\n",
    "\n",
    "#         ret = beutel_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "#         Y, A, Y_hat, A_hat = fair_evaluation(model, valid_data)\n",
    "#         clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1 = compute_metrics(Y, A, Y_hat, A_hat, adim)\n",
    "\n",
    "#         fair_metrics = (dp, deqodds, deqopp)\n",
    "#         tradeoff = []\n",
    "#         for fair_metric in fair_metrics:\n",
    "#             tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "#         result = ['BEUTEL4EqOpp', FAIR_COEFF, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "#         # results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving into DF then CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>cv_seed</th>\n",
       "      <th>fair_coeff</th>\n",
       "      <th>clas_acc</th>\n",
       "      <th>dp</th>\n",
       "      <th>deqodds</th>\n",
       "      <th>deqopp</th>\n",
       "      <th>trade_dp</th>\n",
       "      <th>trade_deqodds</th>\n",
       "      <th>trade_deqopp</th>\n",
       "      <th>TN_a0</th>\n",
       "      <th>FP_a0</th>\n",
       "      <th>FN_a0</th>\n",
       "      <th>TP_a0</th>\n",
       "      <th>TN_a1</th>\n",
       "      <th>FP_a1</th>\n",
       "      <th>FN_a1</th>\n",
       "      <th>TP_a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEUTEL4DP</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.738106</td>\n",
       "      <td>0.998231</td>\n",
       "      <td>0.991943</td>\n",
       "      <td>0.987373</td>\n",
       "      <td>0.848683</td>\n",
       "      <td>0.846403</td>\n",
       "      <td>0.844735</td>\n",
       "      <td>3772.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6104.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>2749.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BEUTEL4DP</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.420878</td>\n",
       "      <td>0.445642</td>\n",
       "      <td>0.446588</td>\n",
       "      <td>0.445488</td>\n",
       "      <td>0.432906</td>\n",
       "      <td>0.433352</td>\n",
       "      <td>0.432833</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>1486.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>5884.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>2689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BEUTEL4DP</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.720228</td>\n",
       "      <td>0.999353</td>\n",
       "      <td>0.987806</td>\n",
       "      <td>0.978557</td>\n",
       "      <td>0.837136</td>\n",
       "      <td>0.833058</td>\n",
       "      <td>0.829751</td>\n",
       "      <td>3697.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5844.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>2701.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BEUTEL4DP</td>\n",
       "      <td>55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.732934</td>\n",
       "      <td>0.998953</td>\n",
       "      <td>0.996213</td>\n",
       "      <td>0.994626</td>\n",
       "      <td>0.845513</td>\n",
       "      <td>0.844530</td>\n",
       "      <td>0.843960</td>\n",
       "      <td>3740.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6074.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BEUTEL4DP</td>\n",
       "      <td>73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.687131</td>\n",
       "      <td>0.993696</td>\n",
       "      <td>0.994319</td>\n",
       "      <td>0.995962</td>\n",
       "      <td>0.812456</td>\n",
       "      <td>0.812664</td>\n",
       "      <td>0.813212</td>\n",
       "      <td>3416.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>5446.0</td>\n",
       "      <td>790.0</td>\n",
       "      <td>2567.0</td>\n",
       "      <td>376.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name  cv_seed  fair_coeff  clas_acc        dp   deqodds    deqopp  \\\n",
       "0  BEUTEL4DP       13         1.0  0.738106  0.998231  0.991943  0.987373   \n",
       "1  BEUTEL4DP       29         1.0  0.420878  0.445642  0.446588  0.445488   \n",
       "2  BEUTEL4DP       42         1.0  0.720228  0.999353  0.987806  0.978557   \n",
       "3  BEUTEL4DP       55         1.0  0.732934  0.998953  0.996213  0.994626   \n",
       "4  BEUTEL4DP       73         1.0  0.687131  0.993696  0.994319  0.995962   \n",
       "\n",
       "   trade_dp  trade_deqodds  trade_deqopp   TN_a0   FP_a0  FN_a0  TP_a0  \\\n",
       "0  0.848683       0.846403      0.844735  3772.0   109.0  488.0   23.0   \n",
       "1  0.432906       0.433352      0.432833  2400.0  1486.0  309.0  197.0   \n",
       "2  0.837136       0.833058      0.829751  3697.0   235.0  460.0   40.0   \n",
       "3  0.845513       0.844530      0.843960  3740.0   115.0  482.0   18.0   \n",
       "4  0.812456       0.812664      0.813212  3416.0   463.0  415.0   63.0   \n",
       "\n",
       "    TN_a1   FP_a1   FN_a1   TP_a1  \n",
       "0  6104.0   199.0  2749.0    92.0  \n",
       "1   411.0  5884.0   160.0  2689.0  \n",
       "2  5844.0   391.0  2701.0   168.0  \n",
       "3  6074.0   201.0  2817.0    89.0  \n",
       "4  5446.0   790.0  2567.0   376.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(results, columns=header)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f'{data_name}-result/beutel-{epochs}.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41359ec383f887151a607ad1e28cb7dbc05f61385692c63e2bb2f343bf03f280"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('falsb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
