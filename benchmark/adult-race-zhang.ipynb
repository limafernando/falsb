{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing file \n",
    "### where we evaluate Zhang's models using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "\n",
    "from util.load_data import load_data\n",
    "from util.evaluation import *\n",
    "from models.zhang.models import FairLogisticRegression\n",
    "from models.zhang.learning import train_loop as zhang_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "opt = Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'adult-race'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, a_train = load_data(data_name, 'train')\n",
    "raw_data = (x_train, y_train, a_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdim = x_train.shape[1]\n",
    "ydim = y_train.shape[1]\n",
    "adim = a_train.shape[1]\n",
    "zdim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 112), (64, 1), (64, 5)), types: (tf.float64, tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid, y_valid, a_valid = load_data(data_name, 'valid')\n",
    "\n",
    "valid_data = Dataset.from_tensor_slices((x_valid, y_valid, a_valid))\n",
    "valid_data = valid_data.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test, a_test = load_data(data_name, 'test')\n",
    "\n",
    "test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "test_data = test_data.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"model_name\", \"clas_acc\", \"dp\", \"deqodds\", \"deqopp\", \"trade_dp\", \"trade_deqodds\", \"trade_deqopp\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing loop\n",
    "#### Each model is evalueted 5 times\n",
    "#### In the end of each iteration we save the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zhang for DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.5199112296104431 | 1.3905614614486694 | 0.6526856763925729 | -0.0019756742118208098\n",
      "> 2 | 0.4611121714115143 | 1.2602486610412598 | 0.8189655172413793 | -0.0019756742118208098\n",
      "> 3 | 0.4426349401473999 | 1.1552791595458984 | 0.8263428381962865 | 0.03217551942213145\n",
      "> 4 | 0.43559280037879944 | 1.079308032989502 | 0.8294098143236074 | 0.14266955030375514\n",
      "> 5 | 0.43303149938583374 | 1.0243663787841797 | 0.8323110079575596 | 0.21482638054248193\n",
      "> 6 | 0.43252211809158325 | 0.9834703207015991 | 0.834507625994695 | 0.2660117253700681\n",
      "> 7 | 0.4330763518810272 | 0.9521521329879761 | 0.8359167771883289 | 0.30107472271754826\n",
      "> 8 | 0.4342109262943268 | 0.9277064800262451 | 0.8367042440318302 | 0.3239112610209842\n",
      "> 9 | 0.4356565475463867 | 0.9081811904907227 | 0.8379476127320955 | 0.3414427597737755\n",
      "> 10 | 0.4372462034225464 | 0.8922717571258545 | 0.8389008620689655 | 0.3523844044942122\n",
      "> 11 | 0.438873827457428 | 0.8793830871582031 | 0.8391909814323607 | 0.3633260489774952\n",
      "> 12 | 0.44046860933303833 | 0.8688688278198242 | 0.8396468832891246 | 0.3708277067233776\n",
      "> 13 | 0.4419856071472168 | 0.859999418258667 | 0.8400613395225465 | 0.3775418981000663\n",
      "> 14 | 0.44339805841445923 | 0.8526548147201538 | 0.840434350132626 | 0.38346862200084353\n",
      "> 15 | 0.44468942284584045 | 0.8465602397918701 | 0.840683023872679 | 0.38790330369845627\n",
      "> 16 | 0.4458528757095337 | 0.8414152264595032 | 0.8408902519893899 | 0.3910946169329575\n",
      "> 17 | 0.44688451290130615 | 0.8370285034179688 | 0.8410974801061007 | 0.395653635579648\n",
      "> 18 | 0.44778284430503845 | 0.8334592580795288 | 0.8415119363395226 | 0.39838904672023157\n",
      "> 19 | 0.44854897260665894 | 0.8304451704025269 | 0.8417191644562334 | 0.4011659033260548\n",
      "> 20 | 0.44918403029441833 | 0.828117311000824 | 0.8422165119363395 | 0.4037355318942184\n",
      "> 21 | 0.44969314336776733 | 0.8259624242782593 | 0.8427138594164456 | 0.4058078132984809\n",
      "> 22 | 0.45007994771003723 | 0.8244364261627197 | 0.8428381962864722 | 0.4072169644130636\n",
      "> 23 | 0.45035168528556824 | 0.8231098651885986 | 0.843045424403183 | 0.40850177849951724\n",
      "> 24 | 0.45051610469818115 | 0.822028636932373 | 0.8431283156498673 | 0.40924779979872766\n",
      "> 25 | 0.4505841135978699 | 0.8213311433792114 | 0.8434184350132626 | 0.40978659290217595\n",
      "> 26 | 0.45056629180908203 | 0.8206870555877686 | 0.8439572281167109 | 0.41011815788891337\n",
      "> 27 | 0.4504730701446533 | 0.8203479051589966 | 0.843998673740053 | 0.4099938210188868\n",
      "> 28 | 0.4503173530101776 | 0.8200756311416626 | 0.8438328912466844 | 0.4099938210979381\n",
      "> 29 | 0.45010727643966675 | 0.8199700117111206 | 0.8441230106100795 | 0.41015960359130677\n",
      "> 30 | 0.44985315203666687 | 0.8199477195739746 | 0.8443716843501327 | 0.41032538608467545\n",
      "> 31 | 0.44956302642822266 | 0.8200193047523499 | 0.8444131299734748 | 0.41003526672128027\n",
      "> 32 | 0.44924286007881165 | 0.8202562928199768 | 0.8452834880636605 | 0.4097865929812272\n",
      "> 33 | 0.44889894127845764 | 0.820334255695343 | 0.8453663793103449 | 0.40920635425443674\n",
      "> 34 | 0.4485355317592621 | 0.8205606937408447 | 0.8457393899204244 | 0.4089162348910415\n",
      "> 35 | 0.44815677404403687 | 0.8209874033927917 | 0.8458222811671088 | 0.40866756115098846\n",
      "> 36 | 0.4477657377719879 | 0.8212436437606812 | 0.8460295092838196 | 0.4080458767218046\n",
      "> 37 | 0.4473649263381958 | 0.821546196937561 | 0.8460709549071618 | 0.40771431173506717\n",
      "> 38 | 0.4469561278820038 | 0.8219300508499146 | 0.8457808355437666 | 0.407424192371672\n",
      "> 39 | 0.4465414881706238 | 0.8222543597221375 | 0.8456979442970822 | 0.4069268451287196\n",
      "> 40 | 0.44612258672714233 | 0.8227362632751465 | 0.8459466180371353 | 0.4066367257653244\n",
      "> 41 | 0.4457007050514221 | 0.823140025138855 | 0.8461538461538461 | 0.4060150414151918\n",
      "> 42 | 0.44527769088745117 | 0.8234723806381226 | 0.8462781830238727 | 0.4054348026884013\n",
      "> 43 | 0.44485485553741455 | 0.823893666267395 | 0.8464439655172413 | 0.4048131181801662\n",
      "> 44 | 0.444431334733963 | 0.8243484497070312 | 0.8464854111405835 | 0.4046473357658487\n",
      "> 45 | 0.4440085291862488 | 0.8247554302215576 | 0.8465683023872679 | 0.40398420563427145\n",
      "> 46 | 0.443587064743042 | 0.8251456022262573 | 0.8467755305039788 | 0.4033625212841388\n",
      "> 47 | 0.4431673586368561 | 0.8255661725997925 | 0.847065649867374 | 0.40294806505071706\n",
      "> 48 | 0.44274914264678955 | 0.8260015249252319 | 0.8469413129973474 | 0.4027822825573484\n",
      "> 49 | 0.4423346519470215 | 0.8265465497970581 | 0.8469827586206896 | 0.40236782640297786\n",
      "> 50 | 0.4419238269329071 | 0.8269166946411133 | 0.8471485411140584 | 0.40187047892287175\n",
      "> 51 | 0.4415169358253479 | 0.8274705410003662 | 0.8473972148541115 | 0.4011659033260548\n",
      "> 52 | 0.4411139488220215 | 0.8278833627700806 | 0.8474386604774535 | 0.4004613277292378\n",
      "> 53 | 0.4407150447368622 | 0.828373372554779 | 0.8474386604774535 | 0.39992253446768705\n",
      "> 54 | 0.4403214454650879 | 0.8289399743080139 | 0.8474801061007957 | 0.39971530635097613\n",
      "> 55 | 0.43996596336364746 | 0.8293101191520691 | 0.8476458885941645 | 0.39921795887087\n",
      "> 56 | 0.4396384358406067 | 0.8297204971313477 | 0.8478531167108754 | 0.39892783958652606\n",
      "> 57 | 0.4393225908279419 | 0.8300960063934326 | 0.8479360079575596 | 0.39843049210641995\n",
      "> 58 | 0.4390093684196472 | 0.830572247505188 | 0.8479360079575596 | 0.3982232639897091\n",
      "> 59 | 0.4386962056159973 | 0.8309628963470459 | 0.8481432360742706 | 0.39797459024965604\n",
      "> 60 | 0.4383831024169922 | 0.8313952684402466 | 0.8482261273209549 | 0.3975186883928921\n",
      "> 61 | 0.43807026743888855 | 0.8318495154380798 | 0.8484333554376657 | 0.39727001465283907\n",
      "> 62 | 0.4377578794956207 | 0.8322792053222656 | 0.8485576923076923 | 0.39693844958705043\n",
      "> 63 | 0.43744632601737976 | 0.8328118920326233 | 0.848723474801061 | 0.3964825477302865\n",
      "> 64 | 0.4371907114982605 | 0.8331582546234131 | 0.8488892572944297 | 0.3961509827435491\n",
      "> 65 | 0.4369570016860962 | 0.8335446119308472 | 0.848723474801061 | 0.39590230900349604\n",
      "> 66 | 0.43672820925712585 | 0.8339627981185913 | 0.8490135941644562 | 0.39536351590004776\n",
      "> 67 | 0.43649834394454956 | 0.8343690633773804 | 0.8490550397877984 | 0.39519773324857654\n",
      "> 68 | 0.4362657070159912 | 0.8349175453186035 | 0.849179376657825 | 0.394949059350421\n",
      "> 69 | 0.43603065609931946 | 0.8352463245391846 | 0.8493866047745358 | 0.39486616810373665\n",
      "> 70 | 0.4357929229736328 | 0.8356850147247314 | 0.8495938328912467 | 0.39474183123371015\n",
      "> 71 | 0.4355531930923462 | 0.8362040519714355 | 0.8498425066312998 | 0.3944517118703149\n",
      "> 72 | 0.43532800674438477 | 0.8365299701690674 | 0.849925397877984 | 0.39420303813026186\n",
      "> 73 | 0.435154527425766 | 0.8368573188781738 | 0.8498425066312998 | 0.39387147314352444\n",
      "> 74 | 0.43491896986961365 | 0.8371866345405579 | 0.8498425066312998 | 0.3937056905711045\n",
      "> 75 | 0.434677392244339 | 0.8375679850578308 | 0.8499668435013262 | 0.39341557120770926\n",
      "> 76 | 0.43443551659584045 | 0.8379925489425659 | 0.8498425066312998 | 0.3930011149742875\n",
      "> 77 | 0.434195876121521 | 0.8383130431175232 | 0.8500911803713528 | 0.39287677818331224\n",
      "> 78 | 0.43396031856536865 | 0.8387512564659119 | 0.8500497347480106 | 0.39262810452231045\n",
      "> 79 | 0.4337279200553894 | 0.8391239643096924 | 0.8500497347480106 | 0.3923794307822574\n",
      "> 80 | 0.43349969387054443 | 0.8395414352416992 | 0.8500497347480106 | 0.39208931141886216\n",
      "> 81 | 0.43317586183547974 | 0.8400850892066956 | 0.8501740716180372 | 0.3913432901196518\n",
      "> 82 | 0.43284744024276733 | 0.8404925465583801 | 0.850381299734748 | 0.3913018444963096\n",
      "> 83 | 0.4325147271156311 | 0.8408305048942566 | 0.8505056366047745 | 0.39105317075625656\n",
      "> 84 | 0.4321892261505127 | 0.8413242101669312 | 0.8506299734748011 | 0.3908459426395457\n",
      "> 85 | 0.43187689781188965 | 0.8417369723320007 | 0.8507543103448276 | 0.3908044970162035\n",
      "> 86 | 0.431578665971756 | 0.8422455787658691 | 0.8509200928381963 | 0.3908873882628878\n",
      "> 87 | 0.4313128590583801 | 0.8426218032836914 | 0.8508786472148541 | 0.3905143776528083\n",
      "> 88 | 0.43109026551246643 | 0.8430910110473633 | 0.8509200928381963 | 0.39022425828941304\n",
      "> 89 | 0.4308863878250122 | 0.8434079885482788 | 0.8510029840848806 | 0.3902657039127552\n",
      "> 90 | 0.4306933879852295 | 0.8437775373458862 | 0.851085875331565 | 0.38997558454936\n",
      "> 91 | 0.4305068254470825 | 0.844199538230896 | 0.8510029840848806 | 0.3897683564326491\n",
      "> 92 | 0.43032440543174744 | 0.844558835029602 | 0.8510029840848806 | 0.38951968269259607\n",
      "> 93 | 0.4301448464393616 | 0.8449721336364746 | 0.851085875331565 | 0.3894782370692539\n",
      "> 94 | 0.42996859550476074 | 0.845370888710022 | 0.8511687665782494 | 0.38918811770585865\n",
      "> 95 | 0.42979520559310913 | 0.8458135724067688 | 0.8510444297082228 | 0.38902233521248997\n",
      "> 96 | 0.42962437868118286 | 0.846347451210022 | 0.8511273209549072 | 0.38902233521248997\n",
      "> 97 | 0.4294567108154297 | 0.8466199040412903 | 0.8511273209549072 | 0.3888565527191213\n",
      "> 98 | 0.42929112911224365 | 0.8471215963363647 | 0.8511687665782494 | 0.38860787897906823\n",
      "> 99 | 0.42912808060646057 | 0.847453773021698 | 0.8511687665782494 | 0.38819342274564644\n",
      "> 100 | 0.4289683997631073 | 0.8478695750236511 | 0.8512931034482759 | 0.3879861946289356\n",
      "> Evaluation\n",
      "> Class Acc = 0.8512300531914894\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.14380833506584167 | 0.6837138354312629 | 0.4257073774933815\n",
      "> Confusion Matrix \n",
      "TN: 4236.0 | FP: 322.0 \n",
      "FN: 573.0 | TP: 885.0\n"
     ]
    }
   ],
   "source": [
    "fairdef = 'DemPar'\n",
    "\n",
    "for i in range(test_loop):\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    model = FairLogisticRegression(xdim, ydim, adim, batch_size, fairdef)\n",
    "    zhang_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "    Y, A, Y_hat, A_hat = fair_evaluation(model, valid_data)\n",
    "    \n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix  = compute_metrics(Y, A, Y_hat, adim=adim)\n",
    "\n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    tradeoff = []\n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "    result = ['Zhang4DP', clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]]\n",
    "\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zhang for Eq Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.6225697994232178 | 0.9331574440002441 | 0.5320374668435013 | 0.016757749277969886\n",
      "> 2 | 0.5413908362388611 | 0.7949942946434021 | 0.6870855437665783 | 0.38334428552607325\n",
      "> 3 | 0.470356822013855 | 0.8387948274612427 | 0.7843998673740054 | 0.40821165945232707\n",
      "> 4 | 0.4523882269859314 | 0.8009197115898132 | 0.8260527188328912 | 0.4198578797695808\n",
      "> 5 | 0.4458237290382385 | 0.7848763465881348 | 0.8311090848806366 | 0.4342395110693155\n",
      "> 6 | 0.44354134798049927 | 0.7742348909378052 | 0.8335958222811671 | 0.4395859961642511\n",
      "> 7 | 0.44286924600601196 | 0.7669211626052856 | 0.8355437665782494 | 0.44397923255472665\n",
      "> 8 | 0.44291868805885315 | 0.7617388963699341 | 0.8370772546419099 | 0.44733632851975036\n",
      "> 9 | 0.44331079721450806 | 0.7577712535858154 | 0.8377403846153846 | 0.4500302939579405\n",
      "> 10 | 0.4438524842262268 | 0.7547512650489807 | 0.838776525198939 | 0.45239269448844444\n",
      "> 11 | 0.444433331489563 | 0.7521641254425049 | 0.8389837533156499 | 0.4546722035351103\n",
      "> 12 | 0.4449842870235443 | 0.7498517632484436 | 0.8395639920424404 | 0.4562885832407114\n",
      "> 13 | 0.4454643130302429 | 0.7477827072143555 | 0.8400613395225465 | 0.45889965766937096\n",
      "> 14 | 0.4458528161048889 | 0.7459356784820557 | 0.8403100132625995 | 0.46010158050914024\n",
      "> 15 | 0.4461364150047302 | 0.744050145149231 | 0.8406001326259946 | 0.46184229684761413\n",
      "> 16 | 0.44631606340408325 | 0.7424817681312561 | 0.8410974801061007 | 0.4637902412237476\n",
      "> 17 | 0.4463939070701599 | 0.7409534454345703 | 0.8413047082228117 | 0.46532372928740806\n",
      "> 18 | 0.44637754559516907 | 0.7396944761276245 | 0.8415948275862069 | 0.46702300008159103\n",
      "> 19 | 0.4462745189666748 | 0.7386084794998169 | 0.842092175066313 | 0.4680591409813504\n",
      "> 20 | 0.4460938572883606 | 0.7377200126647949 | 0.8421750663129973 | 0.4700899764460657\n",
      "> 21 | 0.4458470940589905 | 0.7368831038475037 | 0.8424651856763926 | 0.47137479076967315\n",
      "> 22 | 0.445543110370636 | 0.7362351417541504 | 0.8427553050397878 | 0.47183069231023206\n",
      "> 23 | 0.4451904296875 | 0.7357163429260254 | 0.8431283156498673 | 0.47220370292031166\n",
      "> 24 | 0.4447985589504242 | 0.7354456186294556 | 0.8432112068965517 | 0.4730326150709501\n",
      "> 25 | 0.4443764388561249 | 0.7352088689804077 | 0.8433355437665783 | 0.4740273101102136\n",
      "> 26 | 0.4439300298690796 | 0.7349814176559448 | 0.8432526525198939 | 0.47448321188792625\n",
      "> 27 | 0.4434683322906494 | 0.7347457408905029 | 0.8437914456233422 | 0.47502200499137454\n",
      "> 28 | 0.4430093765258789 | 0.7346591949462891 | 0.8442059018567639 | 0.4757265809834478\n",
      "> 29 | 0.44253745675086975 | 0.7346729040145874 | 0.8440815649867374 | 0.47663838461792435\n",
      "> 30 | 0.4420222342014313 | 0.7348787784576416 | 0.8444131299734748 | 0.4772186236609198\n",
      "> 31 | 0.44150012731552124 | 0.7348510026931763 | 0.8442473474801061 | 0.47730151498665546\n",
      "> 32 | 0.44099533557891846 | 0.7350531816482544 | 0.8443302387267905 | 0.4775916344291019\n",
      "> 33 | 0.4404972791671753 | 0.7353389263153076 | 0.8442473474801061 | 0.4774672977171779\n",
      "> 34 | 0.4400099515914917 | 0.7356626987457275 | 0.8444131299734748 | 0.4784205469749967\n",
      "> 35 | 0.439521849155426 | 0.7360898852348328 | 0.8444131299734748 | 0.4785448838450232\n",
      "> 36 | 0.43904614448547363 | 0.7365499138832092 | 0.8445789124668435 | 0.47862777524981004\n",
      "> 37 | 0.43858134746551514 | 0.7370766401290894 | 0.8447861405835544 | 0.47887644875270935\n",
      "> 38 | 0.43810945749282837 | 0.7376362085342407 | 0.8451591511936339 | 0.478793557506025\n",
      "> 39 | 0.43765556812286377 | 0.7383040189743042 | 0.8453249336870027 | 0.4787521117245803\n",
      "> 40 | 0.4371977746486664 | 0.7389100790023804 | 0.8453663793103449 | 0.47846199212403134\n",
      "> 41 | 0.4368117153644562 | 0.7395252585411072 | 0.84565649867374 | 0.478088981276798\n",
      "> 42 | 0.4365980625152588 | 0.7401019334793091 | 0.8454907161803713 | 0.47829620947256013\n",
      "> 43 | 0.43647146224975586 | 0.7408369779586792 | 0.8459466180371353 | 0.4785448832126132\n",
      "> 44 | 0.43632304668426514 | 0.7415129542350769 | 0.8459880636604774 | 0.47846199180782634\n",
      "> 45 | 0.4361497163772583 | 0.7422051429748535 | 0.8459051724137931 | 0.47846199172877507\n",
      "> 46 | 0.435935914516449 | 0.7429596185684204 | 0.8460295092838196 | 0.4783791004820907\n",
      "> 47 | 0.435710608959198 | 0.7437652349472046 | 0.8463610742705571 | 0.4783791004820907\n",
      "> 48 | 0.43546262383461 | 0.7443732619285583 | 0.8466097480106101 | 0.47850343735211726\n",
      "> 49 | 0.435197114944458 | 0.7452064752578735 | 0.846816976127321 | 0.4783376548587485\n",
      "> 50 | 0.4349190592765808 | 0.7460324764251709 | 0.8469827586206896 | 0.47821331790967075\n",
      "> 51 | 0.434685617685318 | 0.7468096017837524 | 0.8468584217506632 | 0.4781304265839352\n",
      "> 52 | 0.4344522953033447 | 0.7476223707199097 | 0.8468584217506632 | 0.4782547634539617\n",
      "> 53 | 0.43421339988708496 | 0.7486398220062256 | 0.8469413129973474 | 0.47829620907730386\n",
      "> 54 | 0.43396449089050293 | 0.7494074106216431 | 0.8470242042440318 | 0.4780060897929599\n",
      "> 55 | 0.4337221682071686 | 0.7503812313079834 | 0.8472728779840849 | 0.4778403072995912\n",
      "> 56 | 0.4334651827812195 | 0.7511365413665771 | 0.8471899867374005 | 0.477798861676249\n",
      "> 57 | 0.4332271218299866 | 0.7514796257019043 | 0.8473143236074271 | 0.47788175292293333\n",
      "> 58 | 0.4329902231693268 | 0.7518846392631531 | 0.8475215517241379 | 0.4777574161319581\n",
      "> 59 | 0.4327699542045593 | 0.7520960569381714 | 0.8476873342175066 | 0.4777988617553003\n",
      "> 60 | 0.43254002928733826 | 0.7523085474967957 | 0.8477287798408488 | 0.47767452488527373\n",
      "> 61 | 0.43231040239334106 | 0.752723217010498 | 0.8478945623342176 | 0.4777159705086159\n",
      "> 62 | 0.4321208596229553 | 0.7529700994491577 | 0.8483504641909815 | 0.4775087422338025\n",
      "> 63 | 0.4319339692592621 | 0.7532121539115906 | 0.8482675729442971 | 0.47779886159719775\n",
      "> 64 | 0.431751549243927 | 0.7536057233810425 | 0.8483090185676393 | 0.47775741597385557\n",
      "> 65 | 0.43155068159103394 | 0.7538822889328003 | 0.8483919098143236 | 0.4777159705086159\n",
      "> 66 | 0.43135589361190796 | 0.7542404532432556 | 0.8486405835543767 | 0.47763307926193155\n",
      "> 67 | 0.43116137385368347 | 0.7544949650764465 | 0.8488478116710876 | 0.47746729676856287\n",
      "> 68 | 0.4309714436531067 | 0.754806637763977 | 0.8488063660477454 | 0.4772186230285098\n",
      "> 69 | 0.43074631690979004 | 0.7552497982978821 | 0.8488063660477454 | 0.47701139491179895\n",
      "> 70 | 0.4305211901664734 | 0.7555668354034424 | 0.8491379310344828 | 0.47713573162372297\n",
      "> 71 | 0.43030253052711487 | 0.7559557557106018 | 0.8491379310344828 | 0.47696994913035423\n",
      "> 72 | 0.4300920069217682 | 0.7563676834106445 | 0.8489307029177718 | 0.47680416663698555\n",
      "> 73 | 0.4298973083496094 | 0.7567222118377686 | 0.848972148541114 | 0.476845612339379\n",
      "> 74 | 0.42971861362457275 | 0.7571356296539307 | 0.8490550397877984 | 0.4769285035860633\n",
      "> 75 | 0.42953744530677795 | 0.757690966129303 | 0.8491379310344828 | 0.47684561226032773\n",
      "> 76 | 0.42935892939567566 | 0.7579289078712463 | 0.8491379310344828 | 0.47655549273883\n",
      "> 77 | 0.42919740080833435 | 0.7583326101303101 | 0.8493866047745358 | 0.47659693836217215\n",
      "> 78 | 0.42904892563819885 | 0.7587141394615173 | 0.8493037135278515 | 0.47659693836217215\n",
      "> 79 | 0.4289105236530304 | 0.7589931488037109 | 0.8494694960212201 | 0.47663838398551434\n",
      "> 80 | 0.4287830889225006 | 0.7594718933105469 | 0.8493866047745358 | 0.47659693836217215\n",
      "> 81 | 0.4286569654941559 | 0.7594984769821167 | 0.8493866047745358 | 0.47647260165024813\n",
      "> 82 | 0.42856085300445557 | 0.7596274018287659 | 0.8495109416445623 | 0.47634826478022163\n",
      "> 83 | 0.42847511172294617 | 0.7597576379776001 | 0.8496352785145889 | 0.4762653736125885\n",
      "> 84 | 0.42838993668556213 | 0.7599506974220276 | 0.8497181697612732 | 0.47609959111921984\n",
      "> 85 | 0.4283043146133423 | 0.760022759437561 | 0.8497596153846154 | 0.47622392798924634\n",
      "> 86 | 0.4282945394515991 | 0.7601149678230286 | 0.8497596153846154 | 0.4761410365844595\n",
      "> 87 | 0.42827680706977844 | 0.7602734565734863 | 0.8500082891246684 | 0.4759752540910908\n",
      "> 88 | 0.4282541871070862 | 0.7604628205299377 | 0.8500497347480106 | 0.47609959088206605\n",
      "> 89 | 0.4282130002975464 | 0.7606819868087769 | 0.8501740716180372 | 0.476016699714433\n",
      "> 90 | 0.42817071080207825 | 0.7610070705413818 | 0.850381299734748 | 0.47605814533777513\n",
      "> 91 | 0.4281246066093445 | 0.761156439781189 | 0.8505056366047745 | 0.4759752540910908\n",
      "> 92 | 0.4281036853790283 | 0.7613636255264282 | 0.8506714190981433 | 0.47585091722106426\n",
      "> 93 | 0.42809659242630005 | 0.7616674900054932 | 0.8505885278514589 | 0.47593380846774863\n",
      "> 94 | 0.42808854579925537 | 0.7618874907493591 | 0.8505056366047745 | 0.47605814533777513\n",
      "> 95 | 0.42806369066238403 | 0.7622030973434448 | 0.8505885278514589 | 0.4759752540910908\n",
      "> 96 | 0.4280267357826233 | 0.7625257968902588 | 0.8506714190981433 | 0.47605814533777513\n",
      "> 97 | 0.42799532413482666 | 0.7626878023147583 | 0.8507543103448276 | 0.4759752540910908\n",
      "> 98 | 0.4279592037200928 | 0.7629508376121521 | 0.8508786472148541 | 0.47585091722106426\n",
      "> 99 | 0.42792776226997375 | 0.7631915807723999 | 0.8510444297082228 | 0.47593380846774863\n",
      "> 100 | 0.42787548899650574 | 0.7635048627853394 | 0.8512516578249337 | 0.4758923629234577\n",
      "> Evaluation\n",
      "> Class Acc = 0.8508976063829787\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.14169514179229736 | 0.6821408688556403 | 0.4212815538048744\n",
      "> Confusion Matrix \n",
      "TN: 4236.0 | FP: 322.0 \n",
      "FN: 575.0 | TP: 883.0\n"
     ]
    }
   ],
   "source": [
    "fairdef = 'EqOdds'\n",
    "\n",
    "for i in range(test_loop):\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    model = FairLogisticRegression(xdim, ydim, adim, batch_size, fairdef)\n",
    "    zhang_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "    Y, A, Y_hat, A_hat = fair_evaluation(model, valid_data)\n",
    "    \n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix  = compute_metrics(Y, A, Y_hat, adim=adim)\n",
    "\n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    tradeoff = []\n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "    result = ['Zhang4EqOdds', clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]]\n",
    "\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zhang for Eq Opp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.5086170434951782 | 0.4394846558570862 | 0.6637516578249337 | -0.0019756742118208098\n",
      "> 2 | 0.461321622133255 | 0.3287355303764343 | 0.8246850132625995 | 0.07602498860194765\n",
      "> 3 | 0.4544013738632202 | 0.28555962443351746 | 0.8296170424403183 | 0.13578957682895407\n",
      "> 4 | 0.45495688915252686 | 0.25399768352508545 | 0.8314820954907162 | 0.17126703040985594\n",
      "> 5 | 0.45406633615493774 | 0.23822914063930511 | 0.833347148541114 | 0.1967560887652936\n",
      "> 6 | 0.4538351595401764 | 0.22981365025043488 | 0.8345905172413793 | 0.21217386064858273\n",
      "> 7 | 0.4541389048099518 | 0.22525933384895325 | 0.8356266578249337 | 0.22191358213399387\n",
      "> 8 | 0.4548230469226837 | 0.22293925285339355 | 0.8366627984084881 | 0.22746729566184531\n",
      "> 9 | 0.45577552914619446 | 0.22195583581924438 | 0.8379061671087533 | 0.23152896674937848\n",
      "> 10 | 0.4569042921066284 | 0.22180694341659546 | 0.8386521883289124 | 0.23409859539659333\n",
      "> 11 | 0.4581281244754791 | 0.2221590280532837 | 0.8395225464190982 | 0.23654388717378166\n",
      "> 12 | 0.45938101410865784 | 0.22277331352233887 | 0.8401856763925729 | 0.2374556908873095\n",
      "> 13 | 0.46061307191848755 | 0.22353613376617432 | 0.8406001326259946 | 0.23803592961409997\n",
      "> 14 | 0.4617882966995239 | 0.2243744134902954 | 0.8411803713527851 | 0.23840894022417955\n",
      "> 15 | 0.46288228034973145 | 0.22515392303466797 | 0.8420507294429708 | 0.2386576139642326\n",
      "> 16 | 0.4638811945915222 | 0.22594910860061646 | 0.8426724137931034 | 0.23882339645760128\n",
      "> 17 | 0.4647793769836426 | 0.22661934792995453 | 0.8432940981432361 | 0.23890628770428565\n",
      "> 18 | 0.46557557582855225 | 0.22725623846054077 | 0.8438328912466844 | 0.23903062457431218\n",
      "> 19 | 0.46627095341682434 | 0.22783592343330383 | 0.8445789124668435 | 0.23903062457431218\n",
      "> 20 | 0.466869980096817 | 0.2283564805984497 | 0.8447446949602122 | 0.23886484208094347\n",
      "> 21 | 0.4673762023448944 | 0.2288477122783661 | 0.8450348143236074 | 0.23882339645760128\n",
      "> 22 | 0.4677959978580475 | 0.22926223278045654 | 0.8453663793103449 | 0.23882339645760128\n",
      "> 23 | 0.46813392639160156 | 0.22963577508926392 | 0.84565649867374 | 0.23861616834089042\n",
      "> 24 | 0.46839600801467896 | 0.22997057437896729 | 0.8456150530503979 | 0.23832604897749518\n",
      "> 25 | 0.4685889482498169 | 0.23029197752475739 | 0.8461538461538461 | 0.23807737523744213\n",
      "> 26 | 0.4687191843986511 | 0.23056554794311523 | 0.8460709549071618 | 0.2379530383674156\n",
      "> 27 | 0.4687942862510681 | 0.23081637918949127 | 0.8460295092838196 | 0.23774581025070474\n",
      "> 28 | 0.4688199758529663 | 0.2310420721769333 | 0.8458222811671088 | 0.23749713651065169\n",
      "> 29 | 0.4688037633895874 | 0.23120811581611633 | 0.8460295092838196 | 0.2374556908873095\n",
      "> 30 | 0.4687519669532776 | 0.23141345381736755 | 0.8461952917771883 | 0.2374556908873095\n",
      "> 31 | 0.46867018938064575 | 0.2315555214881897 | 0.8462367374005305 | 0.23770436462736255\n",
      "> 32 | 0.46856358647346497 | 0.23173761367797852 | 0.8462781830238727 | 0.2376629190040204\n",
      "> 33 | 0.46843594312667847 | 0.23183514177799225 | 0.846816976127321 | 0.23753858213399387\n",
      "> 34 | 0.4682921767234802 | 0.23196780681610107 | 0.8469827586206896 | 0.2374556908873095\n",
      "> 35 | 0.46813422441482544 | 0.23209945857524872 | 0.8474801061007957 | 0.2371241259005721\n",
      "> 36 | 0.4679655432701111 | 0.23222266137599945 | 0.8476873342175066 | 0.23687545216051906\n",
      "> 37 | 0.4677875339984894 | 0.23233652114868164 | 0.8475215517241379 | 0.236626778420466\n",
      "> 38 | 0.4676029086112976 | 0.23241454362869263 | 0.8474801061007957 | 0.23654388717378166\n",
      "> 39 | 0.46741318702697754 | 0.23251435160636902 | 0.8479360079575596 | 0.2362952134337286\n",
      "> 40 | 0.46721893548965454 | 0.23261301219463348 | 0.8481846816976127 | 0.2358393115769647\n",
      "> 41 | 0.46702203154563904 | 0.23271237313747406 | 0.8482675729442971 | 0.23559063783691164\n",
      "> 42 | 0.46682262420654297 | 0.23281961679458618 | 0.8481432360742706 | 0.2353005184735164\n",
      "> 43 | 0.46662214398384094 | 0.2329094409942627 | 0.848018899204244 | 0.23521762722683207\n",
      "> 44 | 0.46642088890075684 | 0.23299475014209747 | 0.8483919098143236 | 0.2351347359801477\n",
      "> 45 | 0.4662192761898041 | 0.23307445645332336 | 0.8485162466843501 | 0.23505184473346336\n",
      "> 46 | 0.46601855754852295 | 0.23322418332099915 | 0.8483919098143236 | 0.2348446166167525\n",
      "> 47 | 0.4658183455467224 | 0.23331427574157715 | 0.8486405835543767 | 0.23472027974672596\n",
      "> 48 | 0.4656197428703308 | 0.23334401845932007 | 0.8486405835543767 | 0.23467883412338378\n",
      "> 49 | 0.46542268991470337 | 0.23348918557167053 | 0.8488892572944297 | 0.23459594287669944\n",
      "> 50 | 0.46522754430770874 | 0.23353615403175354 | 0.8490135941644562 | 0.2344716060066729\n",
      "> 51 | 0.4650343060493469 | 0.23362240195274353 | 0.8492622679045093 | 0.23414004101993552\n",
      "> 52 | 0.4648436903953552 | 0.23373112082481384 | 0.8494280503978779 | 0.2339742585265668\n",
      "> 53 | 0.4646558165550232 | 0.23381495475769043 | 0.8493866047745358 | 0.234015704149909\n",
      "> 54 | 0.4644700288772583 | 0.2338842898607254 | 0.8496352785145889 | 0.234015704149909\n",
      "> 55 | 0.4642878770828247 | 0.23397408425807953 | 0.8497181697612732 | 0.23409859539659333\n",
      "> 56 | 0.46410873532295227 | 0.23408330976963043 | 0.849925397877984 | 0.2339742585265668\n",
      "> 57 | 0.46393245458602905 | 0.2341611534357071 | 0.8501740716180372 | 0.23372558478651376\n",
      "> 58 | 0.4637588560581207 | 0.23426049947738647 | 0.8502569628647215 | 0.23343546542311852\n",
      "> 59 | 0.4635884761810303 | 0.23436352610588074 | 0.8502569628647215 | 0.23339401979977636\n",
      "> 60 | 0.4634218215942383 | 0.23444804549217224 | 0.8502984084880637 | 0.2334769110464607\n",
      "> 61 | 0.463258296251297 | 0.2345520257949829 | 0.8505056366047745 | 0.233311128553092\n",
      "> 62 | 0.46309810876846313 | 0.23465335369110107 | 0.8505470822281167 | 0.2330210091896968\n",
      "> 63 | 0.46294113993644714 | 0.23477306962013245 | 0.8507957559681698 | 0.23306245481303894\n",
      "> 64 | 0.4627875089645386 | 0.23487095534801483 | 0.8507128647214854 | 0.23273088982630155\n",
      "> 65 | 0.4626377820968628 | 0.23496340215206146 | 0.8508372015915119 | 0.23268944420295937\n",
      "> 66 | 0.4624911844730377 | 0.23508280515670776 | 0.8508786472148541 | 0.2326479985796172\n",
      "> 67 | 0.462348073720932 | 0.2351716011762619 | 0.8510029840848806 | 0.23252366170959068\n",
      "> 68 | 0.46220842003822327 | 0.2352764904499054 | 0.851085875331565 | 0.2321506510995111\n",
      "> 69 | 0.462072491645813 | 0.23539341986179352 | 0.8512931034482759 | 0.2321506510995111\n",
      "> 70 | 0.4619396924972534 | 0.23550494015216827 | 0.8514588859416445 | 0.23202631422948458\n",
      "> 71 | 0.461810827255249 | 0.23565199971199036 | 0.8513759946949602 | 0.23202631422948458\n",
      "> 72 | 0.46168482303619385 | 0.2357288897037506 | 0.8514174403183024 | 0.23181908611277371\n",
      "> 73 | 0.46156296133995056 | 0.23586851358413696 | 0.8515417771883289 | 0.2316947492427472\n",
      "> 74 | 0.4614439606666565 | 0.23597940802574158 | 0.8515003315649867 | 0.23152896674937848\n",
      "> 75 | 0.46132892370224 | 0.23610025644302368 | 0.8514588859416445 | 0.23140462987935195\n",
      "> 76 | 0.4612169861793518 | 0.23620525002479553 | 0.8514588859416445 | 0.23119740176264109\n",
      "> 77 | 0.4611085057258606 | 0.23633763194084167 | 0.8516246684350133 | 0.23111451051595672\n",
      "> 78 | 0.46100345253944397 | 0.23645183444023132 | 0.851790450928382 | 0.23103161926927238\n",
      "> 79 | 0.46090197563171387 | 0.23659083247184753 | 0.8518318965517241 | 0.23094872802258803\n",
      "> 80 | 0.460803747177124 | 0.2367217242717743 | 0.8518733421750663 | 0.2306586086591928\n",
      "> 81 | 0.46070873737335205 | 0.23683568835258484 | 0.8518733421750663 | 0.23036848929579756\n",
      "> 82 | 0.46061697602272034 | 0.23696771264076233 | 0.8519976790450928 | 0.23024415242577104\n",
      "> 83 | 0.46052873134613037 | 0.23710089921951294 | 0.8519976790450928 | 0.23024415242577104\n",
      "> 84 | 0.4604436457157135 | 0.23723255097866058 | 0.8521220159151194 | 0.2301612611790867\n",
      "> 85 | 0.460361510515213 | 0.23739440739154816 | 0.8518733421750663 | 0.23011981555574454\n",
      "> 86 | 0.46028319001197815 | 0.23751749098300934 | 0.8519976790450928 | 0.23007836993240235\n",
      "> 87 | 0.46020740270614624 | 0.23765859007835388 | 0.8520805702917772 | 0.23011981555574454\n",
      "> 88 | 0.46013468503952026 | 0.23779211938381195 | 0.8522049071618037 | 0.2301612611790867\n",
      "> 89 | 0.4600648283958435 | 0.23792041838169098 | 0.8523706896551724 | 0.23007836993240235\n",
      "> 90 | 0.4599986970424652 | 0.23806342482566833 | 0.8524121352785146 | 0.22970535932232278\n",
      "> 91 | 0.4599350392818451 | 0.2382488250732422 | 0.8524121352785146 | 0.22978825056900712\n",
      "> 92 | 0.4598737359046936 | 0.2383643090724945 | 0.8524121352785146 | 0.22958102245229625\n",
      "> 93 | 0.45981597900390625 | 0.23848973214626312 | 0.8524535809018567 | 0.22958102245229625\n",
      "> 94 | 0.4597610533237457 | 0.23864266276359558 | 0.8524121352785146 | 0.2293323487122432\n",
      "> 95 | 0.4597088694572449 | 0.2387753278017044 | 0.8525779177718833 | 0.22912512059553233\n",
      "> 96 | 0.45965859293937683 | 0.23892001807689667 | 0.8526193633952255 | 0.22908367497219015\n",
      "> 97 | 0.459611713886261 | 0.23906932771205902 | 0.852743700265252 | 0.22904222934884796\n",
      "> 98 | 0.45956727862358093 | 0.23921158909797668 | 0.8527851458885941 | 0.22891789247882144\n",
      "> 99 | 0.45952537655830383 | 0.23935726284980774 | 0.8527851458885941 | 0.2287935556087949\n",
      "> 100 | 0.4594857692718506 | 0.23950353264808655 | 0.8529094827586207 | 0.2287935556087949\n",
      "> Evaluation\n",
      "> Class Acc = 0.8500664893617021\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.14072921872138977 | 0.6800524684367701 | 0.4160023555159569\n",
      "> Confusion Matrix \n",
      "TN: 4239.0 | FP: 319.0 \n",
      "FN: 583.0 | TP: 875.0\n"
     ]
    }
   ],
   "source": [
    "fairdef = 'EqOpp'\n",
    "\n",
    "for i in range(test_loop):\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    model = FairLogisticRegression(xdim, ydim, adim, batch_size, fairdef)\n",
    "    zhang_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "    Y, A, Y_hat, A_hat = fair_evaluation(model, valid_data)\n",
    "    \n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix  = compute_metrics(Y, A, Y_hat, adim=adim)\n",
    "\n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    tradeoff = []\n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "    result = ['Zhang4EqOpp', clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]]\n",
    "\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving into DF then CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>clas_acc</th>\n",
       "      <th>dp</th>\n",
       "      <th>deqodds</th>\n",
       "      <th>deqopp</th>\n",
       "      <th>trade_dp</th>\n",
       "      <th>trade_deqodds</th>\n",
       "      <th>trade_deqopp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zhang4DP</td>\n",
       "      <td>0.851230</td>\n",
       "      <td>0.143808</td>\n",
       "      <td>0.683714</td>\n",
       "      <td>0.425707</td>\n",
       "      <td>0.246049</td>\n",
       "      <td>0.758331</td>\n",
       "      <td>0.567569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zhang4EqOdds</td>\n",
       "      <td>0.850898</td>\n",
       "      <td>0.141695</td>\n",
       "      <td>0.682141</td>\n",
       "      <td>0.421282</td>\n",
       "      <td>0.242936</td>\n",
       "      <td>0.757231</td>\n",
       "      <td>0.563549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zhang4EqOpp</td>\n",
       "      <td>0.850066</td>\n",
       "      <td>0.140729</td>\n",
       "      <td>0.680052</td>\n",
       "      <td>0.416002</td>\n",
       "      <td>0.241481</td>\n",
       "      <td>0.755614</td>\n",
       "      <td>0.558626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name  clas_acc        dp   deqodds    deqopp  trade_dp  \\\n",
       "0      Zhang4DP  0.851230  0.143808  0.683714  0.425707  0.246049   \n",
       "1  Zhang4EqOdds  0.850898  0.141695  0.682141  0.421282  0.242936   \n",
       "2   Zhang4EqOpp  0.850066  0.140729  0.680052  0.416002  0.241481   \n",
       "\n",
       "   trade_deqodds  trade_deqopp  \n",
       "0       0.758331      0.567569  \n",
       "1       0.757231      0.563549  \n",
       "2       0.755614      0.558626  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(results, columns=header)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f'{data_name}-result/zhang-{epochs}.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34ca74ed6235dfc7dda926bb3adb31e801e3d02679121d5b444ee035e270bd57"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('falsb': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
