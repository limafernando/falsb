{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.data import Dataset\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from models.unfair_lr.models import UnfairLogisticRegression\n",
    "from models.unfair_lr.learning import train_loop\n",
    "from util.evaluation import *\n",
    "from util.load_data import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_seeds = [13, 29, 42, 55, 73]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'german'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, a = load_data(data_name)\n",
    "raw_data = (x, y, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdim = x.shape[1]\n",
    "ydim = y.shape[1]\n",
    "adim = a.shape[1]\n",
    "zdim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"model_name\", \"cv_seed\", \"clas_acc\", \"dp\", \"deqodds\", \"deqopp\", \"trade_dp\", \"trade_deqodds\", \"trade_deqopp\", \"TN_a0\", \"FP_a0\", \"FN_a0\", \"TP_a0\", \"TN_a1\", \"FP_a1\", \"FN_a1\", \"TP_a1\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5916500687599182 | 0.6734375\n",
      "> 2 | 0.589928388595581 | 0.6734375\n",
      "> 3 | 0.5888773202896118 | 0.6734375\n",
      "> 4 | 0.5881212949752808 | 0.6734375\n",
      "> 5 | 0.587532639503479 | 0.6734375\n",
      "> 6 | 0.5870517492294312 | 0.6734375\n",
      "> 7 | 0.5866460800170898 | 0.6734375\n",
      "> 8 | 0.5862957239151001 | 0.6734375\n",
      "> 9 | 0.5859876871109009 | 0.6734375\n",
      "> 10 | 0.5857132077217102 | 0.6734375\n",
      "> 11 | 0.585465669631958 | 0.6734375\n",
      "> 12 | 0.5852405428886414 | 0.6734375\n",
      "> 13 | 0.5850341320037842 | 0.6734375\n",
      "> 14 | 0.5848437547683716 | 0.6734375\n",
      "> 15 | 0.5846670866012573 | 0.6734375\n",
      "> 16 | 0.5845022797584534 | 0.6734375\n",
      "> 17 | 0.5843480229377747 | 0.6734375\n",
      "> 18 | 0.5842030048370361 | 0.6734375\n",
      "> 19 | 0.5840661525726318 | 0.6734375\n",
      "> 20 | 0.5839367508888245 | 0.6734375\n",
      "> 21 | 0.5838139653205872 | 0.6734375\n",
      "> 22 | 0.5836971998214722 | 0.6734375\n",
      "> 23 | 0.5835859775543213 | 0.6734375\n",
      "> 24 | 0.583479642868042 | 0.6734375\n",
      "> 25 | 0.5833779573440552 | 0.6734375\n",
      "> 26 | 0.5832804441452026 | 0.6734375\n",
      "> 27 | 0.5831868052482605 | 0.6734375\n",
      "> 28 | 0.5830968022346497 | 0.6734375\n",
      "> 29 | 0.5830101370811462 | 0.6734375\n",
      "> 30 | 0.5829265713691711 | 0.6734375\n",
      "> 31 | 0.58284592628479 | 0.6734375\n",
      "> 32 | 0.5827679634094238 | 0.6734375\n",
      "> 33 | 0.5826925039291382 | 0.6734375\n",
      "> 34 | 0.5826195478439331 | 0.6734375\n",
      "> 35 | 0.5825487375259399 | 0.6734375\n",
      "> 36 | 0.5824800729751587 | 0.6734375\n",
      "> 37 | 0.5824134349822998 | 0.6734375\n",
      "> 38 | 0.5823485851287842 | 0.6734375\n",
      "> 39 | 0.5822855830192566 | 0.6734375\n",
      "> 40 | 0.5822243094444275 | 0.6734375\n",
      "> 41 | 0.5821646451950073 | 0.6734375\n",
      "> 42 | 0.5821064114570618 | 0.6734375\n",
      "> 43 | 0.5820496082305908 | 0.6734375\n",
      "> 44 | 0.581994354724884 | 0.6734375\n",
      "> 45 | 0.5819402933120728 | 0.6734375\n",
      "> 46 | 0.5818875432014465 | 0.6734375\n",
      "> 47 | 0.5818359851837158 | 0.6734375\n",
      "> 48 | 0.5817855596542358 | 0.6734375\n",
      "> 49 | 0.5817362666130066 | 0.6734375\n",
      "> 50 | 0.5816880464553833 | 0.6734375\n",
      "> 51 | 0.5816408395767212 | 0.6734375\n",
      "> 52 | 0.5815945863723755 | 0.6734375\n",
      "> 53 | 0.5815492868423462 | 0.6734375\n",
      "> 54 | 0.5815048217773438 | 0.6734375\n",
      "> 55 | 0.5814613103866577 | 0.6734375\n",
      "> 56 | 0.5814185738563538 | 0.6734375\n",
      "> 57 | 0.5813766717910767 | 0.6734375\n",
      "> 58 | 0.5813355445861816 | 0.6734375\n",
      "> 59 | 0.5812951326370239 | 0.6734375\n",
      "> 60 | 0.5812554359436035 | 0.6734375\n",
      "> 61 | 0.5812164545059204 | 0.6734375\n",
      "> 62 | 0.5811782479286194 | 0.6734375\n",
      "> 63 | 0.5811405777931213 | 0.6734375\n",
      "> 64 | 0.5811035633087158 | 0.6734375\n",
      "> 65 | 0.5810672044754028 | 0.6734375\n",
      "> 66 | 0.581031322479248 | 0.6734375\n",
      "> 67 | 0.5809961557388306 | 0.6734375\n",
      "> 68 | 0.5809614658355713 | 0.6734375\n",
      "> 69 | 0.5809272527694702 | 0.6734375\n",
      "> 70 | 0.5808936357498169 | 0.6734375\n",
      "> 71 | 0.5808606147766113 | 0.6734375\n",
      "> 72 | 0.5808279514312744 | 0.6734375\n",
      "> 73 | 0.5807958245277405 | 0.6734375\n",
      "> 74 | 0.5807641744613647 | 0.6734375\n",
      "> 75 | 0.5807329416275024 | 0.6734375\n",
      "> 76 | 0.5807020664215088 | 0.6734375\n",
      "> 77 | 0.5806717872619629 | 0.6734375\n",
      "> 78 | 0.5806418657302856 | 0.6734375\n",
      "> 79 | 0.580612301826477 | 0.6734375\n",
      "> 80 | 0.5805832147598267 | 0.6734375\n",
      "> 81 | 0.5805544257164001 | 0.6734375\n",
      "> 82 | 0.5805259943008423 | 0.6734375\n",
      "> 83 | 0.5804980397224426 | 0.6734375\n",
      "> 84 | 0.5804703831672668 | 0.6734375\n",
      "> 85 | 0.5804430246353149 | 0.6734375\n",
      "> 86 | 0.5804160833358765 | 0.6734375\n",
      "> 87 | 0.5803894996643066 | 0.6734375\n",
      "> 88 | 0.5803632140159607 | 0.6734375\n",
      "> 89 | 0.5803371667861938 | 0.6734375\n",
      "> 90 | 0.5803115367889404 | 0.6734375\n",
      "> 91 | 0.5802861452102661 | 0.6734375\n",
      "> 92 | 0.5802609920501709 | 0.6734375\n",
      "> 93 | 0.5802361965179443 | 0.6734375\n",
      "> 94 | 0.5802116990089417 | 0.6734375\n",
      "> 95 | 0.5801874399185181 | 0.6734375\n",
      "> 96 | 0.5801635384559631 | 0.6734375\n",
      "> 97 | 0.5801398158073425 | 0.6734375\n",
      "> 98 | 0.5801163911819458 | 0.6734375\n",
      "> 99 | 0.5800932049751282 | 0.6734375\n",
      "> 100 | 0.5800702571868896 | 0.6734375\n",
      "> Evaluation\n",
      "> Class Acc = 0.734375\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 1.0 | 1.0 | 1.0\n",
      "> Confusion Matrix \n",
      "TN: 0.0 | FP: 68.0 \n",
      "FN: 0.0 | TP: 188.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 0.0 | FP: 26.0 \n",
      "FN: 0.0 | TP: 58.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 0.0 | FP: 42.0 \n",
      "FN: 0.0 | TP: 130.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.6538881063461304 | 0.7078125\n",
      "> 2 | 0.6519047021865845 | 0.7078125\n",
      "> 3 | 0.6506958603858948 | 0.7078125\n",
      "> 4 | 0.6498240828514099 | 0.7078125\n",
      "> 5 | 0.6491432189941406 | 0.7078125\n",
      "> 6 | 0.6485853791236877 | 0.7078125\n",
      "> 7 | 0.648113489151001 | 0.7078125\n",
      "> 8 | 0.6477049589157104 | 0.7078125\n",
      "> 9 | 0.6473450660705566 | 0.7078125\n",
      "> 10 | 0.6470235586166382 | 0.7078125\n",
      "> 11 | 0.646733283996582 | 0.7078125\n",
      "> 12 | 0.6464689373970032 | 0.7078125\n",
      "> 13 | 0.6462386846542358 | 0.7078125\n",
      "> 14 | 0.6460263133049011 | 0.7078125\n",
      "> 15 | 0.645828902721405 | 0.7078125\n",
      "> 16 | 0.6456446647644043 | 0.7078125\n",
      "> 17 | 0.6454718112945557 | 0.7078125\n",
      "> 18 | 0.6453092098236084 | 0.7078125\n",
      "> 19 | 0.645155668258667 | 0.7078125\n",
      "> 20 | 0.6450101733207703 | 0.7078125\n",
      "> 21 | 0.6448720097541809 | 0.7078125\n",
      "> 22 | 0.6447405815124512 | 0.7078125\n",
      "> 23 | 0.6446150541305542 | 0.7078125\n",
      "> 24 | 0.6444951295852661 | 0.7078125\n",
      "> 25 | 0.6443802118301392 | 0.7078125\n",
      "> 26 | 0.6442700624465942 | 0.7078125\n",
      "> 27 | 0.6441640853881836 | 0.7078125\n",
      "> 28 | 0.6440621614456177 | 0.7078125\n",
      "> 29 | 0.6439639925956726 | 0.7078125\n",
      "> 30 | 0.6438692212104797 | 0.7078125\n",
      "> 31 | 0.6437777280807495 | 0.7078125\n",
      "> 32 | 0.6436891555786133 | 0.7078125\n",
      "> 33 | 0.643603503704071 | 0.7078125\n",
      "> 34 | 0.6435204744338989 | 0.7078125\n",
      "> 35 | 0.6434399485588074 | 0.7078125\n",
      "> 36 | 0.6433618068695068 | 0.7078125\n",
      "> 37 | 0.643285870552063 | 0.7078125\n",
      "> 38 | 0.6432119607925415 | 0.7078125\n",
      "> 39 | 0.6431401968002319 | 0.7078125\n",
      "> 40 | 0.6430702209472656 | 0.7078125\n",
      "> 41 | 0.6430020332336426 | 0.7078125\n",
      "> 42 | 0.6429356336593628 | 0.7078125\n",
      "> 43 | 0.6428707838058472 | 0.7078125\n",
      "> 44 | 0.6428074836730957 | 0.7078125\n",
      "> 45 | 0.6427457332611084 | 0.7078125\n",
      "> 46 | 0.6426853537559509 | 0.7078125\n",
      "> 47 | 0.6426263451576233 | 0.7078125\n",
      "> 48 | 0.6425685882568359 | 0.7078125\n",
      "> 49 | 0.6425120830535889 | 0.7078125\n",
      "> 50 | 0.6424568295478821 | 0.7078125\n",
      "> 51 | 0.6424026489257812 | 0.7078125\n",
      "> 52 | 0.6423496007919312 | 0.7078125\n",
      "> 53 | 0.642297625541687 | 0.7078125\n",
      "> 54 | 0.6422466039657593 | 0.7078125\n",
      "> 55 | 0.6421966552734375 | 0.7078125\n",
      "> 56 | 0.6421475410461426 | 0.7078125\n",
      "> 57 | 0.6420993804931641 | 0.7078125\n",
      "> 58 | 0.6420521140098572 | 0.7078125\n",
      "> 59 | 0.6420056223869324 | 0.7078125\n",
      "> 60 | 0.6419600248336792 | 0.7078125\n",
      "> 61 | 0.6419151425361633 | 0.7078125\n",
      "> 62 | 0.6418710947036743 | 0.7078125\n",
      "> 63 | 0.6418277621269226 | 0.7078125\n",
      "> 64 | 0.6417850852012634 | 0.7078125\n",
      "> 65 | 0.6417431831359863 | 0.7078125\n",
      "> 66 | 0.6417019367218018 | 0.7078125\n",
      "> 67 | 0.6416612863540649 | 0.7078125\n",
      "> 68 | 0.6416213512420654 | 0.7078125\n",
      "> 69 | 0.6415818929672241 | 0.7078125\n",
      "> 70 | 0.6415430903434753 | 0.7078125\n",
      "> 71 | 0.6415048837661743 | 0.7078125\n",
      "> 72 | 0.641467273235321 | 0.7078125\n",
      "> 73 | 0.641430139541626 | 0.7078125\n",
      "> 74 | 0.6413936018943787 | 0.7078125\n",
      "> 75 | 0.6413575410842896 | 0.7078125\n",
      "> 76 | 0.6413219571113586 | 0.7078125\n",
      "> 77 | 0.6412868499755859 | 0.7078125\n",
      "> 78 | 0.6412522196769714 | 0.7078125\n",
      "> 79 | 0.6412181854248047 | 0.7078125\n",
      "> 80 | 0.6411844491958618 | 0.7078125\n",
      "> 81 | 0.6411511898040771 | 0.7078125\n",
      "> 82 | 0.6411184072494507 | 0.7078125\n",
      "> 83 | 0.6410859823226929 | 0.7078125\n",
      "> 84 | 0.6410539746284485 | 0.7078125\n",
      "> 85 | 0.6410223841667175 | 0.7078125\n",
      "> 86 | 0.6409912109375 | 0.7078125\n",
      "> 87 | 0.6409603357315063 | 0.7078125\n",
      "> 88 | 0.6409299373626709 | 0.7078125\n",
      "> 89 | 0.6408998966217041 | 0.7078125\n",
      "> 90 | 0.6408702731132507 | 0.7078125\n",
      "> 91 | 0.6408410668373108 | 0.7078125\n",
      "> 92 | 0.640812337398529 | 0.7078125\n",
      "> 93 | 0.6407840251922607 | 0.7078125\n",
      "> 94 | 0.6407560110092163 | 0.7078125\n",
      "> 95 | 0.640728235244751 | 0.7078125\n",
      "> 96 | 0.6407009363174438 | 0.7078125\n",
      "> 97 | 0.6406738758087158 | 0.7078125\n",
      "> 98 | 0.6406470537185669 | 0.7078125\n",
      "> 99 | 0.6406205892562866 | 0.7078125\n",
      "> 100 | 0.6405943632125854 | 0.7078125\n",
      "> Evaluation\n",
      "> Class Acc = 0.66015625\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 1.0 | 1.0 | 1.0\n",
      "> Confusion Matrix \n",
      "TN: 0.0 | FP: 87.0 \n",
      "FN: 0.0 | TP: 169.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 0.0 | FP: 36.0 \n",
      "FN: 0.0 | TP: 43.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 0.0 | FP: 51.0 \n",
      "FN: 0.0 | TP: 126.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.6229864954948425 | 0.7078125\n",
      "> 2 | 0.6212197542190552 | 0.7078125\n",
      "> 3 | 0.6201352477073669 | 0.7078125\n",
      "> 4 | 0.6193556785583496 | 0.7078125\n",
      "> 5 | 0.6187447309494019 | 0.7078125\n",
      "> 6 | 0.6182428598403931 | 0.7078125\n",
      "> 7 | 0.6178172826766968 | 0.7078125\n",
      "> 8 | 0.6174480319023132 | 0.7078125\n",
      "> 9 | 0.617122232913971 | 0.7078125\n",
      "> 10 | 0.6168308258056641 | 0.7078125\n",
      "> 11 | 0.6165672540664673 | 0.7078125\n",
      "> 12 | 0.6163268685340881 | 0.7078125\n",
      "> 13 | 0.616105854511261 | 0.7078125\n",
      "> 14 | 0.6159014105796814 | 0.7078125\n",
      "> 15 | 0.6157112121582031 | 0.7078125\n",
      "> 16 | 0.6155335903167725 | 0.7078125\n",
      "> 17 | 0.6153668165206909 | 0.7078125\n",
      "> 18 | 0.6152098178863525 | 0.7078125\n",
      "> 19 | 0.6150614023208618 | 0.7078125\n",
      "> 20 | 0.6149207353591919 | 0.7078125\n",
      "> 21 | 0.6147871613502502 | 0.7078125\n",
      "> 22 | 0.61465984582901 | 0.7078125\n",
      "> 23 | 0.614538311958313 | 0.7078125\n",
      "> 24 | 0.6144222021102905 | 0.7078125\n",
      "> 25 | 0.6143108010292053 | 0.7078125\n",
      "> 26 | 0.6142038702964783 | 0.7078125\n",
      "> 27 | 0.6141011714935303 | 0.7078125\n",
      "> 28 | 0.6140022277832031 | 0.7078125\n",
      "> 29 | 0.6139068603515625 | 0.7078125\n",
      "> 30 | 0.6138148307800293 | 0.7078125\n",
      "> 31 | 0.6137259006500244 | 0.7078125\n",
      "> 32 | 0.6136398911476135 | 0.7078125\n",
      "> 33 | 0.6135565042495728 | 0.7078125\n",
      "> 34 | 0.6134757995605469 | 0.7078125\n",
      "> 35 | 0.6133973598480225 | 0.7078125\n",
      "> 36 | 0.6133213043212891 | 0.7078125\n",
      "> 37 | 0.6132473349571228 | 0.7078125\n",
      "> 38 | 0.6131753921508789 | 0.7078125\n",
      "> 39 | 0.6131054162979126 | 0.7078125\n",
      "> 40 | 0.6130372285842896 | 0.7078125\n",
      "> 41 | 0.6129708290100098 | 0.7078125\n",
      "> 42 | 0.6129060387611389 | 0.7078125\n",
      "> 43 | 0.6128427982330322 | 0.7078125\n",
      "> 44 | 0.6127810478210449 | 0.7078125\n",
      "> 45 | 0.6127207279205322 | 0.7078125\n",
      "> 46 | 0.6126618385314941 | 0.7078125\n",
      "> 47 | 0.6126041412353516 | 0.7078125\n",
      "> 48 | 0.612547755241394 | 0.7078125\n",
      "> 49 | 0.6124926209449768 | 0.7078125\n",
      "> 50 | 0.6124385595321655 | 0.7078125\n",
      "> 51 | 0.6123856902122498 | 0.7078125\n",
      "> 52 | 0.6123337745666504 | 0.7078125\n",
      "> 53 | 0.6122829914093018 | 0.7078125\n",
      "> 54 | 0.61223304271698 | 0.7078125\n",
      "> 55 | 0.6121841669082642 | 0.7078125\n",
      "> 56 | 0.6121361255645752 | 0.7078125\n",
      "> 57 | 0.6120889782905579 | 0.7078125\n",
      "> 58 | 0.6120426654815674 | 0.7078125\n",
      "> 59 | 0.6119972467422485 | 0.7078125\n",
      "> 60 | 0.611952543258667 | 0.7078125\n",
      "> 61 | 0.6119085550308228 | 0.7078125\n",
      "> 62 | 0.6118654012680054 | 0.7078125\n",
      "> 63 | 0.6118229627609253 | 0.7078125\n",
      "> 64 | 0.611781120300293 | 0.7078125\n",
      "> 65 | 0.611739993095398 | 0.7078125\n",
      "> 66 | 0.6116995811462402 | 0.7078125\n",
      "> 67 | 0.6116597056388855 | 0.7078125\n",
      "> 68 | 0.6116204857826233 | 0.7078125\n",
      "> 69 | 0.6115818023681641 | 0.7078125\n",
      "> 70 | 0.6115437746047974 | 0.7078125\n",
      "> 71 | 0.6115062236785889 | 0.7078125\n",
      "> 72 | 0.6114693284034729 | 0.7078125\n",
      "> 73 | 0.6114328503608704 | 0.7078125\n",
      "> 74 | 0.6113969683647156 | 0.7078125\n",
      "> 75 | 0.6113615036010742 | 0.7078125\n",
      "> 76 | 0.6113265752792358 | 0.7078125\n",
      "> 77 | 0.6112921237945557 | 0.7078125\n",
      "> 78 | 0.6112580895423889 | 0.7078125\n",
      "> 79 | 0.6112245321273804 | 0.7078125\n",
      "> 80 | 0.61119145154953 | 0.7078125\n",
      "> 81 | 0.6111587882041931 | 0.7078125\n",
      "> 82 | 0.6111264228820801 | 0.7078125\n",
      "> 83 | 0.61109459400177 | 0.7078125\n",
      "> 84 | 0.6110631227493286 | 0.7078125\n",
      "> 85 | 0.6110320091247559 | 0.7078125\n",
      "> 86 | 0.6110012531280518 | 0.7078125\n",
      "> 87 | 0.6109709143638611 | 0.7078125\n",
      "> 88 | 0.6109408736228943 | 0.7078125\n",
      "> 89 | 0.6109112501144409 | 0.7078125\n",
      "> 90 | 0.6108819246292114 | 0.7078125\n",
      "> 91 | 0.6108528971672058 | 0.7078125\n",
      "> 92 | 0.6108243465423584 | 0.7078125\n",
      "> 93 | 0.6107959747314453 | 0.7078125\n",
      "> 94 | 0.6107679605484009 | 0.7078125\n",
      "> 95 | 0.6107403039932251 | 0.7078125\n",
      "> 96 | 0.6107128262519836 | 0.7078125\n",
      "> 97 | 0.6106857657432556 | 0.7078125\n",
      "> 98 | 0.6106588840484619 | 0.7078125\n",
      "> 99 | 0.6106324195861816 | 0.7078125\n",
      "> 100 | 0.6106061339378357 | 0.7078125\n",
      "> Evaluation\n",
      "> Class Acc = 0.7109375\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 1.0 | 1.0 | 1.0\n",
      "> Confusion Matrix \n",
      "TN: 0.0 | FP: 74.0 \n",
      "FN: 0.0 | TP: 182.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 0.0 | FP: 22.0 \n",
      "FN: 0.0 | TP: 52.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 0.0 | FP: 52.0 \n",
      "FN: 0.0 | TP: 130.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5165376663208008 | 0.6984375\n",
      "> 2 | 0.5163644552230835 | 0.6984375\n",
      "> 3 | 0.5162909626960754 | 0.6984375\n",
      "> 4 | 0.5162467956542969 | 0.6984375\n",
      "> 5 | 0.5162157416343689 | 0.6984375\n",
      "> 6 | 0.5161921381950378 | 0.6984375\n",
      "> 7 | 0.5161731243133545 | 0.6984375\n",
      "> 8 | 0.5161572098731995 | 0.6984375\n",
      "> 9 | 0.5161435604095459 | 0.6984375\n",
      "> 10 | 0.516131579875946 | 0.6984375\n",
      "> 11 | 0.5161209106445312 | 0.6984375\n",
      "> 12 | 0.5161113739013672 | 0.6984375\n",
      "> 13 | 0.5161026120185852 | 0.6984375\n",
      "> 14 | 0.5160945653915405 | 0.6984375\n",
      "> 15 | 0.5160871744155884 | 0.6984375\n",
      "> 16 | 0.5160802602767944 | 0.6984375\n",
      "> 17 | 0.5160738229751587 | 0.6984375\n",
      "> 18 | 0.5160677433013916 | 0.6984375\n",
      "> 19 | 0.5160620212554932 | 0.6984375\n",
      "> 20 | 0.5160565376281738 | 0.6984375\n",
      "> 21 | 0.5160514116287231 | 0.6984375\n",
      "> 22 | 0.5160465240478516 | 0.6984375\n",
      "> 23 | 0.5160418152809143 | 0.6984375\n",
      "> 24 | 0.5160373449325562 | 0.6984375\n",
      "> 25 | 0.5160329937934875 | 0.6984375\n",
      "> 26 | 0.5160288214683533 | 0.6984375\n",
      "> 27 | 0.5160248279571533 | 0.6984375\n",
      "> 28 | 0.5160209536552429 | 0.6984375\n",
      "> 29 | 0.5160171985626221 | 0.6984375\n",
      "> 30 | 0.5160136222839355 | 0.6984375\n",
      "> 31 | 0.5160101652145386 | 0.6984375\n",
      "> 32 | 0.5160067081451416 | 0.6984375\n",
      "> 33 | 0.516003429889679 | 0.6984375\n",
      "> 34 | 0.5160002708435059 | 0.6984375\n",
      "> 35 | 0.5159971714019775 | 0.6984375\n",
      "> 36 | 0.5159941911697388 | 0.6984375\n",
      "> 37 | 0.5159912109375 | 0.6984375\n",
      "> 38 | 0.515988290309906 | 0.6984375\n",
      "> 39 | 0.5159854888916016 | 0.6984375\n",
      "> 40 | 0.5159827470779419 | 0.6984375\n",
      "> 41 | 0.5159801244735718 | 0.6984375\n",
      "> 42 | 0.5159775018692017 | 0.6984375\n",
      "> 43 | 0.5159748792648315 | 0.6984375\n",
      "> 44 | 0.5159724354743958 | 0.6984375\n",
      "> 45 | 0.51596999168396 | 0.6984375\n",
      "> 46 | 0.5159675478935242 | 0.6984375\n",
      "> 47 | 0.5159651637077332 | 0.6984375\n",
      "> 48 | 0.5159628391265869 | 0.6984375\n",
      "> 49 | 0.5159605741500854 | 0.6984375\n",
      "> 50 | 0.5159583687782288 | 0.6984375\n",
      "> 51 | 0.5159561634063721 | 0.6984375\n",
      "> 52 | 0.5159540176391602 | 0.6984375\n",
      "> 53 | 0.5159518718719482 | 0.6984375\n",
      "> 54 | 0.5159498453140259 | 0.6984375\n",
      "> 55 | 0.5159478187561035 | 0.6984375\n",
      "> 56 | 0.5159457921981812 | 0.6984375\n",
      "> 57 | 0.5159438252449036 | 0.6984375\n",
      "> 58 | 0.515941858291626 | 0.6984375\n",
      "> 59 | 0.5159399509429932 | 0.6984375\n",
      "> 60 | 0.5159380435943604 | 0.6984375\n",
      "> 61 | 0.5159362554550171 | 0.6984375\n",
      "> 62 | 0.5159343481063843 | 0.6984375\n",
      "> 63 | 0.515932559967041 | 0.6984375\n",
      "> 64 | 0.5159307718276978 | 0.6984375\n",
      "> 65 | 0.5159289836883545 | 0.6984375\n",
      "> 66 | 0.5159273147583008 | 0.6984375\n",
      "> 67 | 0.5159255862236023 | 0.6984375\n",
      "> 68 | 0.5159238576889038 | 0.6984375\n",
      "> 69 | 0.5159221887588501 | 0.6984375\n",
      "> 70 | 0.5159205794334412 | 0.6984375\n",
      "> 71 | 0.5159189701080322 | 0.6984375\n",
      "> 72 | 0.5159173607826233 | 0.6984375\n",
      "> 73 | 0.5159157514572144 | 0.6984375\n",
      "> 74 | 0.5159142017364502 | 0.6984375\n",
      "> 75 | 0.5159125924110413 | 0.6984375\n",
      "> 76 | 0.5159111022949219 | 0.6984375\n",
      "> 77 | 0.5159095525741577 | 0.6984375\n",
      "> 78 | 0.5159081220626831 | 0.6984375\n",
      "> 79 | 0.5159066319465637 | 0.6984375\n",
      "> 80 | 0.5159051418304443 | 0.6984375\n",
      "> 81 | 0.5159037113189697 | 0.6984375\n",
      "> 82 | 0.5159022808074951 | 0.6984375\n",
      "> 83 | 0.5159008502960205 | 0.6984375\n",
      "> 84 | 0.5158994197845459 | 0.6984375\n",
      "> 85 | 0.5158979892730713 | 0.6984375\n",
      "> 86 | 0.5158966779708862 | 0.6984375\n",
      "> 87 | 0.5158953070640564 | 0.6984375\n",
      "> 88 | 0.5158939361572266 | 0.6984375\n",
      "> 89 | 0.5158926248550415 | 0.6984375\n",
      "> 90 | 0.5158913135528564 | 0.6984375\n",
      "> 91 | 0.5158900022506714 | 0.6984375\n",
      "> 92 | 0.5158886909484863 | 0.6984375\n",
      "> 93 | 0.5158873796463013 | 0.6984375\n",
      "> 94 | 0.515886127948761 | 0.6984375\n",
      "> 95 | 0.5158848762512207 | 0.6984375\n",
      "> 96 | 0.5158836245536804 | 0.6984375\n",
      "> 97 | 0.5158823728561401 | 0.6984375\n",
      "> 98 | 0.5158811807632446 | 0.6984375\n",
      "> 99 | 0.5158799886703491 | 0.6984375\n",
      "> 100 | 0.5158787369728088 | 0.6984375\n",
      "> Evaluation\n",
      "> Class Acc = 0.6953125\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 1.0 | 1.0 | 1.0\n",
      "> Confusion Matrix \n",
      "TN: 0.0 | FP: 78.0 \n",
      "FN: 0.0 | TP: 178.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 0.0 | FP: 26.0 \n",
      "FN: 0.0 | TP: 57.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 0.0 | FP: 52.0 \n",
      "FN: 0.0 | TP: 121.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.652016282081604 | 0.690625\n",
      "> 2 | 0.648982584476471 | 0.690625\n",
      "> 3 | 0.6471192836761475 | 0.690625\n",
      "> 4 | 0.6457717418670654 | 0.690625\n",
      "> 5 | 0.644717812538147 | 0.690625\n",
      "> 6 | 0.6438536643981934 | 0.690625\n",
      "> 7 | 0.6431221961975098 | 0.690625\n",
      "> 8 | 0.6424887180328369 | 0.690625\n",
      "> 9 | 0.6419298648834229 | 0.690625\n",
      "> 10 | 0.6414231657981873 | 0.690625\n",
      "> 11 | 0.6409653425216675 | 0.690625\n",
      "> 12 | 0.6405481100082397 | 0.690625\n",
      "> 13 | 0.6401649713516235 | 0.690625\n",
      "> 14 | 0.6398108601570129 | 0.690625\n",
      "> 15 | 0.639481782913208 | 0.690625\n",
      "> 16 | 0.6391745209693909 | 0.690625\n",
      "> 17 | 0.6388864517211914 | 0.690625\n",
      "> 18 | 0.6386152505874634 | 0.690625\n",
      "> 19 | 0.6383591890335083 | 0.690625\n",
      "> 20 | 0.6381166577339172 | 0.690625\n",
      "> 21 | 0.6378864049911499 | 0.690625\n",
      "> 22 | 0.6376672387123108 | 0.690625\n",
      "> 23 | 0.637458086013794 | 0.690625\n",
      "> 24 | 0.6372581720352173 | 0.690625\n",
      "> 25 | 0.6370667219161987 | 0.690625\n",
      "> 26 | 0.6368831396102905 | 0.690625\n",
      "> 27 | 0.6367065906524658 | 0.690625\n",
      "> 28 | 0.6365367770195007 | 0.690625\n",
      "> 29 | 0.6363731622695923 | 0.690625\n",
      "> 30 | 0.636215329170227 | 0.690625\n",
      "> 31 | 0.6360628008842468 | 0.690625\n",
      "> 32 | 0.6359153985977173 | 0.690625\n",
      "> 33 | 0.635772705078125 | 0.690625\n",
      "> 34 | 0.6356343626976013 | 0.690625\n",
      "> 35 | 0.6355001926422119 | 0.690625\n",
      "> 36 | 0.6353700160980225 | 0.690625\n",
      "> 37 | 0.6352435350418091 | 0.690625\n",
      "> 38 | 0.6351206302642822 | 0.690625\n",
      "> 39 | 0.6350009441375732 | 0.690625\n",
      "> 40 | 0.6348844766616821 | 0.690625\n",
      "> 41 | 0.6347709894180298 | 0.690625\n",
      "> 42 | 0.6346604228019714 | 0.690625\n",
      "> 43 | 0.6345524787902832 | 0.690625\n",
      "> 44 | 0.6344472169876099 | 0.690625\n",
      "> 45 | 0.6343443393707275 | 0.690625\n",
      "> 46 | 0.6342438459396362 | 0.690625\n",
      "> 47 | 0.6341456174850464 | 0.690625\n",
      "> 48 | 0.6340495944023132 | 0.690625\n",
      "> 49 | 0.6339555978775024 | 0.690625\n",
      "> 50 | 0.6338636875152588 | 0.690625\n",
      "> 51 | 0.6337735652923584 | 0.690625\n",
      "> 52 | 0.6336856484413147 | 0.690625\n",
      "> 53 | 0.6336003541946411 | 0.690625\n",
      "> 54 | 0.6335167288780212 | 0.690625\n",
      "> 55 | 0.6334347724914551 | 0.690625\n",
      "> 56 | 0.6333543062210083 | 0.690625\n",
      "> 57 | 0.6332753896713257 | 0.690625\n",
      "> 58 | 0.6331979036331177 | 0.690625\n",
      "> 59 | 0.6331217288970947 | 0.690625\n",
      "> 60 | 0.6330469846725464 | 0.690625\n",
      "> 61 | 0.6329734325408936 | 0.690625\n",
      "> 62 | 0.6329011917114258 | 0.690625\n",
      "> 63 | 0.6328302025794983 | 0.690625\n",
      "> 64 | 0.6327602863311768 | 0.690625\n",
      "> 65 | 0.6326915621757507 | 0.690625\n",
      "> 66 | 0.6326239109039307 | 0.690625\n",
      "> 67 | 0.6325573921203613 | 0.690625\n",
      "> 68 | 0.6324917674064636 | 0.690625\n",
      "> 69 | 0.6324272155761719 | 0.690625\n",
      "> 70 | 0.6323636770248413 | 0.690625\n",
      "> 71 | 0.6323010325431824 | 0.690625\n",
      "> 72 | 0.6322393417358398 | 0.690625\n",
      "> 73 | 0.632178544998169 | 0.690625\n",
      "> 74 | 0.6321185827255249 | 0.690625\n",
      "> 75 | 0.6320594549179077 | 0.690625\n",
      "> 76 | 0.6320012211799622 | 0.690625\n",
      "> 77 | 0.6319437026977539 | 0.690625\n",
      "> 78 | 0.6318869590759277 | 0.690625\n",
      "> 79 | 0.6318310499191284 | 0.690625\n",
      "> 80 | 0.6317758560180664 | 0.690625\n",
      "> 81 | 0.6317213773727417 | 0.690625\n",
      "> 82 | 0.6316675543785095 | 0.690625\n",
      "> 83 | 0.6316144466400146 | 0.690625\n",
      "> 84 | 0.6315620541572571 | 0.690625\n",
      "> 85 | 0.6315102577209473 | 0.690625\n",
      "> 86 | 0.63145911693573 | 0.690625\n",
      "> 87 | 0.6314086318016052 | 0.690625\n",
      "> 88 | 0.6313587427139282 | 0.690625\n",
      "> 89 | 0.6313093900680542 | 0.690625\n",
      "> 90 | 0.6312607526779175 | 0.690625\n",
      "> 91 | 0.631212592124939 | 0.690625\n",
      "> 92 | 0.6311650276184082 | 0.690625\n",
      "> 93 | 0.6311179995536804 | 0.690625\n",
      "> 94 | 0.6310715079307556 | 0.690625\n",
      "> 95 | 0.6310255527496338 | 0.690625\n",
      "> 96 | 0.6309801340103149 | 0.690625\n",
      "> 97 | 0.6309351921081543 | 0.690625\n",
      "> 98 | 0.6308907270431519 | 0.690625\n",
      "> 99 | 0.6308467388153076 | 0.690625\n",
      "> 100 | 0.6308032870292664 | 0.690625\n",
      "> Evaluation\n",
      "> Class Acc = 0.7109375\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 1.0 | 1.0 | 1.0\n",
      "> Confusion Matrix \n",
      "TN: 0.0 | FP: 74.0 \n",
      "FN: 0.0 | TP: 182.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 0.0 | FP: 30.0 \n",
      "FN: 0.0 | TP: 49.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 0.0 | FP: 44.0 \n",
      "FN: 0.0 | TP: 133.0\n"
     ]
    }
   ],
   "source": [
    "for cv_seed in cv_seeds:\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        x, y, a, test_size=0.3, random_state=cv_seed)\n",
    "\n",
    "    train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "    train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # train below\n",
    "\n",
    "    model = UnfairLogisticRegression(xdim, ydim, batch_size)\n",
    "    ret = train_loop(model, train_data, epochs)\n",
    "    Y, A, Y_hat = evaluation(model, test_data)\n",
    "    \n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1  = compute_metrics(Y, A, Y_hat, adim=adim)\n",
    "    \n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    \n",
    "    tradeoff = []\n",
    "    \n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "    \n",
    "    result = ['UnfairLR-decay', cv_seed, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc78878fd90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5916500687599182 | 0.6734375\n",
      "> 2 | 0.5883281826972961 | 0.6734375\n",
      "> 3 | 0.5851278305053711 | 0.6734375\n",
      "> 4 | 0.5820651054382324 | 0.6734375\n",
      "> 5 | 0.5791320204734802 | 0.6734375\n",
      "> 6 | 0.5763149857521057 | 0.6734375\n",
      "> 7 | 0.5735998153686523 | 0.6734375\n",
      "> 8 | 0.5709738731384277 | 0.6734375\n",
      "> 9 | 0.5684264302253723 | 0.6734375\n",
      "> 10 | 0.5659487247467041 | 0.6734375\n",
      "> 11 | 0.5635337829589844 | 0.6734375\n",
      "> 12 | 0.5611760020256042 | 0.6734375\n",
      "> 13 | 0.5588711500167847 | 0.6734375\n",
      "> 14 | 0.5566158294677734 | 0.6734375\n",
      "> 15 | 0.5544074773788452 | 0.6734375\n",
      "> 16 | 0.5522441864013672 | 0.6734375\n",
      "> 17 | 0.55012446641922 | 0.6734375\n",
      "> 18 | 0.5480470061302185 | 0.6734375\n",
      "> 19 | 0.5460109710693359 | 0.6734375\n",
      "> 20 | 0.5440154075622559 | 0.6734375\n",
      "> 21 | 0.5420598983764648 | 0.6734375\n",
      "> 22 | 0.540143609046936 | 0.675\n",
      "> 23 | 0.5382660627365112 | 0.675\n",
      "> 24 | 0.5364266633987427 | 0.675\n",
      "> 25 | 0.5346249341964722 | 0.6765625\n",
      "> 26 | 0.5328601598739624 | 0.678125\n",
      "> 27 | 0.5311318039894104 | 0.6828125\n",
      "> 28 | 0.5294392108917236 | 0.684375\n",
      "> 29 | 0.5277817845344543 | 0.6859375\n",
      "> 30 | 0.5261588096618652 | 0.69375\n",
      "> 31 | 0.5245696306228638 | 0.6953125\n",
      "> 32 | 0.5230134725570679 | 0.6984375\n",
      "> 33 | 0.5214897394180298 | 0.70625\n",
      "> 34 | 0.5199977159500122 | 0.7046875\n",
      "> 35 | 0.5185365080833435 | 0.7078125\n",
      "> 36 | 0.5171055197715759 | 0.709375\n",
      "> 37 | 0.5157039165496826 | 0.709375\n",
      "> 38 | 0.5143311619758606 | 0.709375\n",
      "> 39 | 0.5129863023757935 | 0.7109375\n",
      "> 40 | 0.5116687417030334 | 0.7140625\n",
      "> 41 | 0.5103777050971985 | 0.715625\n",
      "> 42 | 0.5091125965118408 | 0.71875\n",
      "> 43 | 0.5078725814819336 | 0.71875\n",
      "> 44 | 0.5066571235656738 | 0.7203125\n",
      "> 45 | 0.5054655075073242 | 0.7265625\n",
      "> 46 | 0.504297137260437 | 0.728125\n",
      "> 47 | 0.5031512975692749 | 0.7265625\n",
      "> 48 | 0.5020273923873901 | 0.7296875\n",
      "> 49 | 0.500924825668335 | 0.7296875\n",
      "> 50 | 0.4998430609703064 | 0.73125\n",
      "> 51 | 0.4987814724445343 | 0.7328125\n",
      "> 52 | 0.49773961305618286 | 0.734375\n",
      "> 53 | 0.49671679735183716 | 0.7375\n",
      "> 54 | 0.4957125782966614 | 0.7390625\n",
      "> 55 | 0.4947265386581421 | 0.7421875\n",
      "> 56 | 0.49375805258750916 | 0.74375\n",
      "> 57 | 0.4928067624568939 | 0.746875\n",
      "> 58 | 0.49187207221984863 | 0.746875\n",
      "> 59 | 0.490953654050827 | 0.746875\n",
      "> 60 | 0.4900510609149933 | 0.7484375\n",
      "> 61 | 0.4891638159751892 | 0.7546875\n",
      "> 62 | 0.48829156160354614 | 0.7546875\n",
      "> 63 | 0.4874339699745178 | 0.75625\n",
      "> 64 | 0.48659059405326843 | 0.7578125\n",
      "> 65 | 0.48576104640960693 | 0.75625\n",
      "> 66 | 0.48494499921798706 | 0.7546875\n",
      "> 67 | 0.48414212465286255 | 0.7546875\n",
      "> 68 | 0.48335206508636475 | 0.7546875\n",
      "> 69 | 0.482574462890625 | 0.7578125\n",
      "> 70 | 0.48180916905403137 | 0.759375\n",
      "> 71 | 0.48105573654174805 | 0.759375\n",
      "> 72 | 0.48031389713287354 | 0.759375\n",
      "> 73 | 0.47958335280418396 | 0.7578125\n",
      "> 74 | 0.478863924741745 | 0.7578125\n",
      "> 75 | 0.478155255317688 | 0.7609375\n",
      "> 76 | 0.4774571657180786 | 0.759375\n",
      "> 77 | 0.4767693281173706 | 0.7578125\n",
      "> 78 | 0.47609150409698486 | 0.7609375\n",
      "> 79 | 0.47542351484298706 | 0.7625\n",
      "> 80 | 0.4747651219367981 | 0.7625\n",
      "> 81 | 0.47411608695983887 | 0.7625\n",
      "> 82 | 0.47347623109817505 | 0.7625\n",
      "> 83 | 0.47284531593322754 | 0.7609375\n",
      "> 84 | 0.472223162651062 | 0.7609375\n",
      "> 85 | 0.471609503030777 | 0.7609375\n",
      "> 86 | 0.47100433707237244 | 0.7578125\n",
      "> 87 | 0.47040724754333496 | 0.7578125\n",
      "> 88 | 0.46981823444366455 | 0.7578125\n",
      "> 89 | 0.46923699975013733 | 0.7578125\n",
      "> 90 | 0.4686635732650757 | 0.759375\n",
      "> 91 | 0.4680975377559662 | 0.7609375\n",
      "> 92 | 0.46753889322280884 | 0.7578125\n",
      "> 93 | 0.4669874310493469 | 0.7578125\n",
      "> 94 | 0.4664430618286133 | 0.759375\n",
      "> 95 | 0.4659056067466736 | 0.759375\n",
      "> 96 | 0.4653749167919159 | 0.7578125\n",
      "> 97 | 0.46485087275505066 | 0.7578125\n",
      "> 98 | 0.46433332562446594 | 0.7578125\n",
      "> 99 | 0.4638221263885498 | 0.7578125\n",
      "> 100 | 0.4633171558380127 | 0.7578125\n",
      "> Evaluation\n",
      "> Class Acc = 0.7109375\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8294574022293091 | 0.7690792083740234 | 0.9392572939395905\n",
      "> Confusion Matrix \n",
      "TN: 25.0 | FP: 43.0 \n",
      "FN: 31.0 | TP: 157.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 16.0 | FP: 10.0 \n",
      "FN: 12.0 | TP: 46.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 9.0 | FP: 33.0 \n",
      "FN: 19.0 | TP: 111.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.6538881063461304 | 0.7078125\n",
      "> 2 | 0.6497178673744202 | 0.7078125\n",
      "> 3 | 0.6456848382949829 | 0.7078125\n",
      "> 4 | 0.6418513059616089 | 0.7078125\n",
      "> 5 | 0.6382236480712891 | 0.7078125\n",
      "> 6 | 0.6347923874855042 | 0.7078125\n",
      "> 7 | 0.6315432786941528 | 0.7078125\n",
      "> 8 | 0.6284615993499756 | 0.7078125\n",
      "> 9 | 0.6255336403846741 | 0.7078125\n",
      "> 10 | 0.6227471232414246 | 0.7078125\n",
      "> 11 | 0.6200913190841675 | 0.7078125\n",
      "> 12 | 0.6175565719604492 | 0.7078125\n",
      "> 13 | 0.6151347160339355 | 0.7078125\n",
      "> 14 | 0.6128180027008057 | 0.7078125\n",
      "> 15 | 0.6105998754501343 | 0.7078125\n",
      "> 16 | 0.6084743738174438 | 0.7078125\n",
      "> 17 | 0.6064358949661255 | 0.7078125\n",
      "> 18 | 0.6044795513153076 | 0.7078125\n",
      "> 19 | 0.6026009321212769 | 0.7078125\n",
      "> 20 | 0.6007956862449646 | 0.7078125\n",
      "> 21 | 0.5990601778030396 | 0.7078125\n",
      "> 22 | 0.5973906517028809 | 0.7078125\n",
      "> 23 | 0.5957839488983154 | 0.7078125\n",
      "> 24 | 0.5942369699478149 | 0.7078125\n",
      "> 25 | 0.5927467942237854 | 0.7078125\n",
      "> 26 | 0.5913108587265015 | 0.7078125\n",
      "> 27 | 0.5899264812469482 | 0.7078125\n",
      "> 28 | 0.5885913372039795 | 0.7078125\n",
      "> 29 | 0.5873032808303833 | 0.7078125\n",
      "> 30 | 0.5860600471496582 | 0.7078125\n",
      "> 31 | 0.5848597288131714 | 0.709375\n",
      "> 32 | 0.58370041847229 | 0.709375\n",
      "> 33 | 0.5825803279876709 | 0.709375\n",
      "> 34 | 0.5814976692199707 | 0.709375\n",
      "> 35 | 0.5804508924484253 | 0.709375\n",
      "> 36 | 0.5794384479522705 | 0.709375\n",
      "> 37 | 0.5784590244293213 | 0.7125\n",
      "> 38 | 0.5775109529495239 | 0.7125\n",
      "> 39 | 0.5765931010246277 | 0.7140625\n",
      "> 40 | 0.5757041573524475 | 0.715625\n",
      "> 41 | 0.5748429298400879 | 0.715625\n",
      "> 42 | 0.5740081667900085 | 0.71875\n",
      "> 43 | 0.5731989741325378 | 0.7171875\n",
      "> 44 | 0.5724141597747803 | 0.7171875\n",
      "> 45 | 0.5716527700424194 | 0.7234375\n",
      "> 46 | 0.5709139108657837 | 0.7234375\n",
      "> 47 | 0.5701965689659119 | 0.7265625\n",
      "> 48 | 0.5694999694824219 | 0.7265625\n",
      "> 49 | 0.5688232183456421 | 0.7265625\n",
      "> 50 | 0.5681656002998352 | 0.728125\n",
      "> 51 | 0.5675262808799744 | 0.7328125\n",
      "> 52 | 0.5669046640396118 | 0.7328125\n",
      "> 53 | 0.5662999153137207 | 0.7328125\n",
      "> 54 | 0.5657113790512085 | 0.734375\n",
      "> 55 | 0.5651385188102722 | 0.7328125\n",
      "> 56 | 0.5645806789398193 | 0.7328125\n",
      "> 57 | 0.5640373229980469 | 0.7296875\n",
      "> 58 | 0.5635079145431519 | 0.7296875\n",
      "> 59 | 0.5629918575286865 | 0.7296875\n",
      "> 60 | 0.5624886751174927 | 0.73125\n",
      "> 61 | 0.5619978308677673 | 0.73125\n",
      "> 62 | 0.5615189075469971 | 0.734375\n",
      "> 63 | 0.561051607131958 | 0.734375\n",
      "> 64 | 0.5605952143669128 | 0.7359375\n",
      "> 65 | 0.5601494312286377 | 0.7359375\n",
      "> 66 | 0.559714138507843 | 0.7359375\n",
      "> 67 | 0.5592885613441467 | 0.7390625\n",
      "> 68 | 0.5588724613189697 | 0.7390625\n",
      "> 69 | 0.5584657192230225 | 0.7390625\n",
      "> 70 | 0.5580676794052124 | 0.7359375\n",
      "> 71 | 0.5576782822608948 | 0.7375\n",
      "> 72 | 0.5572971105575562 | 0.740625\n",
      "> 73 | 0.5569238662719727 | 0.740625\n",
      "> 74 | 0.5565583109855652 | 0.7421875\n",
      "> 75 | 0.5562002062797546 | 0.7421875\n",
      "> 76 | 0.5558491945266724 | 0.7421875\n",
      "> 77 | 0.5555050373077393 | 0.74375\n",
      "> 78 | 0.5551676154136658 | 0.74375\n",
      "> 79 | 0.5548365712165833 | 0.7453125\n",
      "> 80 | 0.5545118451118469 | 0.75\n",
      "> 81 | 0.5541931390762329 | 0.746875\n",
      "> 82 | 0.5538800954818726 | 0.7453125\n",
      "> 83 | 0.5535727739334106 | 0.74375\n",
      "> 84 | 0.5532709360122681 | 0.7453125\n",
      "> 85 | 0.5529743432998657 | 0.7453125\n",
      "> 86 | 0.5526826977729797 | 0.746875\n",
      "> 87 | 0.5523961186408997 | 0.75\n",
      "> 88 | 0.5521142482757568 | 0.75\n",
      "> 89 | 0.5518369674682617 | 0.7484375\n",
      "> 90 | 0.5515642166137695 | 0.7484375\n",
      "> 91 | 0.5512957572937012 | 0.75\n",
      "> 92 | 0.5510314702987671 | 0.7515625\n",
      "> 93 | 0.5507712364196777 | 0.7515625\n",
      "> 94 | 0.5505149960517883 | 0.753125\n",
      "> 95 | 0.5502625107765198 | 0.753125\n",
      "> 96 | 0.5500137805938721 | 0.753125\n",
      "> 97 | 0.5497685670852661 | 0.7546875\n",
      "> 98 | 0.5495268106460571 | 0.75625\n",
      "> 99 | 0.5492885112762451 | 0.75625\n",
      "> 100 | 0.549053430557251 | 0.753125\n",
      "> Evaluation\n",
      "> Class Acc = 0.69921875\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9287706613540649 | 0.9621685743331909 | 0.9472129940986633\n",
      "> Confusion Matrix \n",
      "TN: 23.0 | FP: 64.0 \n",
      "FN: 13.0 | TP: 156.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 10.0 | FP: 26.0 \n",
      "FN: 5.0 | TP: 38.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 13.0 | FP: 38.0 \n",
      "FN: 8.0 | TP: 118.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.6229864954948425 | 0.7078125\n",
      "> 2 | 0.6193436980247498 | 0.7078125\n",
      "> 3 | 0.6157432198524475 | 0.7078125\n",
      "> 4 | 0.6122448444366455 | 0.7078125\n",
      "> 5 | 0.6088608503341675 | 0.7078125\n",
      "> 6 | 0.6055907607078552 | 0.7078125\n",
      "> 7 | 0.6024301052093506 | 0.7078125\n",
      "> 8 | 0.5993729829788208 | 0.7078125\n",
      "> 9 | 0.5964138507843018 | 0.7078125\n",
      "> 10 | 0.5935472846031189 | 0.7078125\n",
      "> 11 | 0.5907682180404663 | 0.7078125\n",
      "> 12 | 0.5880721807479858 | 0.7078125\n",
      "> 13 | 0.585455060005188 | 0.7078125\n",
      "> 14 | 0.5829130411148071 | 0.7078125\n",
      "> 15 | 0.5804426074028015 | 0.7078125\n",
      "> 16 | 0.5780407190322876 | 0.7078125\n",
      "> 17 | 0.5757042169570923 | 0.7078125\n",
      "> 18 | 0.5734304189682007 | 0.7078125\n",
      "> 19 | 0.5712167024612427 | 0.7078125\n",
      "> 20 | 0.5690608024597168 | 0.7078125\n",
      "> 21 | 0.5669602155685425 | 0.7078125\n",
      "> 22 | 0.5649130344390869 | 0.7078125\n",
      "> 23 | 0.5629171133041382 | 0.7078125\n",
      "> 24 | 0.560970664024353 | 0.7078125\n",
      "> 25 | 0.5590716600418091 | 0.7078125\n",
      "> 26 | 0.5572186708450317 | 0.7078125\n",
      "> 27 | 0.5554099678993225 | 0.7078125\n",
      "> 28 | 0.5536439418792725 | 0.7078125\n",
      "> 29 | 0.5519192218780518 | 0.7078125\n",
      "> 30 | 0.550234317779541 | 0.7109375\n",
      "> 31 | 0.5485879182815552 | 0.7109375\n",
      "> 32 | 0.5469788312911987 | 0.709375\n",
      "> 33 | 0.5454057455062866 | 0.709375\n",
      "> 34 | 0.5438674092292786 | 0.7140625\n",
      "> 35 | 0.5423628687858582 | 0.7140625\n",
      "> 36 | 0.5408909320831299 | 0.7140625\n",
      "> 37 | 0.5394506454467773 | 0.715625\n",
      "> 38 | 0.5380409955978394 | 0.7171875\n",
      "> 39 | 0.5366610288619995 | 0.7203125\n",
      "> 40 | 0.5353098511695862 | 0.7234375\n",
      "> 41 | 0.533986508846283 | 0.7203125\n",
      "> 42 | 0.5326902866363525 | 0.721875\n",
      "> 43 | 0.5314202904701233 | 0.7265625\n",
      "> 44 | 0.5301756858825684 | 0.73125\n",
      "> 45 | 0.5289559364318848 | 0.73125\n",
      "> 46 | 0.5277600288391113 | 0.7296875\n",
      "> 47 | 0.5265875458717346 | 0.73125\n",
      "> 48 | 0.5254377126693726 | 0.734375\n",
      "> 49 | 0.5243098735809326 | 0.7359375\n",
      "> 50 | 0.5232033133506775 | 0.7375\n",
      "> 51 | 0.5221176147460938 | 0.7359375\n",
      "> 52 | 0.5210521221160889 | 0.7359375\n",
      "> 53 | 0.5200062990188599 | 0.7390625\n",
      "> 54 | 0.518979549407959 | 0.7390625\n",
      "> 55 | 0.517971396446228 | 0.7375\n",
      "> 56 | 0.5169814825057983 | 0.740625\n",
      "> 57 | 0.5160092115402222 | 0.740625\n",
      "> 58 | 0.5150539875030518 | 0.7390625\n",
      "> 59 | 0.5141156315803528 | 0.7421875\n",
      "> 60 | 0.5131935477256775 | 0.740625\n",
      "> 61 | 0.5122872591018677 | 0.74375\n",
      "> 62 | 0.5113965272903442 | 0.7453125\n",
      "> 63 | 0.5105209350585938 | 0.7484375\n",
      "> 64 | 0.509660005569458 | 0.7484375\n",
      "> 65 | 0.5088134407997131 | 0.7453125\n",
      "> 66 | 0.5079809427261353 | 0.74375\n",
      "> 67 | 0.5071620941162109 | 0.7453125\n",
      "> 68 | 0.5063565969467163 | 0.746875\n",
      "> 69 | 0.5055640935897827 | 0.746875\n",
      "> 70 | 0.5047844052314758 | 0.7453125\n",
      "> 71 | 0.5040171146392822 | 0.7453125\n",
      "> 72 | 0.5032619833946228 | 0.7453125\n",
      "> 73 | 0.5025187134742737 | 0.7453125\n",
      "> 74 | 0.5017871260643005 | 0.7453125\n",
      "> 75 | 0.5010667443275452 | 0.7453125\n",
      "> 76 | 0.5003575682640076 | 0.7453125\n",
      "> 77 | 0.4996591806411743 | 0.746875\n",
      "> 78 | 0.49897146224975586 | 0.7484375\n",
      "> 79 | 0.4982941150665283 | 0.7484375\n",
      "> 80 | 0.4976269006729126 | 0.7484375\n",
      "> 81 | 0.49696967005729675 | 0.75\n",
      "> 82 | 0.4963221848011017 | 0.753125\n",
      "> 83 | 0.4956842362880707 | 0.7546875\n",
      "> 84 | 0.49505558609962463 | 0.7546875\n",
      "> 85 | 0.4944361448287964 | 0.75625\n",
      "> 86 | 0.49382561445236206 | 0.75625\n",
      "> 87 | 0.4932239353656769 | 0.75625\n",
      "> 88 | 0.49263083934783936 | 0.7578125\n",
      "> 89 | 0.49204614758491516 | 0.7578125\n",
      "> 90 | 0.4914698004722595 | 0.75625\n",
      "> 91 | 0.49090149998664856 | 0.7546875\n",
      "> 92 | 0.4903412163257599 | 0.75625\n",
      "> 93 | 0.4897887408733368 | 0.75625\n",
      "> 94 | 0.48924386501312256 | 0.75625\n",
      "> 95 | 0.4887065291404724 | 0.7578125\n",
      "> 96 | 0.4881766140460968 | 0.7578125\n",
      "> 97 | 0.487653911113739 | 0.7578125\n",
      "> 98 | 0.4871383309364319 | 0.7609375\n",
      "> 99 | 0.4866297245025635 | 0.7609375\n",
      "> 100 | 0.48612797260284424 | 0.759375\n",
      "> Evaluation\n",
      "> Class Acc = 0.70703125\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8662014007568359 | 0.82045454159379 | 0.9346153810620308\n",
      "> Confusion Matrix \n",
      "TN: 15.0 | FP: 59.0 \n",
      "FN: 16.0 | TP: 166.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 9.0 | FP: 13.0 \n",
      "FN: 7.0 | TP: 45.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 6.0 | FP: 46.0 \n",
      "FN: 9.0 | TP: 121.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5165376663208008 | 0.6984375\n",
      "> 2 | 0.5151135921478271 | 0.6984375\n",
      "> 3 | 0.5135432481765747 | 0.6984375\n",
      "> 4 | 0.5119408965110779 | 0.6984375\n",
      "> 5 | 0.5103350877761841 | 0.6984375\n",
      "> 6 | 0.5087347626686096 | 0.6984375\n",
      "> 7 | 0.5071432590484619 | 0.6984375\n",
      "> 8 | 0.5055626034736633 | 0.6984375\n",
      "> 9 | 0.5039946436882019 | 0.6984375\n",
      "> 10 | 0.5024410486221313 | 0.6984375\n",
      "> 11 | 0.5009034872055054 | 0.6984375\n",
      "> 12 | 0.4993837773799896 | 0.6984375\n",
      "> 13 | 0.49788346886634827 | 0.6984375\n",
      "> 14 | 0.4964039921760559 | 0.6984375\n",
      "> 15 | 0.49494656920433044 | 0.6984375\n",
      "> 16 | 0.49351245164871216 | 0.6984375\n",
      "> 17 | 0.4921024739742279 | 0.6984375\n",
      "> 18 | 0.4907175600528717 | 0.6984375\n",
      "> 19 | 0.4893583059310913 | 0.6984375\n",
      "> 20 | 0.48802515864372253 | 0.6984375\n",
      "> 21 | 0.4867185354232788 | 0.6984375\n",
      "> 22 | 0.48543864488601685 | 0.6984375\n",
      "> 23 | 0.4841856360435486 | 0.6984375\n",
      "> 24 | 0.48295947909355164 | 0.6984375\n",
      "> 25 | 0.4817601442337036 | 0.6984375\n",
      "> 26 | 0.4805874526500702 | 0.7\n",
      "> 27 | 0.47944116592407227 | 0.6984375\n",
      "> 28 | 0.47832101583480835 | 0.703125\n",
      "> 29 | 0.47722673416137695 | 0.703125\n",
      "> 30 | 0.47615787386894226 | 0.703125\n",
      "> 31 | 0.47511404752731323 | 0.7046875\n",
      "> 32 | 0.47409480810165405 | 0.7046875\n",
      "> 33 | 0.4730997383594513 | 0.703125\n",
      "> 34 | 0.4721282720565796 | 0.70625\n",
      "> 35 | 0.4711799621582031 | 0.709375\n",
      "> 36 | 0.4702543616294861 | 0.7140625\n",
      "> 37 | 0.46935081481933594 | 0.7171875\n",
      "> 38 | 0.46846890449523926 | 0.7171875\n",
      "> 39 | 0.46760812401771545 | 0.71875\n",
      "> 40 | 0.46676790714263916 | 0.7265625\n",
      "> 41 | 0.4659477174282074 | 0.7265625\n",
      "> 42 | 0.46514710783958435 | 0.7265625\n",
      "> 43 | 0.46436551213264465 | 0.7265625\n",
      "> 44 | 0.4636024832725525 | 0.728125\n",
      "> 45 | 0.46285751461982727 | 0.7296875\n",
      "> 46 | 0.462130069732666 | 0.7328125\n",
      "> 47 | 0.4614197611808777 | 0.7359375\n",
      "> 48 | 0.4607260823249817 | 0.7375\n",
      "> 49 | 0.46004852652549744 | 0.7375\n",
      "> 50 | 0.45938676595687866 | 0.7390625\n",
      "> 51 | 0.4587402939796448 | 0.740625\n",
      "> 52 | 0.45810866355895996 | 0.740625\n",
      "> 53 | 0.45749157667160034 | 0.74375\n",
      "> 54 | 0.4568885564804077 | 0.7453125\n",
      "> 55 | 0.45629918575286865 | 0.74375\n",
      "> 56 | 0.4557231664657593 | 0.74375\n",
      "> 57 | 0.4551600515842438 | 0.7421875\n",
      "> 58 | 0.45460957288742065 | 0.7421875\n",
      "> 59 | 0.4540713429450989 | 0.740625\n",
      "> 60 | 0.4535450041294098 | 0.740625\n",
      "> 61 | 0.4530302584171295 | 0.7421875\n",
      "> 62 | 0.4525268077850342 | 0.74375\n",
      "> 63 | 0.4520343542098999 | 0.7421875\n",
      "> 64 | 0.4515525698661804 | 0.7453125\n",
      "> 65 | 0.45108115673065186 | 0.74375\n",
      "> 66 | 0.4506199061870575 | 0.74375\n",
      "> 67 | 0.4501684606075287 | 0.740625\n",
      "> 68 | 0.4497266113758087 | 0.74375\n",
      "> 69 | 0.44929414987564087 | 0.7453125\n",
      "> 70 | 0.4488707184791565 | 0.74375\n",
      "> 71 | 0.44845619797706604 | 0.7453125\n",
      "> 72 | 0.448050320148468 | 0.7421875\n",
      "> 73 | 0.44765281677246094 | 0.7421875\n",
      "> 74 | 0.44726359844207764 | 0.7421875\n",
      "> 75 | 0.44688230752944946 | 0.740625\n",
      "> 76 | 0.4465087950229645 | 0.740625\n",
      "> 77 | 0.44614294171333313 | 0.740625\n",
      "> 78 | 0.4457845091819763 | 0.7421875\n",
      "> 79 | 0.4454333186149597 | 0.7421875\n",
      "> 80 | 0.4450891613960266 | 0.7421875\n",
      "> 81 | 0.44475188851356506 | 0.7421875\n",
      "> 82 | 0.4444213807582855 | 0.7453125\n",
      "> 83 | 0.4440974295139313 | 0.7484375\n",
      "> 84 | 0.443779855966568 | 0.746875\n",
      "> 85 | 0.4434685707092285 | 0.7484375\n",
      "> 86 | 0.4431633949279785 | 0.7484375\n",
      "> 87 | 0.44286423921585083 | 0.7484375\n",
      "> 88 | 0.44257086515426636 | 0.746875\n",
      "> 89 | 0.4422832131385803 | 0.746875\n",
      "> 90 | 0.4420011341571808 | 0.746875\n",
      "> 91 | 0.4417245388031006 | 0.746875\n",
      "> 92 | 0.4414532780647278 | 0.746875\n",
      "> 93 | 0.44118720293045044 | 0.746875\n",
      "> 94 | 0.4409262537956238 | 0.746875\n",
      "> 95 | 0.44067031145095825 | 0.7453125\n",
      "> 96 | 0.4404192268848419 | 0.74375\n",
      "> 97 | 0.44017294049263 | 0.74375\n",
      "> 98 | 0.43993133306503296 | 0.74375\n",
      "> 99 | 0.43969428539276123 | 0.7453125\n",
      "> 100 | 0.43946176767349243 | 0.7453125\n",
      "> Evaluation\n",
      "> Class Acc = 0.71875\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9112751483917236 | 0.9002144001424313 | 0.9350442215800285\n",
      "> Confusion Matrix \n",
      "TN: 20.0 | FP: 58.0 \n",
      "FN: 14.0 | TP: 164.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 9.0 | FP: 17.0 \n",
      "FN: 7.0 | TP: 50.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 11.0 | FP: 41.0 \n",
      "FN: 7.0 | TP: 114.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.652016282081604 | 0.690625\n",
      "> 2 | 0.6462016105651855 | 0.690625\n",
      "> 3 | 0.6406328082084656 | 0.690625\n",
      "> 4 | 0.6353652477264404 | 0.690625\n",
      "> 5 | 0.6303987503051758 | 0.690625\n",
      "> 6 | 0.6257139444351196 | 0.690625\n",
      "> 7 | 0.6212855577468872 | 0.690625\n",
      "> 8 | 0.6170880794525146 | 0.690625\n",
      "> 9 | 0.6130980849266052 | 0.690625\n",
      "> 10 | 0.6092951893806458 | 0.690625\n",
      "> 11 | 0.6056615710258484 | 0.690625\n",
      "> 12 | 0.6021819114685059 | 0.690625\n",
      "> 13 | 0.5988431572914124 | 0.690625\n",
      "> 14 | 0.5956340432167053 | 0.690625\n",
      "> 15 | 0.5925447344779968 | 0.690625\n",
      "> 16 | 0.5895666480064392 | 0.690625\n",
      "> 17 | 0.5866923332214355 | 0.690625\n",
      "> 18 | 0.5839152336120605 | 0.690625\n",
      "> 19 | 0.5812293291091919 | 0.690625\n",
      "> 20 | 0.5786294341087341 | 0.690625\n",
      "> 21 | 0.57611083984375 | 0.690625\n",
      "> 22 | 0.5736691951751709 | 0.690625\n",
      "> 23 | 0.5713006854057312 | 0.690625\n",
      "> 24 | 0.5690016746520996 | 0.690625\n",
      "> 25 | 0.566769003868103 | 0.6921875\n",
      "> 26 | 0.5645996332168579 | 0.6921875\n",
      "> 27 | 0.5624907612800598 | 0.69375\n",
      "> 28 | 0.5604398846626282 | 0.69375\n",
      "> 29 | 0.558444619178772 | 0.6953125\n",
      "> 30 | 0.556502640247345 | 0.6953125\n",
      "> 31 | 0.5546118021011353 | 0.6953125\n",
      "> 32 | 0.5527703762054443 | 0.6953125\n",
      "> 33 | 0.5509762763977051 | 0.7\n",
      "> 34 | 0.5492278337478638 | 0.7015625\n",
      "> 35 | 0.5475234985351562 | 0.7015625\n",
      "> 36 | 0.545861542224884 | 0.7046875\n",
      "> 37 | 0.5442405343055725 | 0.70625\n",
      "> 38 | 0.5426591038703918 | 0.709375\n",
      "> 39 | 0.5411158204078674 | 0.7125\n",
      "> 40 | 0.5396094918251038 | 0.7109375\n",
      "> 41 | 0.5381388664245605 | 0.709375\n",
      "> 42 | 0.5367027521133423 | 0.7078125\n",
      "> 43 | 0.535300076007843 | 0.7078125\n",
      "> 44 | 0.533929705619812 | 0.70625\n",
      "> 45 | 0.5325906872749329 | 0.709375\n",
      "> 46 | 0.5312820672988892 | 0.7125\n",
      "> 47 | 0.530002772808075 | 0.7109375\n",
      "> 48 | 0.5287520289421082 | 0.7140625\n",
      "> 49 | 0.5275288820266724 | 0.71875\n",
      "> 50 | 0.5263325572013855 | 0.725\n",
      "> 51 | 0.5251622200012207 | 0.7328125\n",
      "> 52 | 0.5240172147750854 | 0.73125\n",
      "> 53 | 0.5228966474533081 | 0.73125\n",
      "> 54 | 0.5217998623847961 | 0.7296875\n",
      "> 55 | 0.520726203918457 | 0.73125\n",
      "> 56 | 0.5196750164031982 | 0.73125\n",
      "> 57 | 0.5186455845832825 | 0.7328125\n",
      "> 58 | 0.5176373720169067 | 0.734375\n",
      "> 59 | 0.5166497826576233 | 0.7375\n",
      "> 60 | 0.5156822204589844 | 0.7375\n",
      "> 61 | 0.514734148979187 | 0.734375\n",
      "> 62 | 0.5138050317764282 | 0.734375\n",
      "> 63 | 0.5128942728042603 | 0.734375\n",
      "> 64 | 0.5120015740394592 | 0.7390625\n",
      "> 65 | 0.5111262798309326 | 0.7390625\n",
      "> 66 | 0.5102680325508118 | 0.74375\n",
      "> 67 | 0.5094263553619385 | 0.746875\n",
      "> 68 | 0.5086007714271545 | 0.7453125\n",
      "> 69 | 0.5077909231185913 | 0.74375\n",
      "> 70 | 0.5069963335990906 | 0.746875\n",
      "> 71 | 0.5062166452407837 | 0.75\n",
      "> 72 | 0.5054515600204468 | 0.7484375\n",
      "> 73 | 0.5047005414962769 | 0.7484375\n",
      "> 74 | 0.5039634704589844 | 0.75\n",
      "> 75 | 0.5032397508621216 | 0.7484375\n",
      "> 76 | 0.5025292634963989 | 0.7453125\n",
      "> 77 | 0.5018315315246582 | 0.74375\n",
      "> 78 | 0.5011463165283203 | 0.7453125\n",
      "> 79 | 0.5004732608795166 | 0.7453125\n",
      "> 80 | 0.49981221556663513 | 0.746875\n",
      "> 81 | 0.4991627335548401 | 0.7484375\n",
      "> 82 | 0.49852463603019714 | 0.75\n",
      "> 83 | 0.49789759516716003 | 0.75\n",
      "> 84 | 0.4972813129425049 | 0.75\n",
      "> 85 | 0.49667567014694214 | 0.75\n",
      "> 86 | 0.4960803687572479 | 0.7515625\n",
      "> 87 | 0.49549514055252075 | 0.753125\n",
      "> 88 | 0.4949197471141815 | 0.7546875\n",
      "> 89 | 0.4943540394306183 | 0.753125\n",
      "> 90 | 0.4937977194786072 | 0.75625\n",
      "> 91 | 0.4932505488395691 | 0.7578125\n",
      "> 92 | 0.4927124083042145 | 0.7578125\n",
      "> 93 | 0.4921830892562866 | 0.759375\n",
      "> 94 | 0.4916623830795288 | 0.759375\n",
      "> 95 | 0.4911500811576843 | 0.759375\n",
      "> 96 | 0.49064600467681885 | 0.759375\n",
      "> 97 | 0.4901500344276428 | 0.7609375\n",
      "> 98 | 0.4896618723869324 | 0.7609375\n",
      "> 99 | 0.4891814589500427 | 0.7625\n",
      "> 100 | 0.48870861530303955 | 0.7640625\n",
      "> Evaluation\n",
      "> Class Acc = 0.74609375\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.9554458856582642 | 0.9824691638350487 | 0.9709989279508591\n",
      "> Confusion Matrix \n",
      "TN: 20.0 | FP: 54.0 \n",
      "FN: 11.0 | TP: 171.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 8.0 | FP: 22.0 \n",
      "FN: 4.0 | TP: 45.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 12.0 | FP: 32.0 \n",
      "FN: 7.0 | TP: 126.0\n"
     ]
    }
   ],
   "source": [
    "for cv_seed in cv_seeds:\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        x, y, a, test_size=0.3, random_state=cv_seed)\n",
    "\n",
    "    train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "    train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # train below\n",
    "\n",
    "    opt = Adam(learning_rate=lr)\n",
    "\n",
    "    model = UnfairLogisticRegression(xdim, ydim, batch_size)\n",
    "    ret = train_loop(model, train_data, epochs, opt)\n",
    "    Y, A, Y_hat = evaluation(model, test_data)\n",
    "    \n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1  = compute_metrics(Y, A, Y_hat, adim=adim)\n",
    "    \n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    \n",
    "    tradeoff = []\n",
    "    \n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "    \n",
    "    result = ['UnfairLR', cv_seed, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    del(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc78803a910>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD4CAYAAADSIzzWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAUEklEQVR4nO3df7SVVZ3H8fdHCX+VgoCIQIqFGTk0OcTYNJmFJv4oMBwDy8gh7zSRldVolg2rBlOnlJE0m2uSaIoyakkttSE0hWWSpKKCmmgpFxFIxB+UP+493/njPoPH6/1x7uHcuzmbz8u11z1nPw/7+S7WXV+232c/+1FEYGZmvW+H1AGYmW2vnIDNzBJxAjYzS8QJ2MwsESdgM7NE+vT0BQbvcaCXWdgbfLz/6NQh2Dbokj/N19aO8eqfH68457xp4P5bfb2t4RmwmVkiPT4DNjPrVaWW1BFUzAnYzPLS0pw6goo5AZtZViJKqUOomBOwmeWl5ARsZpaGZ8BmZon4JpyZWSKeAZuZpRFeBWFmlohvwpmZJVJHJQg/imxmeSm1VN66IGmOpPWSHmzTf6qkhyWtkPSfZf1nSlol6RFJR3Y1vmfAZpaX2s6ALwcuAq74/w5JHwImAO+OiJcl7VX0jwImA+8C9gF+LemAiOgw03sGbGZ5aWmuvHUhIu4ANrbp/lfg3Ih4uThnfdE/AbgmIl6OiD8Cq4CxnY3vBGxmeSmVKm6SGiQtK2sNFVzhAOADkpZKul3Se4v+ocDqsvOair4OuQRhZlnp5P/42zk3GoHGbl6iD7AncAjwXmC+pP27OcaWgczM8tHzqyCagBsiIoDfSSoBA4E1wPCy84YVfR1yCcLM8tKNEkSVfg58CEDSAUBf4M/AAmCypJ0kjQBGAr/rbCDPgM0sLzWcAUuaBxwGDJTUBMwA5gBziqVprwBTi9nwCknzgZVAMzC9sxUQ4ARsZrlpebVmQ0XElA4OfaqD888Gzq50fCdgM8uLH0U2M0ukjh5FdgI2s7x4BmxmlogTsJlZGlHDm3A9zQnYzPLiGrCZWSIuQZiZJeIZsJlZIp4Bm5kl4hmwmVkizX4rsplZGp4Bm5kl4hqwmVkingGbmSXiGbCZWSKeAZuZJeJVEGZmiUSkjqBifimnmeWlhi/llDRH0vri/W9tj31VUkgaWHyXpNmSVkm6X9LBXY3vBGxmeantW5EvB8a37ZQ0HPgI8GRZ91G0vgl5JNAAXNLV4E7AZpaXKFXeuhoq4g5gYzuHZgGnA+X1jgnAFdHqLqCfpCGdje8EbGZ5aWmpuElqkLSsrDV0NbykCcCaiFje5tBQYHXZ96air0O+CWdmeenGOuCIaAQaKz1f0q7AN2gtP2w1J2Azy0vPPojxNmAEsFwSwDDgHkljgTXA8LJzhxV9HXIJwszyUsMa8BuGjnggIvaKiP0iYj9aywwHR8TTwALg08VqiEOA5yJibWfjOQGbWVaiFBW3rkiaB/wWeIekJknTOjn9JuBxYBVwKfD5rsZ3CcLM8lLDEkRETOni+H5lnwOY3p3xnYDNLC8tLakjqJgTsJnlxbuhmZkl4gRs+wzdm4t+dB4D9xpARPDTy+dz6Y+upF//PWj8yQUMf+tQVj+5hlM+cxrPbXo+dbjWi2YuuYiXXnyJUqlEqbmFcz925pZj4z57LMef9Wm+9p5pbH72hYRR1rE62ozHCbiHNDe3MOOs83hg+Up2e/NuLLz9em6/7U4+8cnjWHz7Xfxg1qWcetopnHraKcyccX7qcK2XzZry7Tck2P5DBjDq0NE807QhUVSZqKMZcJfL0CQdKOmMYpef2cXnd/ZGcPVs/boNPLB8JQCbX9zMo488xt77DGb80eO49uqfA3Dt1T/nqGMOTxmmbUOO/9ZUbjjnKl6/vYB1Wykqb4l1OgOWdAYwBbgG+F3RPQyYJ+maiDi3h+PLwvC3DuWg0e/knmXLGTRoAOvXtc5w1q/bwKBBAxJHZ70tAr545TchYPHVC1kybxGjjxjDpnUbWfPQE6nDq38ZrYKYBrwrIl4t75R0AbACaDcBFxtaNAC8ZefB7NK3Xw1CrU+77rYrl105m2+deQ4vvrD5DcfDs53tzveP/xbPrXuWtwzYnS/+9Cyefuwpxk8/jtknzUwdWhYioxJECdinnf4hxbF2RURjRIyJiDHbc/Lt06cPc66czfXzf8FNv1gIwIYNz7DX4EEA7DV4EH/e0N5Od5az59Y9C8ALzzzPfb+6m5F/P4qBw/birJu/x8wlF9Fv7wF845fnsfugPRJHWqdyKUEAXwYWSXqU17ZZeyvwduALPRlYDmZdNJNHH3mM/7748i19v7r5Vj5x4kR+MOtSPnHiRG65aVG6AK3X9d1lJ7SDeHnzS/TdZSfe+YHR3DT7Ok4fc8qWc2YuuYhzPnqmV0FUK5eXckbELZIOAMby2r6Wa4C7I6J+Ci0JjD3kYE6YMpGVDz7CosU/A+C735nFDy64lEvnzuLEkybRtPopTvnMaYkjtd60+8A9+JfGrwGww447cveNS1h5e9ttZW2rbAMz20openjN3OA9Dqyfvw3rNR/vPzp1CLYNuuRP87W1Y2z+98kV55zdvnPNVl9va3gdsJnlJZcShJlZ3amjEoQTsJllpZ6WoTkBm1lePAM2M0vECdjMLJGMHkU2M6srlbzrbVvhl3KaWV5q+CiypDmS1kt6sKzve5IelnS/pJ9J6ld27ExJqyQ9IunIrsZ3AjazvJRKlbeuXQ6Mb9O3EDgoIkYDfwDOBJA0CpgMvKv4Mz+UtGNngzsBm1leajgDjog7gI1t+v43IpqLr3fRukUvwATgmoh4OSL+SOvr6cd2Nr4TsJnlpRsJWFKDpGVlraGbV/tn4Obi81Be27QMoInX9tBpl2/CmVlWoqXyBzEiohForOY6kr4JNANXVfPnwQnYzHLTC6sgJH0GOBYYF6/taLYGGF522rCir0MuQZhZVqIUFbdqSBoPnA58LCL+UnZoATBZ0k6SRgAjee1Vbu3yDNjM8lLDGbCkecBhwEBJTcAMWlc97AQslARwV0R8LiJWSJoPrKS1NDG9q33TnYDNLC813IsnIqa0031ZJ+efDZxd6fhOwGaWlWj2bmhmZmnUT/51AjazvNTTXhBOwGaWF8+AzczS8AzYzCwVz4DNzNLYsk1OHXACNrOs1NFb6Z2AzSwzTsBmZml4BmxmlogTsJlZItGi1CFUzAnYzLLiGbCZWSJR8gzYzCwJz4DNzBKJ8AzYzCwJz4DNzBIp1dEqCL+U08yyEiVV3LoiaY6k9ZIeLOvbU9JCSY8WP/sX/ZI0W9IqSfdLOrir8Z2AzSwrtUzAwOXA+DZ9XwcWRcRIYFHxHeAoWt+EPBJoAC7panAnYDPLSkTlreux4g5gY5vuCcDc4vNcYGJZ/xXR6i6gn6QhnY3vBGxmWenODFhSg6RlZa2hgksMjoi1xeengcHF56HA6rLzmoq+DvkmnJllpTvL0CKiEWis/loRkqp+BYcTsJllpaXnV0GskzQkItYWJYb1Rf8aYHjZecOKvg65BGFmWYlQxa1KC4CpxeepwI1l/Z8uVkMcAjxXVqpol2fAZpaVWu4FIWkecBgwUFITMAM4F5gvaRrwBHBCcfpNwNHAKuAvwMldje8EbGZZqWR1Q+VjxZQODo1r59wApndnfCdgM8uKd0MzM0ukpVQ/t7acgM0sK7UsQfQ0J2Azy0rJ21GamaXh/YDNzBJxCaLMM399oacvYXVo9mPnpg7BMuUShJlZIl4FYWaWSB1VIJyAzSwvLkGYmSXiVRBmZonU0UuRnYDNLC+BZ8BmZkk0uwRhZpaGZ8BmZom4BmxmlohnwGZmidTTDLh+ntkzM6tAC6q4dUXSaZJWSHpQ0jxJO0saIWmppFWSrpXUt9pYnYDNLCslVd46I2ko8EVgTEQcBOwITAbOA2ZFxNuBZ4Fp1cbqBGxmWSmhilsF+gC7SOoD7AqsBT4MXFccnwtMrDZWJ2Azy0p0o0lqkLSsrDVsGSdiDfB94ElaE+9zwO+BTRHRXJzWBAytNlbfhDOzrHTnJlxENAKN7R2T1B+YAIwANgH/A4zf6gDLOAGbWVZKqtkytMOBP0bEBgBJNwDvB/pJ6lPMgocBa6q9gEsQZpaVlm60LjwJHCJpV0kCxgErgduA44tzpgI3VhurE7CZZaVWqyAiYimtN9vuAR6gNV82AmcAX5G0ChgAXFZtrC5BmFlWKlzdUJGImAHMaNP9ODC2FuM7AZtZVvxKIjOzRLoqLWxLnIDNLCv1tBeEE7CZZaXFM2AzszQ8AzYzS8QJ2MwskTp6JZwTsJnlxTNgM7NEKnjEeJvhBGxmWfE6YDOzRFyCMDNLxAnYzCwR7wVhZpaIa8BmZol4FYSZWSKlOipCOAGbWVZ8E87MLJH6mf/6nXBmlplSN1pXJPWTdJ2khyU9JOl9kvaUtFDSo8XP/tXG6gRsZllpVlTcKnAhcEtEHAi8G3gI+DqwKCJGAouK71VxAjazrEQ3Wmck7QEcSvHW44h4JSI2AROAucVpc4GJ1cbqBGxmWelOCUJSg6RlZa2hbKgRwAbgJ5LulfRjSbsBgyNibXHO08DgamP1TTgzy0p3lqFFRCPQ2MHhPsDBwKkRsVTShbQpN0RESJXVMtrjGbCZZaVWJQigCWiKiKXF9+toTcjrJA0BKH6urzZWJ2Azy0qtVkFExNPAaknvKLrGASuBBcDUom8qcGO1sboEYWZZaantSuBTgask9QUeB06mdeI6X9I04AnghGoHdwI2s6zU8km4iLgPGNPOoXG1GN8J2MyyEnX0LJwTsJllpZ72gvBNuB5yaeP5PNW0nPvuXbSlb9KkY1l+36288tJq/u7g0Qmjs9501ncv4NBjJjPxU5/b0vfVb53DpKnTmTR1Oh+ZNJVJU6cDsOm55zn5C2fw3sOP4+zzf5gq5LpWIipuqTkB95ArrpjPMcd+8nV9K1Y8zD+dcAqLF9+VKCpLYeLRR/CjC2a+ru/8/ziT6+dezPVzL+aIw/6Rwz/4DwD07duXU085ia9N/2yKULNQw2VoPc4JuIcsXrKUjc9uel3fww+v4g9/eCxRRJbKmL/9G/bY/S3tHosIbrn1Do4+4jAAdt1lZw5+90Hs1LdvL0aYl2ai4paaa8BmCf1++YMM6N+ffYcPTR1KNurpJlzVM2BJJ3dybMvz1aXS5movYZa9mxb+hqOP+GDqMLJSy+0oe9rWlCC+3dGBiGiMiDERMWaHHXbbikuY5au5uYVf334n48cdmjqUrEQ3/kut0xKEpPs7OsRW7ABkZnDXsnvZf99h7L3XoNShZGVbmNlWqqsa8GDgSODZNv0C7uyRiDLx0ysv5oOHvo+BA/fkT48v49vf+T4bn93EhbNmMmjQniy48QqWL1/B0W1WSlh+/m3Gudx97/1s2vQ84yZ+is9PO4lJHz2Sm399O0cdftgbzv/IpKm8uPkvvNrczK2L76Rx1tm8bcS+vR94nWqJ9DPbSik6CVbSZcBPImJJO8eujogTu7pAn75D6+dvw3rNX59anDoE2wa9aeD+2toxTtz3uIpzztVP/Gyrr7c1Op0BR8S0To51mXzNzHrbtlDbrZSXoZlZVnKqAZuZ1ZVt4RHjSjkBm1lWXIIwM0uknlZBOAGbWVZcgjAzS6SebsJ5NzQzy0qtH0WWtKOkeyX9svg+QtJSSaskXVu8L64qTsBmlpUe2JD9S8BDZd/PA2ZFxNtpfUq4w+cluuIEbGZZiYiKW1ckDQOOAX5cfBfwYeC64pS5wMRqY3UCNrOstBAVt/Ktc4vW0Ga4/wJO57XS8gBgU0Q0F9+bgKo3c/ZNODPLSndWQUREI9DY3jFJxwLrI+L3kg6rTXSv5wRsZlmppLRQofcDH5N0NLAzsDtwIdBPUp9iFjwMWFPtBVyCMLOs1OomXEScGRHDImI/YDJwa0R8ErgNOL44bSpwY7WxOgGbWVZ64Y0YZwBfkbSK1prwZdUO5BKEmWWlJx5FjojfAL8pPj8OjK3FuE7AZpYVP4psZpaIE7CZWSI1XAXR45yAzSwrngGbmSXiDdnNzBJpifrZkNIJ2Myy4hqwmVkirgGbmSXiGrCZWSIllyDMzNLwDNjMLBGvgjAzS8QlCDOzRFyCMDNLxDNgM7NEPAM2M0ukJVpSh1AxJ2Azy0o9PYrsd8KZWVZq9VJOScMl3SZppaQVkr5U9O8paaGkR4uf/auN1QnYzLISERW3LjQDX42IUcAhwHRJo4CvA4siYiSwqPheFSdgM8tKKaLi1pmIWBsR9xSfXwAeAoYCE4C5xWlzgYnVxuoasJllpSdWQUjaD3gPsBQYHBFri0NPA4OrHdcJ2Myy0p1HkSU1AA1lXY0R0djmnDcD1wNfjojnJW05FhEhqeqM7wRsZlnpziqIItk2dnRc0ptoTb5XRcQNRfc6SUMiYq2kIcD6amN1DdjMslKrGrBap7qXAQ9FxAVlhxYAU4vPU4Ebq43VM2Azy0oN1wG/HzgJeEDSfUXfN4BzgfmSpgFPACdUewEnYDPLSq1eSRQRSwB1cHhcLa7hBGxmWamnJ+GcgM0sK96Q3cwsEW9HaWaWiEsQZmaJeD9gM7NEPAM2M0uknmrAqqd/LeqdpIa2z5mb+fdi++VHkXtXQ9en2HbIvxfbKSdgM7NEnIDNzBJxAu5drvNZe/x7sZ3yTTgzs0Q8AzYzS8QJ2MwsESfgXiJpvKRHJK2SVPVrrC0fkuZIWi/pwdSxWBpOwL1A0o7AxcBRwChgiqRRaaOybcDlwPjUQVg6TsC9YyywKiIej4hXgGuACYljssQi4g5gY+o4LB0n4N4xFFhd9r2p6DOz7ZgTsJlZIk7AvWMNMLzs+7Ciz8y2Y07AveNuYKSkEZL6ApOBBYljMrPEnIB7QUQ0A18AfgU8BMyPiBVpo7LUJM0Dfgu8Q1KTpGmpY7Le5UeRzcwS8QzYzCwRJ2Azs0ScgM3MEnECNjNLxAnYzCwRJ2Azs0ScgM3MEvk/mba1PRkln8YAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving into DF then CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>cv_seed</th>\n",
       "      <th>clas_acc</th>\n",
       "      <th>dp</th>\n",
       "      <th>deqodds</th>\n",
       "      <th>deqopp</th>\n",
       "      <th>trade_dp</th>\n",
       "      <th>trade_deqodds</th>\n",
       "      <th>trade_deqopp</th>\n",
       "      <th>TN_a0</th>\n",
       "      <th>FP_a0</th>\n",
       "      <th>FN_a0</th>\n",
       "      <th>TP_a0</th>\n",
       "      <th>TN_a1</th>\n",
       "      <th>FP_a1</th>\n",
       "      <th>FN_a1</th>\n",
       "      <th>TP_a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>13</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846847</td>\n",
       "      <td>0.846847</td>\n",
       "      <td>0.846847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>29</td>\n",
       "      <td>0.660156</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.795294</td>\n",
       "      <td>0.795294</td>\n",
       "      <td>0.795294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>42</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.831050</td>\n",
       "      <td>0.831050</td>\n",
       "      <td>0.831050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>55</td>\n",
       "      <td>0.695312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820276</td>\n",
       "      <td>0.820276</td>\n",
       "      <td>0.820276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>73</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.831050</td>\n",
       "      <td>0.831050</td>\n",
       "      <td>0.831050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>13</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>0.829457</td>\n",
       "      <td>0.769079</td>\n",
       "      <td>0.939257</td>\n",
       "      <td>0.765638</td>\n",
       "      <td>0.738866</td>\n",
       "      <td>0.809302</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>29</td>\n",
       "      <td>0.699219</td>\n",
       "      <td>0.928771</td>\n",
       "      <td>0.962169</td>\n",
       "      <td>0.947213</td>\n",
       "      <td>0.797811</td>\n",
       "      <td>0.809885</td>\n",
       "      <td>0.804539</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>42</td>\n",
       "      <td>0.707031</td>\n",
       "      <td>0.866201</td>\n",
       "      <td>0.820455</td>\n",
       "      <td>0.934615</td>\n",
       "      <td>0.778564</td>\n",
       "      <td>0.759532</td>\n",
       "      <td>0.805048</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>55</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.911275</td>\n",
       "      <td>0.900214</td>\n",
       "      <td>0.935044</td>\n",
       "      <td>0.803643</td>\n",
       "      <td>0.799312</td>\n",
       "      <td>0.812753</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>73</td>\n",
       "      <td>0.746094</td>\n",
       "      <td>0.955446</td>\n",
       "      <td>0.982469</td>\n",
       "      <td>0.970999</td>\n",
       "      <td>0.837891</td>\n",
       "      <td>0.848120</td>\n",
       "      <td>0.843817</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name  cv_seed  clas_acc        dp   deqodds    deqopp  trade_dp  \\\n",
       "0  UnfairLR-decay       13  0.734375  1.000000  1.000000  1.000000  0.846847   \n",
       "1  UnfairLR-decay       29  0.660156  1.000000  1.000000  1.000000  0.795294   \n",
       "2  UnfairLR-decay       42  0.710938  1.000000  1.000000  1.000000  0.831050   \n",
       "3  UnfairLR-decay       55  0.695312  1.000000  1.000000  1.000000  0.820276   \n",
       "4  UnfairLR-decay       73  0.710938  1.000000  1.000000  1.000000  0.831050   \n",
       "5        UnfairLR       13  0.710938  0.829457  0.769079  0.939257  0.765638   \n",
       "6        UnfairLR       29  0.699219  0.928771  0.962169  0.947213  0.797811   \n",
       "7        UnfairLR       42  0.707031  0.866201  0.820455  0.934615  0.778564   \n",
       "8        UnfairLR       55  0.718750  0.911275  0.900214  0.935044  0.803643   \n",
       "9        UnfairLR       73  0.746094  0.955446  0.982469  0.970999  0.837891   \n",
       "\n",
       "   trade_deqodds  trade_deqopp  TN_a0  FP_a0  FN_a0  TP_a0  TN_a1  FP_a1  \\\n",
       "0       0.846847      0.846847    0.0   26.0    0.0   58.0    0.0   42.0   \n",
       "1       0.795294      0.795294    0.0   36.0    0.0   43.0    0.0   51.0   \n",
       "2       0.831050      0.831050    0.0   22.0    0.0   52.0    0.0   52.0   \n",
       "3       0.820276      0.820276    0.0   26.0    0.0   57.0    0.0   52.0   \n",
       "4       0.831050      0.831050    0.0   30.0    0.0   49.0    0.0   44.0   \n",
       "5       0.738866      0.809302   16.0   10.0   12.0   46.0    9.0   33.0   \n",
       "6       0.809885      0.804539   10.0   26.0    5.0   38.0   13.0   38.0   \n",
       "7       0.759532      0.805048    9.0   13.0    7.0   45.0    6.0   46.0   \n",
       "8       0.799312      0.812753    9.0   17.0    7.0   50.0   11.0   41.0   \n",
       "9       0.848120      0.843817    8.0   22.0    4.0   45.0   12.0   32.0   \n",
       "\n",
       "   FN_a1  TP_a1  \n",
       "0    0.0  130.0  \n",
       "1    0.0  126.0  \n",
       "2    0.0  130.0  \n",
       "3    0.0  121.0  \n",
       "4    0.0  133.0  \n",
       "5   19.0  111.0  \n",
       "6    8.0  118.0  \n",
       "7    9.0  121.0  \n",
       "8    7.0  114.0  \n",
       "9    7.0  126.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(results, columns=header)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f'{data_name}-result/unfair_lr-{epochs}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('falsb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "34ca74ed6235dfc7dda926bb3adb31e801e3d02679121d5b444ee035e270bd57"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "41359ec383f887151a607ad1e28cb7dbc05f61385692c63e2bb2f343bf03f280"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
