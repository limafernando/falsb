{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.data import Dataset\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from models.unfair_lr.models import UnfairLogisticRegression\n",
    "from models.unfair_lr.learning import train_loop\n",
    "from util.evaluation import *\n",
    "from util.load_data import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_seeds = [13, 29, 42, 55, 73]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'adult'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, a = load_data(data_name)\n",
    "raw_data = (x, y, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdim = x.shape[1]\n",
    "ydim = y.shape[1]\n",
    "adim = a.shape[1]\n",
    "zdim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"model_name\", \"cv_seed\", \"clas_acc\", \"dp\", \"deqodds\", \"deqopp\", \"trade_dp\", \"trade_deqodds\", \"trade_deqopp\", \"TN_a0\", \"FP_a0\", \"FN_a0\", \"TP_a0\", \"TN_a1\", \"FP_a1\", \"FN_a1\", \"TP_a1\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.44726601243019104 | 0.6350581983805668\n",
      "> 2 | 0.3877369463443756 | 0.7975392206477733\n",
      "> 3 | 0.368744820356369 | 0.8197747975708503\n",
      "> 4 | 0.35935795307159424 | 0.8251201923076923\n",
      "> 5 | 0.3537173569202423 | 0.827397520242915\n",
      "> 6 | 0.34990936517715454 | 0.8287892206477733\n",
      "> 7 | 0.3471410274505615 | 0.8294218117408907\n",
      "> 8 | 0.34502410888671875 | 0.8300544028340081\n",
      "> 9 | 0.3433387875556946 | 0.830813512145749\n",
      "> 10 | 0.341957151889801 | 0.8306237348178138\n",
      "> 11 | 0.34079819917678833 | 0.8307818825910931\n",
      "> 12 | 0.33980798721313477 | 0.831066548582996\n",
      "> 13 | 0.3389490842819214 | 0.8312879554655871\n",
      "> 14 | 0.33819472789764404 | 0.8316358805668016\n",
      "> 15 | 0.3375251889228821 | 0.8319521761133604\n",
      "> 16 | 0.3369255065917969 | 0.8321103238866396\n",
      "> 17 | 0.33638420701026917 | 0.8322368421052632\n",
      "> 18 | 0.33589231967926025 | 0.8325531376518218\n",
      "> 19 | 0.33544260263442993 | 0.8329643218623481\n",
      "> 20 | 0.3350293040275574 | 0.8331540991902834\n",
      "> 21 | 0.33464765548706055 | 0.8333122469635628\n",
      "> 22 | 0.33429375290870667 | 0.8335652834008097\n",
      "> 23 | 0.33396434783935547 | 0.8336918016194332\n",
      "> 24 | 0.33365675806999207 | 0.8336601720647774\n",
      "> 25 | 0.33336853981018066 | 0.8338499493927125\n",
      "> 26 | 0.3330976963043213 | 0.833976467611336\n",
      "> 27 | 0.33284246921539307 | 0.8339132085020243\n",
      "> 28 | 0.3326014280319214 | 0.8339132085020243\n",
      "> 29 | 0.33237338066101074 | 0.8341346153846154\n",
      "> 30 | 0.3321570158004761 | 0.8344192813765182\n",
      "> 31 | 0.3319515287876129 | 0.8346090587044535\n",
      "> 32 | 0.3317558169364929 | 0.8347672064777328\n",
      "> 33 | 0.33156922459602356 | 0.834956983805668\n",
      "> 34 | 0.3313920497894287 | 0.8350202429149798\n",
      "> 35 | 0.33122265338897705 | 0.8351783906882592\n",
      "> 36 | 0.33106037974357605 | 0.8352732793522267\n",
      "> 37 | 0.3309047520160675 | 0.8353365384615384\n",
      "> 38 | 0.3307552933692932 | 0.8353997975708503\n",
      "> 39 | 0.33061161637306213 | 0.8354314271255061\n",
      "> 40 | 0.330473393201828 | 0.835463056680162\n",
      "> 41 | 0.3303401470184326 | 0.8354946862348178\n",
      "> 42 | 0.33021169900894165 | 0.8355579453441295\n",
      "> 43 | 0.3300877511501312 | 0.8356212044534413\n",
      "> 44 | 0.3299679756164551 | 0.8356528340080972\n",
      "> 45 | 0.3298521339893341 | 0.8357477226720648\n",
      "> 46 | 0.329740047454834 | 0.8358426113360324\n",
      "> 47 | 0.3296315371990204 | 0.8359375\n",
      "> 48 | 0.3295263350009918 | 0.8359375\n",
      "> 49 | 0.32942426204681396 | 0.8359691295546559\n",
      "> 50 | 0.32932525873184204 | 0.8360640182186235\n",
      "> 51 | 0.32922911643981934 | 0.8360640182186235\n",
      "> 52 | 0.3291357159614563 | 0.8360956477732794\n",
      "> 53 | 0.3290448784828186 | 0.8361272773279352\n",
      "> 54 | 0.3289565443992615 | 0.8362221659919028\n",
      "> 55 | 0.32887059450149536 | 0.8362221659919028\n",
      "> 56 | 0.3287869095802307 | 0.8363170546558705\n",
      "> 57 | 0.3287053108215332 | 0.8363486842105263\n",
      "> 58 | 0.32862573862075806 | 0.836411943319838\n",
      "> 59 | 0.3285481929779053 | 0.8364752024291497\n",
      "> 60 | 0.3284724950790405 | 0.8365068319838057\n",
      "> 61 | 0.3283986449241638 | 0.8364752024291497\n",
      "> 62 | 0.328326553106308 | 0.8365384615384616\n",
      "> 63 | 0.3282560706138611 | 0.8364752024291497\n",
      "> 64 | 0.3281872272491455 | 0.8365068319838057\n",
      "> 65 | 0.3281199038028717 | 0.8365068319838057\n",
      "> 66 | 0.3280540108680725 | 0.8365068319838057\n",
      "> 67 | 0.3279895782470703 | 0.8364752024291497\n",
      "> 68 | 0.3279263973236084 | 0.8365068319838057\n",
      "> 69 | 0.3278646469116211 | 0.8365700910931174\n",
      "> 70 | 0.32780420780181885 | 0.8365700910931174\n",
      "> 71 | 0.3277450203895569 | 0.8366017206477733\n",
      "> 72 | 0.32768696546554565 | 0.836664979757085\n",
      "> 73 | 0.32763004302978516 | 0.8366017206477733\n",
      "> 74 | 0.3275742530822754 | 0.8365700910931174\n",
      "> 75 | 0.3275195062160492 | 0.8365068319838057\n",
      "> 76 | 0.32746583223342896 | 0.8364752024291497\n",
      "> 77 | 0.3274131417274475 | 0.8366017206477733\n",
      "> 78 | 0.32736140489578247 | 0.8366017206477733\n",
      "> 79 | 0.32731062173843384 | 0.8366966093117408\n",
      "> 80 | 0.32726070284843445 | 0.8366966093117408\n",
      "> 81 | 0.3272116780281067 | 0.8367282388663968\n",
      "> 82 | 0.32716354727745056 | 0.8367282388663968\n",
      "> 83 | 0.3271161913871765 | 0.836664979757085\n",
      "> 84 | 0.3270697295665741 | 0.8366966093117408\n",
      "> 85 | 0.3270239233970642 | 0.836664979757085\n",
      "> 86 | 0.3269789218902588 | 0.836664979757085\n",
      "> 87 | 0.32693466544151306 | 0.8366966093117408\n",
      "> 88 | 0.32689112424850464 | 0.8367282388663968\n",
      "> 89 | 0.3268483281135559 | 0.8367282388663968\n",
      "> 90 | 0.3268061876296997 | 0.8368231275303644\n",
      "> 91 | 0.32676464319229126 | 0.8368231275303644\n",
      "> 92 | 0.32672375440597534 | 0.8368231275303644\n",
      "> 93 | 0.32668352127075195 | 0.8368231275303644\n",
      "> 94 | 0.3266439139842987 | 0.8367914979757085\n",
      "> 95 | 0.32660478353500366 | 0.8368863866396761\n",
      "> 96 | 0.32656633853912354 | 0.8368863866396761\n",
      "> 97 | 0.3265284299850464 | 0.8368547570850202\n",
      "> 98 | 0.3264910876750946 | 0.8367914979757085\n",
      "> 99 | 0.3264543414115906 | 0.8367914979757085\n",
      "> 100 | 0.3264180123806 | 0.8368231275303644\n",
      "> Evaluation\n",
      "> Class Acc = 0.8372334241867065\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8062827885150909 | 0.8565637357532978 | 0.7999807000160217\n",
      "> Confusion Matrix \n",
      "TN: 9449.0 | FP: 709.0 \n",
      "FN: 1489.0 | TP: 1857.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3807.0 | FP: 62.0 \n",
      "FN: 314.0 | TP: 197.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 5642.0 | FP: 647.0 \n",
      "FN: 1175.0 | TP: 1660.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.4965749979019165 | 0.6365764170040485\n",
      "> 2 | 0.4529130160808563 | 0.7993421052631579\n",
      "> 3 | 0.4415826201438904 | 0.8209134615384616\n",
      "> 4 | 0.437430739402771 | 0.8255946356275303\n",
      "> 5 | 0.4353753328323364 | 0.827397520242915\n",
      "> 6 | 0.4341280460357666 | 0.8294218117408907\n",
      "> 7 | 0.43326953053474426 | 0.830307439271255\n",
      "> 8 | 0.43262505531311035 | 0.830813512145749\n",
      "> 9 | 0.43211138248443604 | 0.8314461032388664\n",
      "> 10 | 0.4316839575767517 | 0.8317623987854251\n",
      "> 11 | 0.43131715059280396 | 0.8320470647773279\n",
      "> 12 | 0.4309948682785034 | 0.8321735829959515\n",
      "> 13 | 0.43070679903030396 | 0.8325531376518218\n",
      "> 14 | 0.4304456412792206 | 0.8328061740890689\n",
      "> 15 | 0.4302063286304474 | 0.833248987854251\n",
      "> 16 | 0.4299861490726471 | 0.8335652834008097\n",
      "> 17 | 0.4297810196876526 | 0.8335336538461539\n",
      "> 18 | 0.4295889139175415 | 0.8337234311740891\n",
      "> 19 | 0.4294079542160034 | 0.8337866902834008\n",
      "> 20 | 0.42923691868782043 | 0.8339448380566802\n",
      "> 21 | 0.42907455563545227 | 0.8341029858299596\n",
      "> 22 | 0.42892003059387207 | 0.8341662449392713\n",
      "> 23 | 0.42877262830734253 | 0.8343243927125507\n",
      "> 24 | 0.4286315441131592 | 0.8343560222672065\n",
      "> 25 | 0.4284963011741638 | 0.8344192813765182\n",
      "> 26 | 0.42836642265319824 | 0.8346406882591093\n",
      "> 27 | 0.4282413721084595 | 0.8346723178137652\n",
      "> 28 | 0.428120881319046 | 0.8347988360323887\n",
      "> 29 | 0.42800456285476685 | 0.8351151315789473\n",
      "> 30 | 0.42789214849472046 | 0.8353049089068826\n",
      "> 31 | 0.427783340215683 | 0.8353049089068826\n",
      "> 32 | 0.4276779592037201 | 0.8353681680161943\n",
      "> 33 | 0.42757588624954224 | 0.835463056680162\n",
      "> 34 | 0.42747676372528076 | 0.8355263157894737\n",
      "> 35 | 0.42738038301467896 | 0.8356528340080972\n",
      "> 36 | 0.4272867441177368 | 0.8357793522267206\n",
      "> 37 | 0.42719560861587524 | 0.8358109817813765\n",
      "> 38 | 0.4271068572998047 | 0.8359058704453441\n",
      "> 39 | 0.4270203113555908 | 0.8359691295546559\n",
      "> 40 | 0.4269360303878784 | 0.8360007591093117\n",
      "> 41 | 0.4268537163734436 | 0.8359058704453441\n",
      "> 42 | 0.4267733693122864 | 0.8359691295546559\n",
      "> 43 | 0.4266948103904724 | 0.8360640182186235\n",
      "> 44 | 0.4266180694103241 | 0.836190536437247\n",
      "> 45 | 0.42654305696487427 | 0.8362854251012146\n",
      "> 46 | 0.426469624042511 | 0.8362854251012146\n",
      "> 47 | 0.42639780044555664 | 0.8362537955465587\n",
      "> 48 | 0.4263274073600769 | 0.8362854251012146\n",
      "> 49 | 0.4262584447860718 | 0.8362221659919028\n",
      "> 50 | 0.42619088292121887 | 0.836190536437247\n",
      "> 51 | 0.426124632358551 | 0.8363170546558705\n",
      "> 52 | 0.42605966329574585 | 0.8362854251012146\n",
      "> 53 | 0.42599600553512573 | 0.8363486842105263\n",
      "> 54 | 0.42593345046043396 | 0.8364752024291497\n",
      "> 55 | 0.42587196826934814 | 0.8365068319838057\n",
      "> 56 | 0.4258117079734802 | 0.8365384615384616\n",
      "> 57 | 0.4257524609565735 | 0.836664979757085\n",
      "> 58 | 0.42569419741630554 | 0.8368231275303644\n",
      "> 59 | 0.4256369471549988 | 0.8367914979757085\n",
      "> 60 | 0.4255807101726532 | 0.836918016194332\n",
      "> 61 | 0.425525426864624 | 0.8369496457489879\n",
      "> 62 | 0.42547091841697693 | 0.836918016194332\n",
      "> 63 | 0.42541730403900146 | 0.8369496457489879\n",
      "> 64 | 0.42536461353302 | 0.8369496457489879\n",
      "> 65 | 0.4253125786781311 | 0.836918016194332\n",
      "> 66 | 0.425261527299881 | 0.8369496457489879\n",
      "> 67 | 0.4252111315727234 | 0.8370445344129555\n",
      "> 68 | 0.4251614809036255 | 0.8369812753036437\n",
      "> 69 | 0.42511263489723206 | 0.8370129048582996\n",
      "> 70 | 0.4250645041465759 | 0.8371394230769231\n",
      "> 71 | 0.42501699924468994 | 0.8372026821862348\n",
      "> 72 | 0.42497026920318604 | 0.8372343117408907\n",
      "> 73 | 0.4249240756034851 | 0.8372659412955465\n",
      "> 74 | 0.4248785376548767 | 0.8373292004048583\n",
      "> 75 | 0.42483365535736084 | 0.8374240890688259\n",
      "> 76 | 0.4247893691062927 | 0.8374240890688259\n",
      "> 77 | 0.4247455596923828 | 0.8374240890688259\n",
      "> 78 | 0.4247024357318878 | 0.8375189777327935\n",
      "> 79 | 0.4246598482131958 | 0.8374557186234818\n",
      "> 80 | 0.4246177673339844 | 0.8374557186234818\n",
      "> 81 | 0.4245762228965759 | 0.8374240890688259\n",
      "> 82 | 0.42453524470329285 | 0.8375189777327935\n",
      "> 83 | 0.42449474334716797 | 0.8375189777327935\n",
      "> 84 | 0.4244546592235565 | 0.8375189777327935\n",
      "> 85 | 0.42441511154174805 | 0.8375506072874493\n",
      "> 86 | 0.4243760108947754 | 0.8376138663967612\n",
      "> 87 | 0.42433735728263855 | 0.8376138663967612\n",
      "> 88 | 0.4242991507053375 | 0.8375506072874493\n",
      "> 89 | 0.4242614507675171 | 0.8375189777327935\n",
      "> 90 | 0.42422422766685486 | 0.8375189777327935\n",
      "> 91 | 0.4241872727870941 | 0.8375189777327935\n",
      "> 92 | 0.42415082454681396 | 0.8374873481781376\n",
      "> 93 | 0.424114853143692 | 0.8375189777327935\n",
      "> 94 | 0.4240792393684387 | 0.8374873481781376\n",
      "> 95 | 0.42404401302337646 | 0.8374873481781376\n",
      "> 96 | 0.4240095019340515 | 0.8374240890688259\n",
      "> 97 | 0.42397540807724 | 0.8374557186234818\n",
      "> 98 | 0.4239417314529419 | 0.8374557186234818\n",
      "> 99 | 0.4239083528518677 | 0.8374873481781376\n",
      "> 100 | 0.4238753318786621 | 0.8374873481781376\n",
      "> Evaluation\n",
      "> Class Acc = 0.8365669250488281\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.7984385043382645 | 0.8463364727795124 | 0.7829828262329102\n",
      "> Confusion Matrix \n",
      "TN: 9390.0 | FP: 766.0 \n",
      "FN: 1441.0 | TP: 1907.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3802.0 | FP: 76.0 \n",
      "FN: 311.0 | TP: 195.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 5588.0 | FP: 690.0 \n",
      "FN: 1130.0 | TP: 1712.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5018805265426636 | 0.6341093117408907\n",
      "> 2 | 0.4352794289588928 | 0.7979187753036437\n",
      "> 3 | 0.41004884243011475 | 0.8191738360323887\n",
      "> 4 | 0.39837077260017395 | 0.8252783400809717\n",
      "> 5 | 0.39173513650894165 | 0.8280617408906883\n",
      "> 6 | 0.38743773102760315 | 0.8296748481781376\n",
      "> 7 | 0.38442009687423706 | 0.8303390688259109\n",
      "> 8 | 0.38216692209243774 | 0.8315093623481782\n",
      "> 9 | 0.3804108500480652 | 0.832268471659919\n",
      "> 10 | 0.3789948523044586 | 0.8322052125506073\n",
      "> 11 | 0.3778266906738281 | 0.8324266194331984\n",
      "> 12 | 0.3768429756164551 | 0.8328694331983806\n",
      "> 13 | 0.37600094079971313 | 0.8328694331983806\n",
      "> 14 | 0.3752700686454773 | 0.8329010627530364\n",
      "> 15 | 0.3746282160282135 | 0.8331857287449392\n",
      "> 16 | 0.3740590214729309 | 0.8336285425101214\n",
      "> 17 | 0.37355005741119385 | 0.8339448380566802\n",
      "> 18 | 0.3730914294719696 | 0.8340713562753036\n",
      "> 19 | 0.3726755380630493 | 0.833976467611336\n",
      "> 20 | 0.3722962737083435 | 0.8342611336032388\n",
      "> 21 | 0.37194859981536865 | 0.8343560222672065\n",
      "> 22 | 0.3716282844543457 | 0.8343876518218624\n",
      "> 23 | 0.3713320791721344 | 0.8346090587044535\n",
      "> 24 | 0.37105709314346313 | 0.834703947368421\n",
      "> 25 | 0.3708008825778961 | 0.8345774291497976\n",
      "> 26 | 0.3705615997314453 | 0.8347672064777328\n",
      "> 27 | 0.3703373074531555 | 0.8347988360323887\n",
      "> 28 | 0.3701265752315521 | 0.8348620951417004\n",
      "> 29 | 0.36992812156677246 | 0.8350518724696356\n",
      "> 30 | 0.3697408139705658 | 0.8351783906882592\n",
      "> 31 | 0.36956357955932617 | 0.8353049089068826\n",
      "> 32 | 0.3693956136703491 | 0.8353365384615384\n",
      "> 33 | 0.36923614144325256 | 0.8353997975708503\n",
      "> 34 | 0.36908453702926636 | 0.8353681680161943\n",
      "> 35 | 0.3689400553703308 | 0.8353049089068826\n",
      "> 36 | 0.36880218982696533 | 0.8352732793522267\n",
      "> 37 | 0.3686704635620117 | 0.8353681680161943\n",
      "> 38 | 0.36854439973831177 | 0.8353365384615384\n",
      "> 39 | 0.3684237003326416 | 0.8354314271255061\n",
      "> 40 | 0.3683079779148102 | 0.8354946862348178\n",
      "> 41 | 0.36819690465927124 | 0.8354946862348178\n",
      "> 42 | 0.36809009313583374 | 0.8355895748987854\n",
      "> 43 | 0.36798742413520813 | 0.8355895748987854\n",
      "> 44 | 0.367888480424881 | 0.8355895748987854\n",
      "> 45 | 0.36779317259788513 | 0.8356528340080972\n",
      "> 46 | 0.3677011728286743 | 0.835684463562753\n",
      "> 47 | 0.36761248111724854 | 0.8357793522267206\n",
      "> 48 | 0.36752673983573914 | 0.8358109817813765\n",
      "> 49 | 0.36744385957717896 | 0.8358426113360324\n",
      "> 50 | 0.36736375093460083 | 0.8358426113360324\n",
      "> 51 | 0.36728668212890625 | 0.8359375\n",
      "> 52 | 0.36721229553222656 | 0.8360640182186235\n",
      "> 53 | 0.36714014410972595 | 0.8360956477732794\n",
      "> 54 | 0.3670702278614044 | 0.8361589068825911\n",
      "> 55 | 0.3670024275779724 | 0.8361589068825911\n",
      "> 56 | 0.3669365346431732 | 0.8362221659919028\n",
      "> 57 | 0.36687251925468445 | 0.8362537955465587\n",
      "> 58 | 0.36681029200553894 | 0.8363486842105263\n",
      "> 59 | 0.36674973368644714 | 0.8364435728744939\n",
      "> 60 | 0.3666907548904419 | 0.8364435728744939\n",
      "> 61 | 0.36663341522216797 | 0.8363803137651822\n",
      "> 62 | 0.36657750606536865 | 0.836411943319838\n",
      "> 63 | 0.36652305722236633 | 0.8365068319838057\n",
      "> 64 | 0.36647000908851624 | 0.8365068319838057\n",
      "> 65 | 0.36641812324523926 | 0.8365384615384616\n",
      "> 66 | 0.36636751890182495 | 0.8365384615384616\n",
      "> 67 | 0.36631810665130615 | 0.8366017206477733\n",
      "> 68 | 0.36626988649368286 | 0.836664979757085\n",
      "> 69 | 0.3662227690219879 | 0.8366966093117408\n",
      "> 70 | 0.3661767244338989 | 0.836664979757085\n",
      "> 71 | 0.3661317229270935 | 0.836664979757085\n",
      "> 72 | 0.3660877048969269 | 0.8366333502024291\n",
      "> 73 | 0.36604464054107666 | 0.8366966093117408\n",
      "> 74 | 0.3660024106502533 | 0.836664979757085\n",
      "> 75 | 0.36596110463142395 | 0.8366017206477733\n",
      "> 76 | 0.36592066287994385 | 0.8365700910931174\n",
      "> 77 | 0.3658811151981354 | 0.8365700910931174\n",
      "> 78 | 0.3658423125743866 | 0.8365384615384616\n",
      "> 79 | 0.3658042848110199 | 0.8364752024291497\n",
      "> 80 | 0.3657670021057129 | 0.8364752024291497\n",
      "> 81 | 0.3657304644584656 | 0.8365384615384616\n",
      "> 82 | 0.36569464206695557 | 0.8364435728744939\n",
      "> 83 | 0.3656594753265381 | 0.8365700910931174\n",
      "> 84 | 0.3656249940395355 | 0.8365700910931174\n",
      "> 85 | 0.3655911087989807 | 0.8365384615384616\n",
      "> 86 | 0.36555784940719604 | 0.8365700910931174\n",
      "> 87 | 0.36552518606185913 | 0.8366017206477733\n",
      "> 88 | 0.36549311876296997 | 0.836664979757085\n",
      "> 89 | 0.3654615879058838 | 0.8366966093117408\n",
      "> 90 | 0.365430623292923 | 0.8367282388663968\n",
      "> 91 | 0.36540019512176514 | 0.8367282388663968\n",
      "> 92 | 0.36537036299705505 | 0.8367282388663968\n",
      "> 93 | 0.3653409779071808 | 0.8367598684210527\n",
      "> 94 | 0.36531203985214233 | 0.8366966093117408\n",
      "> 95 | 0.3652836084365845 | 0.8367282388663968\n",
      "> 96 | 0.36525559425354004 | 0.8367282388663968\n",
      "> 97 | 0.3652280867099762 | 0.8367914979757085\n",
      "> 98 | 0.365200936794281 | 0.8368547570850202\n",
      "> 99 | 0.36517423391342163 | 0.836918016194332\n",
      "> 100 | 0.36514797806739807 | 0.8369496457489879\n",
      "> Evaluation\n",
      "> Class Acc = 0.8355302214622498\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.811083123087883 | 0.865257203578949 | 0.8119860887527466\n",
      "> Confusion Matrix \n",
      "TN: 9459.0 | FP: 685.0 \n",
      "FN: 1536.0 | TP: 1824.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3856.0 | FP: 69.0 \n",
      "FN: 308.0 | TP: 191.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 5603.0 | FP: 616.0 \n",
      "FN: 1228.0 | TP: 1633.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.37400007247924805 | 0.6368927125506073\n",
      "> 2 | 0.29875510931015015 | 0.8003858805668016\n",
      "> 3 | 0.27769920229911804 | 0.8215144230769231\n",
      "> 4 | 0.26777780055999756 | 0.8267016700404858\n",
      "> 5 | 0.2620789408683777 | 0.8292004048582996\n",
      "> 6 | 0.2584208846092224 | 0.8298013663967612\n",
      "> 7 | 0.25589150190353394 | 0.8310981781376519\n",
      "> 8 | 0.2540471851825714 | 0.8316991396761133\n",
      "> 9 | 0.25264787673950195 | 0.8322368421052632\n",
      "> 10 | 0.2515532672405243 | 0.8323317307692307\n",
      "> 11 | 0.25067591667175293 | 0.8326796558704453\n",
      "> 12 | 0.2499586045742035 | 0.8328694331983806\n",
      "> 13 | 0.249362513422966 | 0.8330908400809717\n",
      "> 14 | 0.24886028468608856 | 0.8334703947368421\n",
      "> 15 | 0.2484319508075714 | 0.8339132085020243\n",
      "> 16 | 0.24806320667266846 | 0.8342927631578947\n",
      "> 17 | 0.24774286150932312 | 0.8345141700404858\n",
      "> 18 | 0.24746233224868774 | 0.8344192813765182\n",
      "> 19 | 0.24721497297286987 | 0.8346090587044535\n",
      "> 20 | 0.2469956874847412 | 0.8348620951417004\n",
      "> 21 | 0.24680013954639435 | 0.8348620951417004\n",
      "> 22 | 0.2466248720884323 | 0.8348937246963563\n",
      "> 23 | 0.2464670091867447 | 0.8349253542510121\n",
      "> 24 | 0.24632439017295837 | 0.8351783906882592\n",
      "> 25 | 0.24619509279727936 | 0.8353049089068826\n",
      "> 26 | 0.24607738852500916 | 0.8354314271255061\n",
      "> 27 | 0.24596990644931793 | 0.8355263157894737\n",
      "> 28 | 0.24587151408195496 | 0.8355895748987854\n",
      "> 29 | 0.2457813024520874 | 0.8358109817813765\n",
      "> 30 | 0.2456982135772705 | 0.8359375\n",
      "> 31 | 0.24562151730060577 | 0.8360007591093117\n",
      "> 32 | 0.24555067718029022 | 0.8362537955465587\n",
      "> 33 | 0.2454851120710373 | 0.8363486842105263\n",
      "> 34 | 0.2454243302345276 | 0.8364435728744939\n",
      "> 35 | 0.24536779522895813 | 0.8366333502024291\n",
      "> 36 | 0.24531523883342743 | 0.8366017206477733\n",
      "> 37 | 0.24526631832122803 | 0.8366017206477733\n",
      "> 38 | 0.2452206015586853 | 0.8366333502024291\n",
      "> 39 | 0.24517783522605896 | 0.8366333502024291\n",
      "> 40 | 0.24513790011405945 | 0.8366966093117408\n",
      "> 41 | 0.2451004981994629 | 0.8367914979757085\n",
      "> 42 | 0.24506540596485138 | 0.836664979757085\n",
      "> 43 | 0.24503251910209656 | 0.8366333502024291\n",
      "> 44 | 0.2450016587972641 | 0.8367282388663968\n",
      "> 45 | 0.2449725866317749 | 0.8367598684210527\n",
      "> 46 | 0.24494528770446777 | 0.8368231275303644\n",
      "> 47 | 0.2449195683002472 | 0.8367914979757085\n",
      "> 48 | 0.244895339012146 | 0.8367914979757085\n",
      "> 49 | 0.24487242102622986 | 0.8368231275303644\n",
      "> 50 | 0.24485082924365997 | 0.8368863866396761\n",
      "> 51 | 0.24483045935630798 | 0.8370129048582996\n",
      "> 52 | 0.2448112666606903 | 0.8370129048582996\n",
      "> 53 | 0.2447931468486786 | 0.8368863866396761\n",
      "> 54 | 0.24477599561214447 | 0.8369496457489879\n",
      "> 55 | 0.24475976824760437 | 0.8369812753036437\n",
      "> 56 | 0.24474439024925232 | 0.8370761639676113\n",
      "> 57 | 0.24472981691360474 | 0.8371077935222672\n",
      "> 58 | 0.24471601843833923 | 0.8372026821862348\n",
      "> 59 | 0.2447030246257782 | 0.8372975708502024\n",
      "> 60 | 0.24469076097011566 | 0.8374240890688259\n",
      "> 61 | 0.2446790486574173 | 0.8374240890688259\n",
      "> 62 | 0.24466796219348907 | 0.8374557186234818\n",
      "> 63 | 0.2446574568748474 | 0.8375189777327935\n",
      "> 64 | 0.2446475625038147 | 0.8375506072874493\n",
      "> 65 | 0.24463820457458496 | 0.8376138663967612\n",
      "> 66 | 0.24462926387786865 | 0.8376138663967612\n",
      "> 67 | 0.24462085962295532 | 0.837645495951417\n",
      "> 68 | 0.24461284279823303 | 0.8377403846153846\n",
      "> 69 | 0.24460533261299133 | 0.8377087550607287\n",
      "> 70 | 0.2445981502532959 | 0.837645495951417\n",
      "> 71 | 0.24459129571914673 | 0.8377720141700404\n",
      "> 72 | 0.244584858417511 | 0.8378036437246964\n",
      "> 73 | 0.2445787489414215 | 0.8378669028340081\n",
      "> 74 | 0.2445729821920395 | 0.837898532388664\n",
      "> 75 | 0.2445676475763321 | 0.837898532388664\n",
      "> 76 | 0.24456249177455902 | 0.8379301619433198\n",
      "> 77 | 0.24455761909484863 | 0.8379301619433198\n",
      "> 78 | 0.24455304443836212 | 0.8379934210526315\n",
      "> 79 | 0.24454867839813232 | 0.8380883097165992\n",
      "> 80 | 0.24454465508460999 | 0.838119939271255\n",
      "> 81 | 0.24454078078269958 | 0.8380566801619433\n",
      "> 82 | 0.24453715980052948 | 0.838119939271255\n",
      "> 83 | 0.2445337027311325 | 0.8381831983805668\n",
      "> 84 | 0.24453049898147583 | 0.8381831983805668\n",
      "> 85 | 0.24452745914459229 | 0.8382148279352226\n",
      "> 86 | 0.24452465772628784 | 0.8381831983805668\n",
      "> 87 | 0.24452200531959534 | 0.8382148279352226\n",
      "> 88 | 0.24451956152915955 | 0.8380883097165992\n",
      "> 89 | 0.2445172816514969 | 0.8380566801619433\n",
      "> 90 | 0.24451515078544617 | 0.8380566801619433\n",
      "> 91 | 0.24451319873332977 | 0.8381515688259109\n",
      "> 92 | 0.24451139569282532 | 0.8382148279352226\n",
      "> 93 | 0.2445097267627716 | 0.8382148279352226\n",
      "> 94 | 0.24450814723968506 | 0.8382780870445344\n",
      "> 95 | 0.24450674653053284 | 0.8382780870445344\n",
      "> 96 | 0.24450546503067017 | 0.8382780870445344\n",
      "> 97 | 0.24450430274009705 | 0.8383413461538461\n",
      "> 98 | 0.24450324475765228 | 0.838372975708502\n",
      "> 99 | 0.24450230598449707 | 0.8384046052631579\n",
      "> 100 | 0.24450147151947021 | 0.8383413461538461\n",
      "> Evaluation\n",
      "> Class Acc = 0.8343454003334045\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8102344870567322 | 0.8618028797209263 | 0.80466428399086\n",
      "> Confusion Matrix \n",
      "TN: 9416.0 | FP: 694.0 \n",
      "FN: 1543.0 | TP: 1851.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3778.0 | FP: 71.0 \n",
      "FN: 310.0 | TP: 189.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 5638.0 | FP: 623.0 \n",
      "FN: 1233.0 | TP: 1662.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.4469220042228699 | 0.6324962044534413\n",
      "> 2 | 0.3824007511138916 | 0.7995002530364372\n",
      "> 3 | 0.3625262975692749 | 0.8206920546558705\n",
      "> 4 | 0.3534698486328125 | 0.8265751518218624\n",
      "> 5 | 0.3483113646507263 | 0.8287892206477733\n",
      "> 6 | 0.3449519872665405 | 0.8295167004048583\n",
      "> 7 | 0.34256526827812195 | 0.831066548582996\n",
      "> 8 | 0.3407667875289917 | 0.8323949898785425\n",
      "> 9 | 0.3393530547618866 | 0.8330908400809717\n",
      "> 10 | 0.33820587396621704 | 0.8335652834008097\n",
      "> 11 | 0.3372519314289093 | 0.8341978744939271\n",
      "> 12 | 0.3364393413066864 | 0.8345141700404858\n",
      "> 13 | 0.33573901653289795 | 0.8346090587044535\n",
      "> 14 | 0.33512750267982483 | 0.834703947368421\n",
      "> 15 | 0.3345876932144165 | 0.8347988360323887\n",
      "> 16 | 0.3341066241264343 | 0.8349886133603239\n",
      "> 17 | 0.33367449045181274 | 0.8353681680161943\n",
      "> 18 | 0.33328351378440857 | 0.835463056680162\n",
      "> 19 | 0.3329275846481323 | 0.8355579453441295\n",
      "> 20 | 0.33260101079940796 | 0.8356212044534413\n",
      "> 21 | 0.3323005735874176 | 0.8357793522267206\n",
      "> 22 | 0.332023024559021 | 0.8358109817813765\n",
      "> 23 | 0.3317655026912689 | 0.8359058704453441\n",
      "> 24 | 0.33152586221694946 | 0.8359058704453441\n",
      "> 25 | 0.3313019871711731 | 0.8359691295546559\n",
      "> 26 | 0.33109235763549805 | 0.8362221659919028\n",
      "> 27 | 0.33089542388916016 | 0.8363486842105263\n",
      "> 28 | 0.33070990443229675 | 0.8363486842105263\n",
      "> 29 | 0.33053475618362427 | 0.8363170546558705\n",
      "> 30 | 0.330369234085083 | 0.8363486842105263\n",
      "> 31 | 0.3302122950553894 | 0.8364752024291497\n",
      "> 32 | 0.33006319403648376 | 0.836411943319838\n",
      "> 33 | 0.3299213647842407 | 0.8365068319838057\n",
      "> 34 | 0.3297862410545349 | 0.8365068319838057\n",
      "> 35 | 0.32965728640556335 | 0.8366333502024291\n",
      "> 36 | 0.32953405380249023 | 0.8367598684210527\n",
      "> 37 | 0.3294161558151245 | 0.836918016194332\n",
      "> 38 | 0.32930320501327515 | 0.8369812753036437\n",
      "> 39 | 0.3291948139667511 | 0.8371394230769231\n",
      "> 40 | 0.3290907144546509 | 0.8372343117408907\n",
      "> 41 | 0.328990638256073 | 0.8372343117408907\n",
      "> 42 | 0.3288942575454712 | 0.8373924595141701\n",
      "> 43 | 0.32880139350891113 | 0.8374557186234818\n",
      "> 44 | 0.32871195673942566 | 0.8375189777327935\n",
      "> 45 | 0.3286256194114685 | 0.8374873481781376\n",
      "> 46 | 0.32854217290878296 | 0.8375822368421053\n",
      "> 47 | 0.32846152782440186 | 0.8375822368421053\n",
      "> 48 | 0.3283834755420685 | 0.8378036437246964\n",
      "> 49 | 0.32830798625946045 | 0.837898532388664\n",
      "> 50 | 0.32823482155799866 | 0.8380250506072875\n",
      "> 51 | 0.32816389203071594 | 0.8380883097165992\n",
      "> 52 | 0.328095018863678 | 0.8380883097165992\n",
      "> 53 | 0.32802814245224 | 0.8381831983805668\n",
      "> 54 | 0.327963262796402 | 0.8381515688259109\n",
      "> 55 | 0.3279001712799072 | 0.8383097165991903\n",
      "> 56 | 0.32783886790275574 | 0.8383413461538461\n",
      "> 57 | 0.3277791738510132 | 0.8384046052631579\n",
      "> 58 | 0.3277210593223572 | 0.8384678643724697\n",
      "> 59 | 0.32766449451446533 | 0.8384678643724697\n",
      "> 60 | 0.32760941982269287 | 0.8385627530364372\n",
      "> 61 | 0.32755568623542786 | 0.8385627530364372\n",
      "> 62 | 0.3275032043457031 | 0.8385627530364372\n",
      "> 63 | 0.3274520933628082 | 0.8385627530364372\n",
      "> 64 | 0.3274022042751312 | 0.8385943825910931\n",
      "> 65 | 0.3273535370826721 | 0.8386576417004049\n",
      "> 66 | 0.3273058831691742 | 0.8386892712550608\n",
      "> 67 | 0.3272594213485718 | 0.8386576417004049\n",
      "> 68 | 0.32721397280693054 | 0.838626012145749\n",
      "> 69 | 0.3271695077419281 | 0.838626012145749\n",
      "> 70 | 0.3271259665489197 | 0.8386892712550608\n",
      "> 71 | 0.32708337903022766 | 0.8387525303643725\n",
      "> 72 | 0.32704171538352966 | 0.8388157894736842\n",
      "> 73 | 0.3270009458065033 | 0.8389106781376519\n",
      "> 74 | 0.3269610106945038 | 0.8389423076923077\n",
      "> 75 | 0.32692188024520874 | 0.8390055668016194\n",
      "> 76 | 0.32688355445861816 | 0.8390371963562753\n",
      "> 77 | 0.3268459439277649 | 0.8390688259109311\n",
      "> 78 | 0.32680898904800415 | 0.8391004554655871\n",
      "> 79 | 0.3267728388309479 | 0.8391637145748988\n",
      "> 80 | 0.32673734426498413 | 0.8391637145748988\n",
      "> 81 | 0.32670244574546814 | 0.8391637145748988\n",
      "> 82 | 0.32666826248168945 | 0.839132085020243\n",
      "> 83 | 0.3266347050666809 | 0.8391637145748988\n",
      "> 84 | 0.3266017436981201 | 0.8391953441295547\n",
      "> 85 | 0.3265693783760071 | 0.8391953441295547\n",
      "> 86 | 0.3265376389026642 | 0.8391953441295547\n",
      "> 87 | 0.3265063464641571 | 0.8392269736842105\n",
      "> 88 | 0.3264756202697754 | 0.8392586032388664\n",
      "> 89 | 0.32644540071487427 | 0.8393218623481782\n",
      "> 90 | 0.32641565799713135 | 0.8394800101214575\n",
      "> 91 | 0.32638639211654663 | 0.8394800101214575\n",
      "> 92 | 0.3263576626777649 | 0.8394167510121457\n",
      "> 93 | 0.32632938027381897 | 0.8394483805668016\n",
      "> 94 | 0.32630160450935364 | 0.8394167510121457\n",
      "> 95 | 0.32627415657043457 | 0.8394800101214575\n",
      "> 96 | 0.3262472152709961 | 0.8394483805668016\n",
      "> 97 | 0.32622063159942627 | 0.8394167510121457\n",
      "> 98 | 0.3261944651603699 | 0.8395116396761133\n",
      "> 99 | 0.3261687159538269 | 0.8395748987854251\n",
      "> 100 | 0.3261433243751526 | 0.8396381578947368\n",
      "> Evaluation\n",
      "> Class Acc = 0.8336048722267151\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.804183229804039 | 0.8688927255570889 | 0.824304848909378\n",
      "> Confusion Matrix \n",
      "TN: 9371.0 | FP: 721.0 \n",
      "FN: 1526.0 | TP: 1886.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3799.0 | FP: 70.0 \n",
      "FN: 286.0 | TP: 192.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 5572.0 | FP: 651.0 \n",
      "FN: 1240.0 | TP: 1694.0\n"
     ]
    }
   ],
   "source": [
    "for cv_seed in cv_seeds:\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        x, y, a, test_size=0.3, random_state=cv_seed)\n",
    "\n",
    "    train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "    train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # train below\n",
    "\n",
    "    model = UnfairLogisticRegression(xdim, ydim, batch_size)\n",
    "    ret = train_loop(model, train_data, epochs)\n",
    "    Y, A, Y_hat = evaluation(model, test_data)\n",
    "    \n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1  = compute_metrics(Y, A, Y_hat, adim=adim)\n",
    "    \n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    \n",
    "    tradeoff = []\n",
    "    \n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "    \n",
    "    result = ['UnfairLR-decay', cv_seed, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6e10c6b430>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.44726601243019104 | 0.6350581983805668\n",
      "> 2 | 0.38056838512420654 | 0.8073127530364372\n",
      "> 3 | 0.35443824529647827 | 0.8240131578947368\n",
      "> 4 | 0.34044915437698364 | 0.8300227732793523\n",
      "> 5 | 0.33180204033851624 | 0.8318256578947368\n",
      "> 6 | 0.32612842321395874 | 0.835210020242915\n",
      "> 7 | 0.32231807708740234 | 0.837171052631579\n",
      "> 8 | 0.3197230100631714 | 0.8386892712550608\n",
      "> 9 | 0.3179227113723755 | 0.8395748987854251\n",
      "> 10 | 0.31663697957992554 | 0.840808451417004\n",
      "> 11 | 0.3156821131706238 | 0.8412196356275303\n",
      "> 12 | 0.3149406909942627 | 0.8419154858299596\n",
      "> 13 | 0.3143402338027954 | 0.8422317813765182\n",
      "> 14 | 0.3138370215892792 | 0.8426429655870445\n",
      "> 15 | 0.31340479850769043 | 0.8435285931174089\n",
      "> 16 | 0.31302762031555176 | 0.8436234817813765\n",
      "> 17 | 0.3126956522464752 | 0.8442877024291497\n",
      "> 18 | 0.3124023973941803 | 0.8446672570850202\n",
      "> 19 | 0.3121435046195984 | 0.8448886639676113\n",
      "> 20 | 0.31191587448120117 | 0.8450784412955465\n",
      "> 21 | 0.3117170035839081 | 0.8453314777327935\n",
      "> 22 | 0.3115447759628296 | 0.8455528846153846\n",
      "> 23 | 0.3113974928855896 | 0.8458691801619433\n",
      "> 24 | 0.31127336621284485 | 0.8462171052631579\n",
      "> 25 | 0.31117063760757446 | 0.8462803643724697\n",
      "> 26 | 0.31108757853507996 | 0.8465017712550608\n",
      "> 27 | 0.31102269887924194 | 0.8468496963562753\n",
      "> 28 | 0.3109744191169739 | 0.846944585020243\n",
      "> 29 | 0.31094130873680115 | 0.847165991902834\n",
      "> 30 | 0.31092196702957153 | 0.8471027327935222\n",
      "> 31 | 0.3109152317047119 | 0.8471343623481782\n",
      "> 32 | 0.3109200596809387 | 0.8471027327935222\n",
      "> 33 | 0.31093549728393555 | 0.8473241396761133\n",
      "> 34 | 0.310960590839386 | 0.8475139170040485\n",
      "> 35 | 0.31099462509155273 | 0.8477353238866396\n",
      "> 36 | 0.31103694438934326 | 0.8479567307692307\n",
      "> 37 | 0.31108683347702026 | 0.8479883603238867\n",
      "> 38 | 0.311143696308136 | 0.8483362854251012\n",
      "> 39 | 0.3112070560455322 | 0.8486842105263158\n",
      "> 40 | 0.3112764358520508 | 0.8490321356275303\n",
      "> 41 | 0.311351478099823 | 0.8490321356275303\n",
      "> 42 | 0.31143173575401306 | 0.8490953947368421\n",
      "> 43 | 0.31151679158210754 | 0.8492219129554656\n",
      "> 44 | 0.31160640716552734 | 0.8493484311740891\n",
      "> 45 | 0.3117001950740814 | 0.849601467611336\n",
      "> 46 | 0.31179797649383545 | 0.8498861336032388\n",
      "> 47 | 0.3118993937969208 | 0.8499810222672065\n",
      "> 48 | 0.3120042681694031 | 0.8502656882591093\n",
      "> 49 | 0.31211236119270325 | 0.8505503542510121\n",
      "> 50 | 0.31222349405288696 | 0.8505187246963563\n",
      "> 51 | 0.31233733892440796 | 0.8506768724696356\n",
      "> 52 | 0.3124538064002991 | 0.8509299089068826\n",
      "> 53 | 0.3125726580619812 | 0.8509299089068826\n",
      "> 54 | 0.31269383430480957 | 0.8511196862348178\n",
      "> 55 | 0.3128170669078827 | 0.851088056680162\n",
      "> 56 | 0.312942236661911 | 0.8512145748987854\n",
      "> 57 | 0.31306925415992737 | 0.8514359817813765\n",
      "> 58 | 0.31319791078567505 | 0.851309463562753\n",
      "> 59 | 0.31332796812057495 | 0.8513727226720648\n",
      "> 60 | 0.31345945596694946 | 0.8514359817813765\n",
      "> 61 | 0.31359225511550903 | 0.8515308704453441\n",
      "> 62 | 0.313726007938385 | 0.8516573886639676\n",
      "> 63 | 0.31386077404022217 | 0.8516257591093117\n",
      "> 64 | 0.31399643421173096 | 0.8515308704453441\n",
      "> 65 | 0.3141328692436218 | 0.8516257591093117\n",
      "> 66 | 0.31426993012428284 | 0.8514992408906883\n",
      "> 67 | 0.31440746784210205 | 0.8514992408906883\n",
      "> 68 | 0.31454533338546753 | 0.8516573886639676\n",
      "> 69 | 0.31468355655670166 | 0.851815536437247\n",
      "> 70 | 0.31482189893722534 | 0.851815536437247\n",
      "> 71 | 0.3149603605270386 | 0.851815536437247\n",
      "> 72 | 0.31509867310523987 | 0.8518787955465587\n",
      "> 73 | 0.3152369260787964 | 0.8520053137651822\n",
      "> 74 | 0.3153749704360962 | 0.8520053137651822\n",
      "> 75 | 0.31551259756088257 | 0.8519736842105263\n",
      "> 76 | 0.3156498372554779 | 0.852036943319838\n",
      "> 77 | 0.31578654050827026 | 0.8519104251012146\n",
      "> 78 | 0.31592267751693726 | 0.8521002024291497\n",
      "> 79 | 0.31605809926986694 | 0.8522267206477733\n",
      "> 80 | 0.31619277596473694 | 0.8522583502024291\n",
      "> 81 | 0.31632667779922485 | 0.8521950910931174\n",
      "> 82 | 0.316459596157074 | 0.8521950910931174\n",
      "> 83 | 0.3165915608406067 | 0.8521002024291497\n",
      "> 84 | 0.316722571849823 | 0.8521002024291497\n",
      "> 85 | 0.31685250997543335 | 0.8521634615384616\n",
      "> 86 | 0.3169812560081482 | 0.8522583502024291\n",
      "> 87 | 0.3171088397502899 | 0.852289979757085\n",
      "> 88 | 0.31723517179489136 | 0.8521634615384616\n",
      "> 89 | 0.31736019253730774 | 0.8522267206477733\n",
      "> 90 | 0.3174840211868286 | 0.8523216093117408\n",
      "> 91 | 0.31760644912719727 | 0.8525113866396761\n",
      "> 92 | 0.31772738695144653 | 0.8525746457489879\n",
      "> 93 | 0.31784701347351074 | 0.8526062753036437\n",
      "> 94 | 0.31796520948410034 | 0.8527327935222672\n",
      "> 95 | 0.31808197498321533 | 0.8527644230769231\n",
      "> 96 | 0.318197101354599 | 0.8526379048582996\n",
      "> 97 | 0.31831085681915283 | 0.8527011639676113\n",
      "> 98 | 0.3184230923652649 | 0.8527011639676113\n",
      "> 99 | 0.31853383779525757 | 0.8527011639676113\n",
      "> 100 | 0.3186430335044861 | 0.8527011639676113\n",
      "> Evaluation\n",
      "> Class Acc = 0.8491557836532593\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8149847239255905 | 0.9029470458626747 | 0.8850136399269104\n",
      "> Confusion Matrix \n",
      "TN: 9432.0 | FP: 726.0 \n",
      "FN: 1311.0 | TP: 2035.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3782.0 | FP: 87.0 \n",
      "FN: 250.0 | TP: 261.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 5650.0 | FP: 639.0 \n",
      "FN: 1061.0 | TP: 1774.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.4965749979019165 | 0.6365764170040485\n",
      "> 2 | 0.4453296661376953 | 0.8080086032388664\n",
      "> 3 | 0.43372130393981934 | 0.8252783400809717\n",
      "> 4 | 0.4295806586742401 | 0.8305921052631579\n",
      "> 5 | 0.4264655113220215 | 0.832742914979757\n",
      "> 6 | 0.4232688546180725 | 0.8362537955465587\n",
      "> 7 | 0.41998887062072754 | 0.8374240890688259\n",
      "> 8 | 0.41677623987197876 | 0.839132085020243\n",
      "> 9 | 0.4137395918369293 | 0.8396697874493927\n",
      "> 10 | 0.4109318256378174 | 0.8405237854251012\n",
      "> 11 | 0.40836846828460693 | 0.8407451923076923\n",
      "> 12 | 0.4060443937778473 | 0.8412512651821862\n",
      "> 13 | 0.40394335985183716 | 0.8418205971659919\n",
      "> 14 | 0.4020448625087738 | 0.8421052631578947\n",
      "> 15 | 0.400327205657959 | 0.8423582995951417\n",
      "> 16 | 0.39876970648765564 | 0.8429592611336032\n",
      "> 17 | 0.39735352993011475 | 0.8436551113360324\n",
      "> 18 | 0.3960617482662201 | 0.8441928137651822\n",
      "> 19 | 0.39487943053245544 | 0.8443509615384616\n",
      "> 20 | 0.3937934935092926 | 0.8447937753036437\n",
      "> 21 | 0.39279231429100037 | 0.8449202935222672\n",
      "> 22 | 0.39186573028564453 | 0.8451417004048583\n",
      "> 23 | 0.3910049796104431 | 0.8452998481781376\n",
      "> 24 | 0.3902023434638977 | 0.8453314777327935\n",
      "> 25 | 0.38945117592811584 | 0.8455528846153846\n",
      "> 26 | 0.3887459635734558 | 0.8458691801619433\n",
      "> 27 | 0.38808196783065796 | 0.8462171052631579\n",
      "> 28 | 0.38745516538619995 | 0.8463436234817814\n",
      "> 29 | 0.38686227798461914 | 0.846691548582996\n",
      "> 30 | 0.3863002359867096 | 0.8469762145748988\n",
      "> 31 | 0.3857664465904236 | 0.8470711032388664\n",
      "> 32 | 0.38525888323783875 | 0.8472608805668016\n",
      "> 33 | 0.3847754895687103 | 0.8473873987854251\n",
      "> 34 | 0.3843145966529846 | 0.8474822874493927\n",
      "> 35 | 0.3838745951652527 | 0.8476404352226721\n",
      "> 36 | 0.3834541141986847 | 0.847893471659919\n",
      "> 37 | 0.3830517530441284 | 0.8481148785425101\n",
      "> 38 | 0.38266658782958984 | 0.848367914979757\n",
      "> 39 | 0.38229745626449585 | 0.84865258097166\n",
      "> 40 | 0.3819435238838196 | 0.8484628036437247\n",
      "> 41 | 0.38160380721092224 | 0.8487790991902834\n",
      "> 42 | 0.38127753138542175 | 0.849127024291498\n",
      "> 43 | 0.3809638023376465 | 0.849380060728745\n",
      "> 44 | 0.380662202835083 | 0.8495382085020243\n",
      "> 45 | 0.3803718686103821 | 0.8495698380566802\n",
      "> 46 | 0.38009244203567505 | 0.8496330971659919\n",
      "> 47 | 0.3798230290412903 | 0.8497279858299596\n",
      "> 48 | 0.379563570022583 | 0.8497596153846154\n",
      "> 49 | 0.37931323051452637 | 0.8499493927125507\n",
      "> 50 | 0.37907159328460693 | 0.8501707995951417\n",
      "> 51 | 0.3788384199142456 | 0.8500759109311741\n",
      "> 52 | 0.37861335277557373 | 0.8500442813765182\n",
      "> 53 | 0.37839579582214355 | 0.8500759109311741\n",
      "> 54 | 0.37818557024002075 | 0.8502973178137652\n",
      "> 55 | 0.3779822587966919 | 0.8503922064777328\n",
      "> 56 | 0.37778574228286743 | 0.8504238360323887\n",
      "> 57 | 0.3775954842567444 | 0.8502973178137652\n",
      "> 58 | 0.3774113059043884 | 0.8503605769230769\n",
      "> 59 | 0.37723299860954285 | 0.8505187246963563\n",
      "> 60 | 0.37706026434898376 | 0.8505503542510121\n",
      "> 61 | 0.37689295411109924 | 0.8506136133603239\n",
      "> 62 | 0.3767307698726654 | 0.850835020242915\n",
      "> 63 | 0.3765735626220703 | 0.8510564271255061\n",
      "> 64 | 0.37642109394073486 | 0.8509931680161943\n",
      "> 65 | 0.3762732744216919 | 0.8512778340080972\n",
      "> 66 | 0.376129686832428 | 0.8513727226720648\n",
      "> 67 | 0.3759903907775879 | 0.8514676113360324\n",
      "> 68 | 0.37585514783859253 | 0.8515625\n",
      "> 69 | 0.37572383880615234 | 0.8517839068825911\n",
      "> 70 | 0.3755962550640106 | 0.8516573886639676\n",
      "> 71 | 0.3754723072052002 | 0.8517522773279352\n",
      "> 72 | 0.3753519058227539 | 0.8519420546558705\n",
      "> 73 | 0.37523484230041504 | 0.8519104251012146\n",
      "> 74 | 0.3751210570335388 | 0.8518471659919028\n",
      "> 75 | 0.3750104010105133 | 0.851815536437247\n",
      "> 76 | 0.3749026656150818 | 0.8516573886639676\n",
      "> 77 | 0.37479791045188904 | 0.8516573886639676\n",
      "> 78 | 0.3746960163116455 | 0.8516257591093117\n",
      "> 79 | 0.3745967149734497 | 0.8516890182186235\n",
      "> 80 | 0.37450018525123596 | 0.8517206477732794\n",
      "> 81 | 0.3744060695171356 | 0.8519104251012146\n",
      "> 82 | 0.374314546585083 | 0.8519420546558705\n",
      "> 83 | 0.37422531843185425 | 0.852036943319838\n",
      "> 84 | 0.3741384744644165 | 0.852036943319838\n",
      "> 85 | 0.3740537464618683 | 0.8521950910931174\n",
      "> 86 | 0.37397125363349915 | 0.8523216093117408\n",
      "> 87 | 0.37389087677001953 | 0.8523848684210527\n",
      "> 88 | 0.3738124966621399 | 0.8524481275303644\n",
      "> 89 | 0.37373608350753784 | 0.8524797570850202\n",
      "> 90 | 0.3736615478992462 | 0.8524481275303644\n",
      "> 91 | 0.3735889792442322 | 0.8524164979757085\n",
      "> 92 | 0.37351807951927185 | 0.8524797570850202\n",
      "> 93 | 0.37344890832901 | 0.8525113866396761\n",
      "> 94 | 0.37338152527809143 | 0.8523532388663968\n",
      "> 95 | 0.373315691947937 | 0.852289979757085\n",
      "> 96 | 0.3732515573501587 | 0.8523532388663968\n",
      "> 97 | 0.373188853263855 | 0.852289979757085\n",
      "> 98 | 0.37312763929367065 | 0.8522583502024291\n",
      "> 99 | 0.37306785583496094 | 0.8522583502024291\n",
      "> 100 | 0.3730096220970154 | 0.8522267206477733\n",
      "> Evaluation\n",
      "> Class Acc = 0.8492298722267151\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8036512285470963 | 0.889695905148983 | 0.8658866286277771\n",
      "> Confusion Matrix \n",
      "TN: 9393.0 | FP: 763.0 \n",
      "FN: 1273.0 | TP: 2075.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3794.0 | FP: 84.0 \n",
      "FN: 250.0 | TP: 256.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 5599.0 | FP: 679.0 \n",
      "FN: 1023.0 | TP: 1819.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5018805265426636 | 0.6341093117408907\n",
      "> 2 | 0.42689305543899536 | 0.8065536437246964\n",
      "> 3 | 0.3989161252975464 | 0.8262904858299596\n",
      "> 4 | 0.3854158818721771 | 0.8311298076923077\n",
      "> 5 | 0.3774683475494385 | 0.8335652834008097\n",
      "> 6 | 0.3721945881843567 | 0.8348304655870445\n",
      "> 7 | 0.3684536814689636 | 0.8362221659919028\n",
      "> 8 | 0.3657018542289734 | 0.8379934210526315\n",
      "> 9 | 0.363627552986145 | 0.838626012145749\n",
      "> 10 | 0.36202332377433777 | 0.8392902327935222\n",
      "> 11 | 0.36073946952819824 | 0.8399544534412956\n",
      "> 12 | 0.3596671223640442 | 0.841061487854251\n",
      "> 13 | 0.35872942209243774 | 0.841314524291498\n",
      "> 14 | 0.35787433385849 | 0.8418205971659919\n",
      "> 15 | 0.35706841945648193 | 0.8421685222672065\n",
      "> 16 | 0.35629117488861084 | 0.8429276315789473\n",
      "> 17 | 0.35553061962127686 | 0.8429592611336032\n",
      "> 18 | 0.35478079319000244 | 0.842516447368421\n",
      "> 19 | 0.3540388345718384 | 0.8424848178137652\n",
      "> 20 | 0.3533046543598175 | 0.8427062246963563\n",
      "> 21 | 0.35257917642593384 | 0.8429908906882592\n",
      "> 22 | 0.35186418890953064 | 0.8430541497975709\n",
      "> 23 | 0.35116156935691833 | 0.8433704453441295\n",
      "> 24 | 0.3504730463027954 | 0.8435918522267206\n",
      "> 25 | 0.3498002290725708 | 0.8438765182186235\n",
      "> 26 | 0.3491441011428833 | 0.8440979251012146\n",
      "> 27 | 0.34850552678108215 | 0.8444142206477733\n",
      "> 28 | 0.3478848338127136 | 0.8446039979757085\n",
      "> 29 | 0.3472822308540344 | 0.844730516194332\n",
      "> 30 | 0.3466976284980774 | 0.8449519230769231\n",
      "> 31 | 0.34613072872161865 | 0.8448886639676113\n",
      "> 32 | 0.3455812931060791 | 0.8452365890688259\n",
      "> 33 | 0.3450489342212677 | 0.845457995951417\n",
      "> 34 | 0.34453320503234863 | 0.8456161437246964\n",
      "> 35 | 0.3440335988998413 | 0.845711032388664\n",
      "> 36 | 0.3435497581958771 | 0.8458691801619433\n",
      "> 37 | 0.34308117628097534 | 0.8458375506072875\n",
      "> 38 | 0.3426273465156555 | 0.8459008097165992\n",
      "> 39 | 0.34218794107437134 | 0.8461222165991903\n",
      "> 40 | 0.34176236391067505 | 0.8462487348178138\n",
      "> 41 | 0.3413503170013428 | 0.8465334008097166\n",
      "> 42 | 0.34095144271850586 | 0.8465966599190283\n",
      "> 43 | 0.34056511521339417 | 0.84665991902834\n",
      "> 44 | 0.3401910066604614 | 0.84665991902834\n",
      "> 45 | 0.3398287892341614 | 0.8467548076923077\n",
      "> 46 | 0.3394780158996582 | 0.8469762145748988\n",
      "> 47 | 0.3391382694244385 | 0.8473241396761133\n",
      "> 48 | 0.3388093113899231 | 0.8476404352226721\n",
      "> 49 | 0.3384905755519867 | 0.8477353238866396\n",
      "> 50 | 0.33818185329437256 | 0.847893471659919\n",
      "> 51 | 0.3378828167915344 | 0.847893471659919\n",
      "> 52 | 0.33759307861328125 | 0.8479251012145749\n",
      "> 53 | 0.33731234073638916 | 0.8479251012145749\n",
      "> 54 | 0.33704033493995667 | 0.8481148785425101\n",
      "> 55 | 0.33677661418914795 | 0.848146508097166\n",
      "> 56 | 0.3365210294723511 | 0.8484311740890689\n",
      "> 57 | 0.3362733721733093 | 0.8484311740890689\n",
      "> 58 | 0.33603328466415405 | 0.8485260627530364\n",
      "> 59 | 0.335800439119339 | 0.8487158400809717\n",
      "> 60 | 0.335574746131897 | 0.8485576923076923\n",
      "> 61 | 0.3353557884693146 | 0.8487158400809717\n",
      "> 62 | 0.33514344692230225 | 0.8487790991902834\n",
      "> 63 | 0.33493757247924805 | 0.8487790991902834\n",
      "> 64 | 0.33473777770996094 | 0.8487474696356275\n",
      "> 65 | 0.334543913602829 | 0.8488107287449392\n",
      "> 66 | 0.3343558609485626 | 0.84865258097166\n",
      "> 67 | 0.3341732621192932 | 0.8487158400809717\n",
      "> 68 | 0.33399614691734314 | 0.8488107287449392\n",
      "> 69 | 0.3338242173194885 | 0.8490005060728745\n",
      "> 70 | 0.33365732431411743 | 0.8490637651821862\n",
      "> 71 | 0.3334953188896179 | 0.8490637651821862\n",
      "> 72 | 0.33333805203437805 | 0.8490321356275303\n",
      "> 73 | 0.33318525552749634 | 0.8489688765182186\n",
      "> 74 | 0.3330369293689728 | 0.8489688765182186\n",
      "> 75 | 0.33289289474487305 | 0.8489688765182186\n",
      "> 76 | 0.33275306224823 | 0.8490005060728745\n",
      "> 77 | 0.3326171636581421 | 0.849127024291498\n",
      "> 78 | 0.3324851393699646 | 0.8490637651821862\n",
      "> 79 | 0.33235687017440796 | 0.8490953947368421\n",
      "> 80 | 0.3322322964668274 | 0.849380060728745\n",
      "> 81 | 0.3321112394332886 | 0.8493484311740891\n",
      "> 82 | 0.3319935202598572 | 0.8493168016194332\n",
      "> 83 | 0.3318791389465332 | 0.8493168016194332\n",
      "> 84 | 0.3317680358886719 | 0.8494749493927125\n",
      "> 85 | 0.3316599726676941 | 0.8495698380566802\n",
      "> 86 | 0.33155497908592224 | 0.8495382085020243\n",
      "> 87 | 0.331452876329422 | 0.8496330971659919\n",
      "> 88 | 0.33135366439819336 | 0.8496963562753036\n",
      "> 89 | 0.331257164478302 | 0.8496963562753036\n",
      "> 90 | 0.3311632573604584 | 0.8496963562753036\n",
      "> 91 | 0.33107203245162964 | 0.8496330971659919\n",
      "> 92 | 0.3309832215309143 | 0.8496647267206477\n",
      "> 93 | 0.33089688420295715 | 0.849601467611336\n",
      "> 94 | 0.33081287145614624 | 0.8497279858299596\n",
      "> 95 | 0.3307311534881592 | 0.849601467611336\n",
      "> 96 | 0.3306517004966736 | 0.8497279858299596\n",
      "> 97 | 0.33057430386543274 | 0.8497596153846154\n",
      "> 98 | 0.33049899339675903 | 0.8497279858299596\n",
      "> 99 | 0.3304256498813629 | 0.8497279858299596\n",
      "> 100 | 0.33035439252853394 | 0.8497596153846154\n",
      "> Evaluation\n",
      "> Class Acc = 0.8544135093688965\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8120026141405106 | 0.9132812172174454 | 0.9027506113052368\n",
      "> Confusion Matrix \n",
      "TN: 9435.0 | FP: 709.0 \n",
      "FN: 1257.0 | TP: 2103.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3834.0 | FP: 91.0 \n",
      "FN: 228.0 | TP: 271.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 5601.0 | FP: 618.0 \n",
      "FN: 1029.0 | TP: 1832.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.37400007247924805 | 0.6368927125506073\n",
      "> 2 | 0.2947944402694702 | 0.8090523785425101\n",
      "> 3 | 0.26671046018600464 | 0.8273026315789473\n",
      "> 4 | 0.25431638956069946 | 0.831794028340081\n",
      "> 5 | 0.2485128492116928 | 0.834229504048583\n",
      "> 6 | 0.24581816792488098 | 0.8365068319838057\n",
      "> 7 | 0.24465537071228027 | 0.8379301619433198\n",
      "> 8 | 0.2442525029182434 | 0.838879048582996\n",
      "> 9 | 0.244219109416008 | 0.840334008097166\n",
      "> 10 | 0.24435299634933472 | 0.8404288967611336\n",
      "> 11 | 0.24454721808433533 | 0.8412512651821862\n",
      "> 12 | 0.24474437534809113 | 0.8414726720647774\n",
      "> 13 | 0.24491409957408905 | 0.8419154858299596\n",
      "> 14 | 0.24504168331623077 | 0.8423266700404858\n",
      "> 15 | 0.2451213151216507 | 0.842516447368421\n",
      "> 16 | 0.24515222012996674 | 0.8429276315789473\n",
      "> 17 | 0.24513667821884155 | 0.8435602226720648\n",
      "> 18 | 0.24507826566696167 | 0.8440346659919028\n",
      "> 19 | 0.2449813187122345 | 0.8441295546558705\n",
      "> 20 | 0.24485087394714355 | 0.8445723684210527\n",
      "> 21 | 0.24469171464443207 | 0.8450784412955465\n",
      "> 22 | 0.24450847506523132 | 0.8457742914979757\n",
      "> 23 | 0.24430546164512634 | 0.8460905870445344\n",
      "> 24 | 0.24408647418022156 | 0.846438512145749\n",
      "> 25 | 0.24385488033294678 | 0.8465334008097166\n",
      "> 26 | 0.24361340701580048 | 0.846691548582996\n",
      "> 27 | 0.24336466193199158 | 0.8471343623481782\n",
      "> 28 | 0.2431105077266693 | 0.8471976214574899\n",
      "> 29 | 0.24285274744033813 | 0.8473557692307693\n",
      "> 30 | 0.24259278178215027 | 0.8473557692307693\n",
      "> 31 | 0.24233195185661316 | 0.8471976214574899\n",
      "> 32 | 0.2420712560415268 | 0.8472925101214575\n",
      "> 33 | 0.2418116331100464 | 0.8472292510121457\n",
      "> 34 | 0.24155393242835999 | 0.8475771761133604\n",
      "> 35 | 0.24129879474639893 | 0.8474506578947368\n",
      "> 36 | 0.241046741604805 | 0.8476088056680162\n",
      "> 37 | 0.24079826474189758 | 0.8478302125506073\n",
      "> 38 | 0.24055375158786774 | 0.8479567307692307\n",
      "> 39 | 0.2403135895729065 | 0.8483362854251012\n",
      "> 40 | 0.2400779128074646 | 0.8484944331983806\n",
      "> 41 | 0.2398470640182495 | 0.84865258097166\n",
      "> 42 | 0.23962119221687317 | 0.8489056174089069\n",
      "> 43 | 0.239400252699852 | 0.8489688765182186\n",
      "> 44 | 0.23918449878692627 | 0.8491586538461539\n",
      "> 45 | 0.23897385597229004 | 0.8493484311740891\n",
      "> 46 | 0.23876835405826569 | 0.8494433198380567\n",
      "> 47 | 0.23856794834136963 | 0.8497912449392713\n",
      "> 48 | 0.2383725941181183 | 0.8497912449392713\n",
      "> 49 | 0.23818226158618927 | 0.8498861336032388\n",
      "> 50 | 0.2379968762397766 | 0.8501391700404858\n",
      "> 51 | 0.23781636357307434 | 0.8502024291497976\n",
      "> 52 | 0.2376406043767929 | 0.8501075404858299\n",
      "> 53 | 0.23746953904628754 | 0.850328947368421\n",
      "> 54 | 0.23730307817459106 | 0.8507085020242915\n",
      "> 55 | 0.23714101314544678 | 0.850835020242915\n",
      "> 56 | 0.23698335886001587 | 0.8508033906882592\n",
      "> 57 | 0.23682990670204163 | 0.8508033906882592\n",
      "> 58 | 0.23668065667152405 | 0.8507401315789473\n",
      "> 59 | 0.23653538525104523 | 0.8508666497975709\n",
      "> 60 | 0.2363939881324768 | 0.8509931680161943\n",
      "> 61 | 0.23625637590885162 | 0.8510247975708503\n",
      "> 62 | 0.2361224889755249 | 0.8512145748987854\n",
      "> 63 | 0.2359922081232071 | 0.8513727226720648\n",
      "> 64 | 0.23586538434028625 | 0.8515308704453441\n",
      "> 65 | 0.23574185371398926 | 0.8515941295546559\n",
      "> 66 | 0.2356216013431549 | 0.8514992408906883\n",
      "> 67 | 0.23550458252429962 | 0.8512778340080972\n",
      "> 68 | 0.23539061844348907 | 0.851309463562753\n",
      "> 69 | 0.2352796345949173 | 0.8513727226720648\n",
      "> 70 | 0.23517157137393951 | 0.8516257591093117\n",
      "> 71 | 0.2350662648677826 | 0.8516890182186235\n",
      "> 72 | 0.23496368527412415 | 0.8516890182186235\n",
      "> 73 | 0.23486380279064178 | 0.8516890182186235\n",
      "> 74 | 0.23476646840572357 | 0.8516257591093117\n",
      "> 75 | 0.23467163741588593 | 0.8516890182186235\n",
      "> 76 | 0.2345791906118393 | 0.8517839068825911\n",
      "> 77 | 0.23448915779590607 | 0.8516573886639676\n",
      "> 78 | 0.2344013750553131 | 0.8516573886639676\n",
      "> 79 | 0.2343158721923828 | 0.8516890182186235\n",
      "> 80 | 0.23423245549201965 | 0.8515941295546559\n",
      "> 81 | 0.23415115475654602 | 0.8516257591093117\n",
      "> 82 | 0.23407192528247833 | 0.8516573886639676\n",
      "> 83 | 0.23399463295936584 | 0.8516573886639676\n",
      "> 84 | 0.23391926288604736 | 0.8515941295546559\n",
      "> 85 | 0.23384572565555573 | 0.8515308704453441\n",
      "> 86 | 0.23377405107021332 | 0.8515625\n",
      "> 87 | 0.2337040901184082 | 0.8514676113360324\n",
      "> 88 | 0.23363590240478516 | 0.8514359817813765\n",
      "> 89 | 0.23356935381889343 | 0.8513410931174089\n",
      "> 90 | 0.23350436985492706 | 0.8512778340080972\n",
      "> 91 | 0.23344102501869202 | 0.8512778340080972\n",
      "> 92 | 0.23337915539741516 | 0.8513410931174089\n",
      "> 93 | 0.2333187758922577 | 0.8515308704453441\n",
      "> 94 | 0.23325976729393005 | 0.8515625\n",
      "> 95 | 0.233202263712883 | 0.8515625\n",
      "> 96 | 0.2331460416316986 | 0.8515308704453441\n",
      "> 97 | 0.23309125006198883 | 0.8515941295546559\n",
      "> 98 | 0.23303769528865814 | 0.8517206477732794\n",
      "> 99 | 0.2329854518175125 | 0.8516890182186235\n",
      "> 100 | 0.23293429613113403 | 0.851815536437247\n",
      "> Evaluation\n",
      "> Class Acc = 0.8504887223243713\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8150623738765717 | 0.9031478129327297 | 0.8783439099788666\n",
      "> Confusion Matrix \n",
      "TN: 9412.0 | FP: 698.0 \n",
      "FN: 1321.0 | TP: 2073.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3755.0 | FP: 94.0 \n",
      "FN: 246.0 | TP: 253.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 5657.0 | FP: 604.0 \n",
      "FN: 1075.0 | TP: 1820.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.4469220042228699 | 0.6324962044534413\n",
      "> 2 | 0.3745906949043274 | 0.808103491902834\n",
      "> 3 | 0.3482304513454437 | 0.8270179655870445\n",
      "> 4 | 0.3357279300689697 | 0.8320154352226721\n",
      "> 5 | 0.32886409759521484 | 0.8350518724696356\n",
      "> 6 | 0.3248271048069 | 0.8371394230769231\n",
      "> 7 | 0.3224026560783386 | 0.8390055668016194\n",
      "> 8 | 0.32094961404800415 | 0.8406186740890689\n",
      "> 9 | 0.3200809955596924 | 0.841567560728745\n",
      "> 10 | 0.31955114006996155 | 0.842042004048583\n",
      "> 11 | 0.31920623779296875 | 0.8430857793522267\n",
      "> 12 | 0.3189542293548584 | 0.8434020748987854\n",
      "> 13 | 0.31874358654022217 | 0.8441611842105263\n",
      "> 14 | 0.3185473084449768 | 0.8446356275303644\n",
      "> 15 | 0.3183523714542389 | 0.8452049595141701\n",
      "> 16 | 0.31815338134765625 | 0.8452682186234818\n",
      "> 17 | 0.31794866919517517 | 0.8455845141700404\n",
      "> 18 | 0.3177383840084076 | 0.846185475708502\n",
      "> 19 | 0.31752365827560425 | 0.8468813259109311\n",
      "> 20 | 0.3173057734966278 | 0.8473873987854251\n",
      "> 21 | 0.31708621978759766 | 0.8477669534412956\n",
      "> 22 | 0.31686636805534363 | 0.8480832489878543\n",
      "> 23 | 0.3166477084159851 | 0.8484311740890689\n",
      "> 24 | 0.31643128395080566 | 0.84865258097166\n",
      "> 25 | 0.3162183165550232 | 0.8491586538461539\n",
      "> 26 | 0.3160095810890198 | 0.849380060728745\n",
      "> 27 | 0.31580600142478943 | 0.849380060728745\n",
      "> 28 | 0.31560787558555603 | 0.8496330971659919\n",
      "> 29 | 0.3154158592224121 | 0.8497596153846154\n",
      "> 30 | 0.31523004174232483 | 0.8501075404858299\n",
      "> 31 | 0.31505081057548523 | 0.8502973178137652\n",
      "> 32 | 0.3148781657218933 | 0.8506136133603239\n",
      "> 33 | 0.3147122263908386 | 0.8504554655870445\n",
      "> 34 | 0.31455308198928833 | 0.8505503542510121\n",
      "> 35 | 0.3144005537033081 | 0.8507717611336032\n",
      "> 36 | 0.31425464153289795 | 0.850835020242915\n",
      "> 37 | 0.3141152560710907 | 0.8510247975708503\n",
      "> 38 | 0.313982218503952 | 0.8512778340080972\n",
      "> 39 | 0.31385546922683716 | 0.8514359817813765\n",
      "> 40 | 0.3137347102165222 | 0.8513410931174089\n",
      "> 41 | 0.31362003087997437 | 0.8514043522267206\n",
      "> 42 | 0.3135110139846802 | 0.8514676113360324\n",
      "> 43 | 0.31340765953063965 | 0.8514992408906883\n",
      "> 44 | 0.3133096992969513 | 0.8514359817813765\n",
      "> 45 | 0.31321704387664795 | 0.8514676113360324\n",
      "> 46 | 0.3131294846534729 | 0.8516890182186235\n",
      "> 47 | 0.31304681301116943 | 0.8515941295546559\n",
      "> 48 | 0.3129688501358032 | 0.8519104251012146\n",
      "> 49 | 0.3128954768180847 | 0.8517839068825911\n",
      "> 50 | 0.3128265142440796 | 0.8518471659919028\n",
      "> 51 | 0.3127617835998535 | 0.8518471659919028\n",
      "> 52 | 0.31270116567611694 | 0.8519104251012146\n",
      "> 53 | 0.31264448165893555 | 0.8522267206477733\n",
      "> 54 | 0.312591552734375 | 0.8523216093117408\n",
      "> 55 | 0.312542200088501 | 0.852543016194332\n",
      "> 56 | 0.31249624490737915 | 0.852543016194332\n",
      "> 57 | 0.3124536871910095 | 0.852796052631579\n",
      "> 58 | 0.3124142289161682 | 0.8528276821862348\n",
      "> 59 | 0.3123778700828552 | 0.8529858299595142\n",
      "> 60 | 0.3123443126678467 | 0.8531123481781376\n",
      "> 61 | 0.3123135566711426 | 0.8531123481781376\n",
      "> 62 | 0.31228554248809814 | 0.8530807186234818\n",
      "> 63 | 0.3122599124908447 | 0.8531123481781376\n",
      "> 64 | 0.3122367858886719 | 0.8530174595141701\n",
      "> 65 | 0.3122158646583557 | 0.8531439777327935\n",
      "> 66 | 0.31219714879989624 | 0.8531756072874493\n",
      "> 67 | 0.3121804893016815 | 0.8531123481781376\n",
      "> 68 | 0.31216567754745483 | 0.8532072368421053\n",
      "> 69 | 0.3121528625488281 | 0.853270495951417\n",
      "> 70 | 0.3121417164802551 | 0.8531756072874493\n",
      "> 71 | 0.3121322691440582 | 0.853270495951417\n",
      "> 72 | 0.31212443113327026 | 0.8534919028340081\n",
      "> 73 | 0.3121179938316345 | 0.8534286437246964\n",
      "> 74 | 0.3121129870414734 | 0.8534919028340081\n",
      "> 75 | 0.31210917234420776 | 0.853523532388664\n",
      "> 76 | 0.3121066689491272 | 0.8535551619433198\n",
      "> 77 | 0.31210532784461975 | 0.8533653846153846\n",
      "> 78 | 0.31210505962371826 | 0.8533970141700404\n",
      "> 79 | 0.31210577487945557 | 0.853523532388664\n",
      "> 80 | 0.31210747361183167 | 0.8535867914979757\n",
      "> 81 | 0.3121100962162018 | 0.8537765688259109\n",
      "> 82 | 0.31211352348327637 | 0.853744939271255\n",
      "> 83 | 0.3121177554130554 | 0.8536816801619433\n",
      "> 84 | 0.3121227025985718 | 0.8536816801619433\n",
      "> 85 | 0.31212830543518066 | 0.8537765688259109\n",
      "> 86 | 0.31213444471359253 | 0.853744939271255\n",
      "> 87 | 0.3121412992477417 | 0.8536816801619433\n",
      "> 88 | 0.3121486306190491 | 0.8538398279352226\n",
      "> 89 | 0.31215643882751465 | 0.8539030870445344\n",
      "> 90 | 0.31216466426849365 | 0.8539663461538461\n",
      "> 91 | 0.31217342615127563 | 0.8540928643724697\n",
      "> 92 | 0.3121824562549591 | 0.8540928643724697\n",
      "> 93 | 0.312191903591156 | 0.8541244939271255\n",
      "> 94 | 0.3122016191482544 | 0.8540612348178138\n",
      "> 95 | 0.31221163272857666 | 0.8541877530364372\n",
      "> 96 | 0.3122220039367676 | 0.8540928643724697\n",
      "> 97 | 0.31223249435424805 | 0.8540928643724697\n",
      "> 98 | 0.3122432827949524 | 0.8541561234817814\n",
      "> 99 | 0.31225425004959106 | 0.8541877530364372\n",
      "> 100 | 0.31226539611816406 | 0.854251012145749\n",
      "> Evaluation\n",
      "> Class Acc = 0.8478228449821472\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8033664226531982 | 0.8972716443240643 | 0.8744684457778931\n",
      "> Confusion Matrix \n",
      "TN: 9339.0 | FP: 753.0 \n",
      "FN: 1302.0 | TP: 2110.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 3771.0 | FP: 98.0 \n",
      "FN: 234.0 | TP: 244.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 5568.0 | FP: 655.0 \n",
      "FN: 1068.0 | TP: 1866.0\n"
     ]
    }
   ],
   "source": [
    "for cv_seed in cv_seeds:\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        x, y, a, test_size=0.3, random_state=cv_seed)\n",
    "\n",
    "    train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "    train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # train below\n",
    "\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model = UnfairLogisticRegression(xdim, ydim, batch_size)\n",
    "    ret = train_loop(model, train_data, epochs, opt)\n",
    "    Y, A, Y_hat = evaluation(model, test_data)\n",
    "    \n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1  = compute_metrics(Y, A, Y_hat, adim=adim)\n",
    "    \n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    \n",
    "    tradeoff = []\n",
    "    \n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "    \n",
    "    result = ['UnfairLR', cv_seed, clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    del(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6e0fba0d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving into DF then CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>cv_seed</th>\n",
       "      <th>clas_acc</th>\n",
       "      <th>dp</th>\n",
       "      <th>deqodds</th>\n",
       "      <th>deqopp</th>\n",
       "      <th>trade_dp</th>\n",
       "      <th>trade_deqodds</th>\n",
       "      <th>trade_deqopp</th>\n",
       "      <th>TN_a0</th>\n",
       "      <th>FP_a0</th>\n",
       "      <th>FN_a0</th>\n",
       "      <th>TP_a0</th>\n",
       "      <th>TN_a1</th>\n",
       "      <th>FP_a1</th>\n",
       "      <th>FN_a1</th>\n",
       "      <th>TP_a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>13</td>\n",
       "      <td>0.837233</td>\n",
       "      <td>0.806283</td>\n",
       "      <td>0.856564</td>\n",
       "      <td>0.799981</td>\n",
       "      <td>0.821467</td>\n",
       "      <td>0.846788</td>\n",
       "      <td>0.818183</td>\n",
       "      <td>3807.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>5642.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>1175.0</td>\n",
       "      <td>1660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>29</td>\n",
       "      <td>0.836567</td>\n",
       "      <td>0.798439</td>\n",
       "      <td>0.846336</td>\n",
       "      <td>0.782983</td>\n",
       "      <td>0.817058</td>\n",
       "      <td>0.841423</td>\n",
       "      <td>0.808888</td>\n",
       "      <td>3802.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>5588.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1712.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>42</td>\n",
       "      <td>0.835530</td>\n",
       "      <td>0.811083</td>\n",
       "      <td>0.865257</td>\n",
       "      <td>0.811986</td>\n",
       "      <td>0.823125</td>\n",
       "      <td>0.850134</td>\n",
       "      <td>0.823590</td>\n",
       "      <td>3856.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>5603.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>1228.0</td>\n",
       "      <td>1633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>55</td>\n",
       "      <td>0.834345</td>\n",
       "      <td>0.810234</td>\n",
       "      <td>0.861803</td>\n",
       "      <td>0.804664</td>\n",
       "      <td>0.822113</td>\n",
       "      <td>0.847852</td>\n",
       "      <td>0.819236</td>\n",
       "      <td>3778.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>5638.0</td>\n",
       "      <td>623.0</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>1662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>73</td>\n",
       "      <td>0.833605</td>\n",
       "      <td>0.804183</td>\n",
       "      <td>0.868893</td>\n",
       "      <td>0.824305</td>\n",
       "      <td>0.818630</td>\n",
       "      <td>0.850883</td>\n",
       "      <td>0.828929</td>\n",
       "      <td>3799.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>5572.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>1694.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>13</td>\n",
       "      <td>0.849156</td>\n",
       "      <td>0.814985</td>\n",
       "      <td>0.902947</td>\n",
       "      <td>0.885014</td>\n",
       "      <td>0.831719</td>\n",
       "      <td>0.875226</td>\n",
       "      <td>0.866714</td>\n",
       "      <td>3782.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>1061.0</td>\n",
       "      <td>1774.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>29</td>\n",
       "      <td>0.849230</td>\n",
       "      <td>0.803651</td>\n",
       "      <td>0.889696</td>\n",
       "      <td>0.865887</td>\n",
       "      <td>0.825812</td>\n",
       "      <td>0.868992</td>\n",
       "      <td>0.857477</td>\n",
       "      <td>3794.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>5599.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>1819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>42</td>\n",
       "      <td>0.854414</td>\n",
       "      <td>0.812003</td>\n",
       "      <td>0.913281</td>\n",
       "      <td>0.902751</td>\n",
       "      <td>0.832668</td>\n",
       "      <td>0.882867</td>\n",
       "      <td>0.877917</td>\n",
       "      <td>3834.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>5601.0</td>\n",
       "      <td>618.0</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>1832.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>55</td>\n",
       "      <td>0.850489</td>\n",
       "      <td>0.815062</td>\n",
       "      <td>0.903148</td>\n",
       "      <td>0.878344</td>\n",
       "      <td>0.832399</td>\n",
       "      <td>0.876028</td>\n",
       "      <td>0.864192</td>\n",
       "      <td>3755.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>5657.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>1820.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>73</td>\n",
       "      <td>0.847823</td>\n",
       "      <td>0.803366</td>\n",
       "      <td>0.897272</td>\n",
       "      <td>0.874468</td>\n",
       "      <td>0.824996</td>\n",
       "      <td>0.871847</td>\n",
       "      <td>0.860940</td>\n",
       "      <td>3771.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>5568.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>1866.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name  cv_seed  clas_acc        dp   deqodds    deqopp  trade_dp  \\\n",
       "0  UnfairLR-decay       13  0.837233  0.806283  0.856564  0.799981  0.821467   \n",
       "1  UnfairLR-decay       29  0.836567  0.798439  0.846336  0.782983  0.817058   \n",
       "2  UnfairLR-decay       42  0.835530  0.811083  0.865257  0.811986  0.823125   \n",
       "3  UnfairLR-decay       55  0.834345  0.810234  0.861803  0.804664  0.822113   \n",
       "4  UnfairLR-decay       73  0.833605  0.804183  0.868893  0.824305  0.818630   \n",
       "5        UnfairLR       13  0.849156  0.814985  0.902947  0.885014  0.831719   \n",
       "6        UnfairLR       29  0.849230  0.803651  0.889696  0.865887  0.825812   \n",
       "7        UnfairLR       42  0.854414  0.812003  0.913281  0.902751  0.832668   \n",
       "8        UnfairLR       55  0.850489  0.815062  0.903148  0.878344  0.832399   \n",
       "9        UnfairLR       73  0.847823  0.803366  0.897272  0.874468  0.824996   \n",
       "\n",
       "   trade_deqodds  trade_deqopp   TN_a0  FP_a0  FN_a0  TP_a0   TN_a1  FP_a1  \\\n",
       "0       0.846788      0.818183  3807.0   62.0  314.0  197.0  5642.0  647.0   \n",
       "1       0.841423      0.808888  3802.0   76.0  311.0  195.0  5588.0  690.0   \n",
       "2       0.850134      0.823590  3856.0   69.0  308.0  191.0  5603.0  616.0   \n",
       "3       0.847852      0.819236  3778.0   71.0  310.0  189.0  5638.0  623.0   \n",
       "4       0.850883      0.828929  3799.0   70.0  286.0  192.0  5572.0  651.0   \n",
       "5       0.875226      0.866714  3782.0   87.0  250.0  261.0  5650.0  639.0   \n",
       "6       0.868992      0.857477  3794.0   84.0  250.0  256.0  5599.0  679.0   \n",
       "7       0.882867      0.877917  3834.0   91.0  228.0  271.0  5601.0  618.0   \n",
       "8       0.876028      0.864192  3755.0   94.0  246.0  253.0  5657.0  604.0   \n",
       "9       0.871847      0.860940  3771.0   98.0  234.0  244.0  5568.0  655.0   \n",
       "\n",
       "    FN_a1   TP_a1  \n",
       "0  1175.0  1660.0  \n",
       "1  1130.0  1712.0  \n",
       "2  1228.0  1633.0  \n",
       "3  1233.0  1662.0  \n",
       "4  1240.0  1694.0  \n",
       "5  1061.0  1774.0  \n",
       "6  1023.0  1819.0  \n",
       "7  1029.0  1832.0  \n",
       "8  1075.0  1820.0  \n",
       "9  1068.0  1866.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(results, columns=header)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f'{data_name}-result/unfair_lr-{epochs}.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41359ec383f887151a607ad1e28cb7dbc05f61385692c63e2bb2f343bf03f280"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('falsb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "34ca74ed6235dfc7dda926bb3adb31e801e3d02679121d5b444ee035e270bd57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
