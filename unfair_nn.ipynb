{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('falsb': conda)"
  },
  "interpreter": {
   "hash": "34ca74ed6235dfc7dda926bb3adb31e801e3d02679121d5b444ee035e270bd57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from math import sqrt, isnan\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "from util import metrics\n",
    "from util.load_data import load_data\n",
    "from util.evaluation import compute_tradeoff\n",
    "\n",
    "from madras_laftr.models import UnfairMLP"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "batch_size = 64\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "opt = Adam(learning_rate=lr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "header = \"model_name\", \"clas_acc\", \"dp\", \"deqodds\", \"deqopp\", \"trade_dp\", \"trade_deqodds\", \"trade_deqopp\", \"TN_a0\", \"FP_a0\", \"FN_a0\", \"TP_a0\", \"TN_a1\", \"FP_a1\", \"FN_a1\", \"TP_a1\"\n",
    "results = []\n",
    "\n",
    "test_loop = 5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x_train, y_train, a_train = load_data('adult', 'train')\n",
    "raw_data = (x_train, y_train, a_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "xdim = x_train.shape[1]\n",
    "ydim = y_train.shape[1]\n",
    "adim = a_train.shape[1]\n",
    "zdim = 8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "train_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 113), (64, 1), (64, 1)), types: (tf.float64, tf.float64, tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "x_valid, y_valid, a_valid = load_data('adult', 'valid')\n",
    "\n",
    "valid_data = Dataset.from_tensor_slices((x_valid, y_valid, a_valid))\n",
    "valid_data = valid_data.batch(batch_size, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "x_test, y_test, a_test = load_data('adult', 'test')\n",
    "\n",
    "test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "test_data = test_data.batch(batch_size, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Lopp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def train(unfair_clas, X, Y, optimizer):\n",
    "    \n",
    "    with tf.GradientTape() as tape0:\n",
    "        \n",
    "        unfair_clas(X, Y, training=True) #to compute the foward\n",
    "        current_loss = unfair_clas.loss #current loss\n",
    "    \n",
    "    grads = tape0.gradient(current_loss, unfair_clas.variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(grads, unfair_clas.variables))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def train_loop(unfair_clas, train_dataset, epochs, optmizer):\n",
    "    \n",
    "    print(\"> Epoch | Class Loss | Class Acc\")\n",
    "\n",
    "    x_train, y_train, a_train = raw_data\n",
    "    l = y_train.shape[0]\n",
    "    l = (l//batch_size) * batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        Y_hat = None\n",
    "        X_hat = None\n",
    "        batch_count = 1\n",
    "        \n",
    "        for X, Y, A in train_dataset:\n",
    "            \n",
    "            train(model, X, Y, optmizer)\n",
    "\n",
    "            if batch_count == 1:\n",
    "                Y_hat = model.Y_hat\n",
    "                batch_count += 1\n",
    "                \n",
    "            else:\n",
    "                Y_hat = tf.concat([Y_hat, model.Y_hat], 0)\n",
    "\n",
    "        clas_loss = tf.reduce_mean(model.loss)\n",
    "        clas_acc = metrics.accuracy(y_train[:l], tf.math.round(Y_hat))\n",
    "\n",
    "        print(\"> {} | {} | {}\".format(\n",
    "            epoch+1,\n",
    "            clas_loss,\n",
    "            clas_acc))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "\n",
    "def evaluation(model, valid_data):\n",
    "    Y_hat = None\n",
    "    batch_count = 1\n",
    "    \n",
    "    for X, Y, A in valid_data:\n",
    "        \n",
    "        model(X, Y, A)\n",
    "        \n",
    "        if batch_count == 1:\n",
    "            Y_hat = model.Y_hat\n",
    "            batch_count += 1\n",
    "        else:\n",
    "            Y_hat = tf.concat([Y_hat, model.Y_hat], 0)\n",
    "    \n",
    "    return Y_hat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def compute_metrics(Y, Y_hat, A):\n",
    "    Y_hat = tf.math.round(Y_hat)\n",
    "    \n",
    "    clas_acc = metrics.accuracy(Y, Y_hat)\n",
    "\n",
    "    print(\"> Class Acc\")\n",
    "    print(\"> {}\".format(clas_acc))\n",
    "\n",
    "    dp = metrics.DP(Y_hat.numpy(), A)\n",
    "    deqodds = metrics.DEqOdds(Y, Y_hat.numpy(), A)\n",
    "    deqopp = metrics.DEqOpp(Y, Y_hat.numpy(), A)\n",
    "\n",
    "    print(\"> DP | DI | DEOPP\")\n",
    "    print(\"> {} | {} | {}\".format(dp, deqodds, deqopp))\n",
    "\n",
    "    tp = metrics.TP(Y, Y_hat.numpy())\n",
    "    tn = metrics.TN(Y, Y_hat.numpy())\n",
    "    fp = metrics.FP(Y, Y_hat.numpy())\n",
    "    fn = metrics.FN(Y, Y_hat.numpy())\n",
    "\n",
    "    print('> Confusion Matrix \\n' +\n",
    "                'TN: {} | FP: {} \\n'.format(tn, fp) +\n",
    "                'FN: {} | TP: {}'.format(fn, tp))\n",
    "\n",
    "    confusion_matrix = np.array([[tn, fp],\n",
    "                                [fn, tp]])\n",
    "\n",
    "    m = [metrics.TN, metrics.FP, metrics.FN, metrics.TP]\n",
    "    metrics_a0 = [0, 0, 0, 0]\n",
    "    metrics_a1 = [0, 0, 0, 0]\n",
    "    for i in range(len(m)):\n",
    "        metrics_a0[i] = metrics.subgroup(m[i], A, Y, Y_hat.numpy())\n",
    "        metrics_a1[i] = metrics.subgroup(m[i], 1 - A, Y, Y_hat.numpy())\n",
    "\n",
    "    print('> Confusion Matrix for A = 0 \\n' +\n",
    "            'TN: {} | FP: {} \\n'.format(metrics_a0[0], metrics_a0[1]) +\n",
    "            'FN: {} | TP: {}'.format(metrics_a0[2], metrics_a0[3]))\n",
    "\n",
    "    print('> Confusion Matrix for A = 1 \\n' +\n",
    "            'TN: {} | FP: {} \\n'.format(metrics_a1[0], metrics_a1[1]) +\n",
    "            'FN: {} | TP: {}'.format(metrics_a1[2], metrics_a1[3]))\n",
    "\n",
    "    confusion_matrix = np.array([[tn, fp],\n",
    "                                [fn, tp]])\n",
    "\n",
    "    return clas_acc, confusion_matrix, dp, deqodds, deqopp, metrics_a0, metrics_a1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "for i in range(test_loop):\n",
    "    opt = Adam(learning_rate=lr)\n",
    "\n",
    "    model = UnfairMLP(xdim, zdim, ydim)\n",
    "    ret = train_loop(model, train_data, epochs, opt)\n",
    "    Y_hat = evaluation(model, valid_data)\n",
    "    \n",
    "    clas_acc, confusion_matrix, dp, deqodds, deqopp, metrics_a0, metrics_a1  = compute_metrics(y_valid, Y_hat, a_valid)\n",
    "    \n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    \n",
    "    tradeoff = []\n",
    "    \n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "    \n",
    "    result = ['UnfairNN', clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.3882751166820526 | 0.7657078912466844\n",
      "> 2 | 0.3911265730857849 | 0.8118783156498673\n",
      "> 3 | 0.39347174763679504 | 0.8273789787798409\n",
      "> 4 | 0.3386576473712921 | 0.8274204244031831\n",
      "> 5 | 0.347187876701355 | 0.830487400530504\n",
      "> 6 | 0.33378928899765015 | 0.8265086206896551\n",
      "> 7 | 0.365893691778183 | 0.8252238063660478\n",
      "> 8 | 0.3630523085594177 | 0.8309433023872679\n",
      "> 9 | 0.3241446018218994 | 0.8299900530503979\n",
      "> 10 | 0.42222434282302856 | 0.8318136604774535\n",
      "> 11 | 0.3776119351387024 | 0.8231929708222812\n",
      "> 12 | 0.3099239468574524 | 0.831191976127321\n",
      "> 13 | 0.33585864305496216 | 0.82907824933687\n",
      "> 14 | 0.3018547296524048 | 0.8343832891246684\n",
      "> 15 | 0.37438905239105225 | 0.8275033156498673\n",
      "> 16 | 0.34225133061408997 | 0.8276690981432361\n",
      "> 17 | 0.30041420459747314 | 0.8353365384615384\n",
      "> 18 | 0.36504611372947693 | 0.8284565649867374\n",
      "> 19 | 0.33269864320755005 | 0.8310261936339522\n",
      "> 20 | 0.30380529165267944 | 0.8338444960212201\n",
      "> 21 | 0.30134326219558716 | 0.8400613395225465\n",
      "> 22 | 0.2945544719696045 | 0.8393982095490716\n",
      "> 23 | 0.32687243819236755 | 0.8368285809018567\n",
      "> 24 | 0.3008618950843811 | 0.8356266578249337\n",
      "> 25 | 0.3260415494441986 | 0.8319794429708223\n",
      "> 26 | 0.3010311424732208 | 0.8364970159151194\n",
      "> 27 | 0.33437591791152954 | 0.8378647214854111\n",
      "> 28 | 0.302618145942688 | 0.8359996684350133\n",
      "> 29 | 0.3236303925514221 | 0.8310676392572944\n",
      "> 30 | 0.2973473370075226 | 0.837823275862069\n",
      "> 31 | 0.3027607500553131 | 0.8379061671087533\n",
      "> 32 | 0.3135007321834564 | 0.8398541114058355\n",
      "> 33 | 0.3426706790924072 | 0.8355852122015915\n",
      "> 34 | 0.3323158919811249 | 0.833554376657825\n",
      "> 35 | 0.32630038261413574 | 0.8369943633952255\n",
      "> 36 | 0.3656062185764313 | 0.8350049734748011\n",
      "> 37 | 0.33228468894958496 | 0.8301143899204244\n",
      "> 38 | 0.30611899495124817 | 0.8350464190981433\n",
      "> 39 | 0.30765610933303833 | 0.8407244694960212\n",
      "> 40 | 0.31613990664482117 | 0.8411803713527851\n",
      "> 41 | 0.33766302466392517 | 0.8309433023872679\n",
      "> 42 | 0.3120839297771454 | 0.8344661803713528\n",
      "> 43 | 0.2989777624607086 | 0.8455736074270557\n",
      "> 44 | 0.29794204235076904 | 0.8422994031830239\n",
      "> 45 | 0.30181440711021423 | 0.8398126657824934\n",
      "> 46 | 0.3459439277648926 | 0.8319379973474801\n",
      "> 47 | 0.3238271474838257 | 0.8318965517241379\n",
      "> 48 | 0.30589622259140015 | 0.8347977453580901\n",
      "> 49 | 0.3129117488861084 | 0.8435013262599469\n",
      "> 50 | 0.30092981457710266 | 0.8457393899204244\n",
      "> 51 | 0.3188168406486511 | 0.8438328912466844\n",
      "> 52 | 0.29805389046669006 | 0.8396883289124668\n",
      "> 53 | 0.3435716927051544 | 0.8323110079575597\n",
      "> 54 | 0.3220714032649994 | 0.8333885941644562\n",
      "> 55 | 0.302541047334671 | 0.8365384615384616\n",
      "> 56 | 0.29643386602401733 | 0.8476873342175066\n",
      "> 57 | 0.3087082505226135 | 0.846112400530504\n",
      "> 58 | 0.32428300380706787 | 0.8324353448275862\n",
      "> 59 | 0.2977660894393921 | 0.8363726790450928\n",
      "> 60 | 0.3009134531021118 | 0.8466511936339522\n",
      "> 61 | 0.29431530833244324 | 0.8428381962864722\n",
      "> 62 | 0.2976570129394531 | 0.8465683023872679\n",
      "> 63 | 0.2983085513114929 | 0.84565649867374\n",
      "> 64 | 0.30042892694473267 | 0.8435427718832891\n",
      "> 65 | 0.3393920660018921 | 0.8323524535809018\n",
      "> 66 | 0.31939905881881714 | 0.8345490716180372\n",
      "> 67 | 0.29687732458114624 | 0.8374088196286472\n",
      "> 68 | 0.2968798279762268 | 0.8465268567639257\n",
      "> 69 | 0.3135567307472229 | 0.8438328912466844\n",
      "> 70 | 0.2954620122909546 | 0.8442887931034483\n",
      "> 71 | 0.3276565372943878 | 0.8328083554376657\n",
      "> 72 | 0.3091464638710022 | 0.8348391909814323\n",
      "> 73 | 0.2928928732872009 | 0.8451591511936339\n",
      "> 74 | 0.3003489375114441 | 0.8469827586206897\n",
      "> 75 | 0.30934834480285645 | 0.839481100795756\n",
      "> 76 | 0.29756978154182434 | 0.8434184350132626\n",
      "> 77 | 0.3147426247596741 | 0.8401856763925729\n",
      "> 78 | 0.2975875437259674 | 0.8395225464190982\n",
      "> 79 | 0.34623077511787415 | 0.8382377320954907\n",
      "> 80 | 0.3217821717262268 | 0.8332642572944298\n",
      "> 81 | 0.30569377541542053 | 0.8369114721485411\n",
      "> 82 | 0.2992869019508362 | 0.8464439655172413\n",
      "> 83 | 0.29311424493789673 | 0.8472314323607427\n",
      "> 84 | 0.2967512905597687 | 0.8430868700265252\n",
      "> 85 | 0.30181884765625 | 0.847770225464191\n",
      "> 86 | 0.3193711042404175 | 0.8435013262599469\n",
      "> 87 | 0.2955193519592285 | 0.840683023872679\n",
      "> 88 | 0.30032116174697876 | 0.8448275862068966\n",
      "> 89 | 0.3168453574180603 | 0.8414290450928382\n",
      "> 90 | 0.2912535071372986 | 0.84184350132626\n",
      "> 91 | 0.3096695840358734 | 0.8405172413793103\n",
      "> 92 | 0.3212658166885376 | 0.8423408488063661\n",
      "> 93 | 0.3035566806793213 | 0.8391495358090186\n",
      "> 94 | 0.32387396693229675 | 0.841387599469496\n",
      "> 95 | 0.29497912526130676 | 0.8415948275862069\n",
      "> 96 | 0.294195294380188 | 0.8443302387267905\n",
      "> 97 | 0.31869834661483765 | 0.8394396551724138\n",
      "> 98 | 0.31293922662734985 | 0.8378647214854111\n",
      "> 99 | 0.31371939182281494 | 0.8442059018567639\n",
      "> 100 | 0.3139076828956604 | 0.8410974801061009\n",
      "> 101 | 0.29659292101860046 | 0.8405172413793103\n",
      "> 102 | 0.2903013229370117 | 0.8470242042440318\n",
      "> 103 | 0.2948981523513794 | 0.8424651856763926\n",
      "> 104 | 0.3046908378601074 | 0.8363312334217506\n",
      "> 105 | 0.309370219707489 | 0.8420092838196287\n",
      "> 106 | 0.306838721036911 | 0.8452420424403183\n",
      "> 107 | 0.31119781732559204 | 0.8448690318302388\n",
      "> 108 | 0.3027680814266205 | 0.8408902519893899\n",
      "> 109 | 0.31408974528312683 | 0.8426724137931034\n",
      "> 110 | 0.3060048222541809 | 0.8433355437665783\n",
      "> 111 | 0.31849950551986694 | 0.8419263925729443\n",
      "> 112 | 0.2956429123878479 | 0.8408902519893899\n",
      "> 113 | 0.33033716678619385 | 0.8327669098143236\n",
      "> 114 | 0.31399327516555786 | 0.8351293103448276\n",
      "> 115 | 0.2936520576477051 | 0.8419678381962865\n",
      "> 116 | 0.30169379711151123 | 0.8426309681697612\n",
      "> 117 | 0.2905847728252411 | 0.84184350132626\n",
      "> 118 | 0.30101943016052246 | 0.8430454244031831\n",
      "> 119 | 0.30757108330726624 | 0.8426724137931034\n",
      "> 120 | 0.29238614439964294 | 0.846816976127321\n",
      "> 121 | 0.28869280219078064 | 0.8453663793103449\n",
      "> 122 | 0.31080812215805054 | 0.8466926392572944\n",
      "> 123 | 0.307739794254303 | 0.8461538461538461\n",
      "> 124 | 0.2936897277832031 | 0.8458222811671088\n",
      "> 125 | 0.29642295837402344 | 0.8445374668435013\n",
      "> 126 | 0.3288978338241577 | 0.8327669098143236\n",
      "> 127 | 0.3131151795387268 | 0.8353365384615384\n",
      "> 128 | 0.3075034022331238 | 0.8436256631299734\n",
      "> 129 | 0.30388763546943665 | 0.8473143236074271\n",
      "> 130 | 0.2929658591747284 | 0.8454907161803713\n",
      "> 131 | 0.3094061315059662 | 0.8451591511936339\n",
      "> 132 | 0.3055885434150696 | 0.8426724137931034\n",
      "> 133 | 0.29462429881095886 | 0.8461952917771883\n",
      "> 134 | 0.31349438428878784 | 0.8466511936339522\n",
      "> 135 | 0.31384462118148804 | 0.8456150530503979\n",
      "> 136 | 0.2928730249404907 | 0.8433769893899205\n",
      "> 137 | 0.28897160291671753 | 0.8464854111405835\n",
      "> 138 | 0.30614805221557617 | 0.8459466180371353\n",
      "> 139 | 0.30117201805114746 | 0.8455321618037135\n",
      "> 140 | 0.311273992061615 | 0.8432112068965517\n",
      "> 141 | 0.30158185958862305 | 0.8473143236074271\n",
      "> 142 | 0.3068285882472992 | 0.8451177055702918\n",
      "> 143 | 0.29306676983833313 | 0.8461952917771883\n",
      "> 144 | 0.2943757176399231 | 0.8449519230769231\n",
      "> 145 | 0.29861894249916077 | 0.8425066312997347\n",
      "> 146 | 0.3254550099372864 | 0.8434598806366047\n",
      "> 147 | 0.29067206382751465 | 0.8364970159151194\n",
      "> 148 | 0.2965541481971741 | 0.8435842175066313\n",
      "> 149 | 0.30458539724349976 | 0.8478531167108754\n",
      "> 150 | 0.3137039542198181 | 0.8420507294429709\n",
      "> 151 | 0.29514080286026 | 0.8457808355437666\n",
      "> 152 | 0.29731637239456177 | 0.8474386604774535\n",
      "> 153 | 0.30459436774253845 | 0.8423822944297082\n",
      "> 154 | 0.28762611746788025 | 0.8447446949602122\n",
      "> 155 | 0.30730047821998596 | 0.8445374668435013\n",
      "> 156 | 0.30867546796798706 | 0.8465268567639257\n",
      "> 157 | 0.2928055226802826 | 0.8469413129973475\n",
      "> 158 | 0.3290896713733673 | 0.8341760610079576\n",
      "> 159 | 0.31565797328948975 | 0.8352950928381963\n",
      "> 160 | 0.3066164255142212 | 0.8434184350132626\n",
      "> 161 | 0.30746763944625854 | 0.8463610742705571\n",
      "> 162 | 0.28670984506607056 | 0.8468998673740054\n",
      "> 163 | 0.29771894216537476 | 0.8487649204244032\n",
      "> 164 | 0.3105890154838562 | 0.8471070954907162\n",
      "> 165 | 0.2842349708080292 | 0.8438328912466844\n",
      "> 166 | 0.3072255253791809 | 0.8483504641909814\n",
      "> 167 | 0.28918564319610596 | 0.8418020557029178\n",
      "> 168 | 0.2961442470550537 | 0.848723474801061\n",
      "> 169 | 0.30724674463272095 | 0.8462367374005305\n",
      "> 170 | 0.2909051179885864 | 0.8483504641909814\n",
      "> 171 | 0.314302921295166 | 0.8458222811671088\n",
      "> 172 | 0.2946743369102478 | 0.8447861405835544\n",
      "> 173 | 0.31339573860168457 | 0.8424237400530504\n",
      "> 174 | 0.31305742263793945 | 0.8449104774535809\n",
      "> 175 | 0.312477707862854 | 0.8447446949602122\n",
      "> 176 | 0.30264416337013245 | 0.8444131299734748\n",
      "> 177 | 0.31280517578125 | 0.8426309681697612\n",
      "> 178 | 0.30281904339790344 | 0.8444131299734748\n",
      "> 179 | 0.31217557191848755 | 0.8426309681697612\n",
      "> 180 | 0.3112425208091736 | 0.8437085543766578\n",
      "> 181 | 0.29478931427001953 | 0.8462367374005305\n",
      "> 182 | 0.31621742248535156 | 0.8361240053050398\n",
      "> 183 | 0.2915831208229065 | 0.8428381962864722\n",
      "> 184 | 0.28419703245162964 | 0.8466926392572944\n",
      "> 185 | 0.2840709090232849 | 0.8432526525198939\n",
      "> 186 | 0.3083409070968628 | 0.8468998673740054\n",
      "> 187 | 0.29382285475730896 | 0.8469413129973475\n",
      "> 188 | 0.29698309302330017 | 0.8452005968169761\n",
      "> 189 | 0.2859639823436737 | 0.8459466180371353\n",
      "> 190 | 0.29584020376205444 | 0.8452005968169761\n",
      "> 191 | 0.2900868356227875 | 0.8466511936339522\n",
      "> 192 | 0.32802659273147583 | 0.8345905172413793\n",
      "> 193 | 0.3134844899177551 | 0.8357924403183024\n",
      "> 194 | 0.30796802043914795 | 0.8439572281167109\n",
      "> 195 | 0.30597299337387085 | 0.8464854111405835\n",
      "> 196 | 0.28557053208351135 | 0.8457808355437666\n",
      "> 197 | 0.283300518989563 | 0.8490135941644562\n",
      "> 198 | 0.28509101271629333 | 0.8500497347480106\n",
      "> 199 | 0.3093264102935791 | 0.8462367374005305\n",
      "> 200 | 0.3040257394313812 | 0.8460295092838196\n",
      "> Class Acc\n",
      "> 0.8297872340425532\n",
      "> DP | DI | DEOPP\n",
      "> 0.7860314697027206 | 0.9142525047063828 | 0.939537525177002\n",
      "> Confusion Matrix \n",
      "TN: 3925.0 | FP: 593.0 \n",
      "FN: 431.0 | TP: 1067.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1565.0 | FP: 102.0 \n",
      "FN: 72.0 | TP: 140.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2360.0 | FP: 491.0 \n",
      "FN: 359.0 | TP: 927.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.3882751166820526 | 0.7657078912466844\n",
      "> 2 | 0.3911265730857849 | 0.8118783156498673\n",
      "> 3 | 0.39347174763679504 | 0.8273789787798409\n",
      "> 4 | 0.3386576473712921 | 0.8274204244031831\n",
      "> 5 | 0.347187876701355 | 0.830487400530504\n",
      "> 6 | 0.33378928899765015 | 0.8265086206896551\n",
      "> 7 | 0.365893691778183 | 0.8252238063660478\n",
      "> 8 | 0.3630523085594177 | 0.8309433023872679\n",
      "> 9 | 0.3241446018218994 | 0.8299900530503979\n",
      "> 10 | 0.42222434282302856 | 0.8318136604774535\n",
      "> 11 | 0.3776119351387024 | 0.8231929708222812\n",
      "> 12 | 0.3099239468574524 | 0.831191976127321\n",
      "> 13 | 0.33585864305496216 | 0.82907824933687\n",
      "> 14 | 0.3018547296524048 | 0.8343832891246684\n",
      "> 15 | 0.37438905239105225 | 0.8275033156498673\n",
      "> 16 | 0.34225133061408997 | 0.8276690981432361\n",
      "> 17 | 0.30041420459747314 | 0.8353365384615384\n",
      "> 18 | 0.36504611372947693 | 0.8284565649867374\n",
      "> 19 | 0.33269864320755005 | 0.8310261936339522\n",
      "> 20 | 0.30380529165267944 | 0.8338444960212201\n",
      "> 21 | 0.30134326219558716 | 0.8400613395225465\n",
      "> 22 | 0.2945544719696045 | 0.8393982095490716\n",
      "> 23 | 0.32687243819236755 | 0.8368285809018567\n",
      "> 24 | 0.3008618950843811 | 0.8356266578249337\n",
      "> 25 | 0.3260415494441986 | 0.8319794429708223\n",
      "> 26 | 0.3010311424732208 | 0.8364970159151194\n",
      "> 27 | 0.33437591791152954 | 0.8378647214854111\n",
      "> 28 | 0.302618145942688 | 0.8359996684350133\n",
      "> 29 | 0.3236303925514221 | 0.8310676392572944\n",
      "> 30 | 0.2973473370075226 | 0.837823275862069\n",
      "> 31 | 0.3027607500553131 | 0.8379061671087533\n",
      "> 32 | 0.3135007321834564 | 0.8398541114058355\n",
      "> 33 | 0.3426706790924072 | 0.8355852122015915\n",
      "> 34 | 0.3323158919811249 | 0.833554376657825\n",
      "> 35 | 0.32630038261413574 | 0.8369943633952255\n",
      "> 36 | 0.3656062185764313 | 0.8350049734748011\n",
      "> 37 | 0.33228468894958496 | 0.8301143899204244\n",
      "> 38 | 0.30611899495124817 | 0.8350464190981433\n",
      "> 39 | 0.30765610933303833 | 0.8407244694960212\n",
      "> 40 | 0.31613990664482117 | 0.8411803713527851\n",
      "> 41 | 0.33766302466392517 | 0.8309433023872679\n",
      "> 42 | 0.3120839297771454 | 0.8344661803713528\n",
      "> 43 | 0.2989777624607086 | 0.8455736074270557\n",
      "> 44 | 0.29794204235076904 | 0.8422994031830239\n",
      "> 45 | 0.30181440711021423 | 0.8398126657824934\n",
      "> 46 | 0.3459439277648926 | 0.8319379973474801\n",
      "> 47 | 0.3238271474838257 | 0.8318965517241379\n",
      "> 48 | 0.30589622259140015 | 0.8347977453580901\n",
      "> 49 | 0.3129117488861084 | 0.8435013262599469\n",
      "> 50 | 0.30092981457710266 | 0.8457393899204244\n",
      "> 51 | 0.3188168406486511 | 0.8438328912466844\n",
      "> 52 | 0.29805389046669006 | 0.8396883289124668\n",
      "> 53 | 0.3435716927051544 | 0.8323110079575597\n",
      "> 54 | 0.3220714032649994 | 0.8333885941644562\n",
      "> 55 | 0.302541047334671 | 0.8365384615384616\n",
      "> 56 | 0.29643386602401733 | 0.8476873342175066\n",
      "> 57 | 0.3087082505226135 | 0.846112400530504\n",
      "> 58 | 0.32428300380706787 | 0.8324353448275862\n",
      "> 59 | 0.2977660894393921 | 0.8363726790450928\n",
      "> 60 | 0.3009134531021118 | 0.8466511936339522\n",
      "> 61 | 0.29431530833244324 | 0.8428381962864722\n",
      "> 62 | 0.2976570129394531 | 0.8465683023872679\n",
      "> 63 | 0.2983085513114929 | 0.84565649867374\n",
      "> 64 | 0.30042892694473267 | 0.8435427718832891\n",
      "> 65 | 0.3393920660018921 | 0.8323524535809018\n",
      "> 66 | 0.31939905881881714 | 0.8345490716180372\n",
      "> 67 | 0.29687732458114624 | 0.8374088196286472\n",
      "> 68 | 0.2968798279762268 | 0.8465268567639257\n",
      "> 69 | 0.3135567307472229 | 0.8438328912466844\n",
      "> 70 | 0.2954620122909546 | 0.8442887931034483\n",
      "> 71 | 0.3276565372943878 | 0.8328083554376657\n",
      "> 72 | 0.3091464638710022 | 0.8348391909814323\n",
      "> 73 | 0.2928928732872009 | 0.8451591511936339\n",
      "> 74 | 0.3003489375114441 | 0.8469827586206897\n",
      "> 75 | 0.30934834480285645 | 0.839481100795756\n",
      "> 76 | 0.29756978154182434 | 0.8434184350132626\n",
      "> 77 | 0.3147426247596741 | 0.8401856763925729\n",
      "> 78 | 0.2975875437259674 | 0.8395225464190982\n",
      "> 79 | 0.34623077511787415 | 0.8382377320954907\n",
      "> 80 | 0.3217821717262268 | 0.8332642572944298\n",
      "> 81 | 0.30569377541542053 | 0.8369114721485411\n",
      "> 82 | 0.2992869019508362 | 0.8464439655172413\n",
      "> 83 | 0.29311424493789673 | 0.8472314323607427\n",
      "> 84 | 0.2967512905597687 | 0.8430868700265252\n",
      "> 85 | 0.30181884765625 | 0.847770225464191\n",
      "> 86 | 0.3193711042404175 | 0.8435013262599469\n",
      "> 87 | 0.2955193519592285 | 0.840683023872679\n",
      "> 88 | 0.30032116174697876 | 0.8448275862068966\n",
      "> 89 | 0.3168453574180603 | 0.8414290450928382\n",
      "> 90 | 0.2912535071372986 | 0.84184350132626\n",
      "> 91 | 0.3096695840358734 | 0.8405172413793103\n",
      "> 92 | 0.3212658166885376 | 0.8423408488063661\n",
      "> 93 | 0.3035566806793213 | 0.8391495358090186\n",
      "> 94 | 0.32387396693229675 | 0.841387599469496\n",
      "> 95 | 0.29497912526130676 | 0.8415948275862069\n",
      "> 96 | 0.294195294380188 | 0.8443302387267905\n",
      "> 97 | 0.31869834661483765 | 0.8394396551724138\n",
      "> 98 | 0.31293922662734985 | 0.8378647214854111\n",
      "> 99 | 0.31371939182281494 | 0.8442059018567639\n",
      "> 100 | 0.3139076828956604 | 0.8410974801061009\n",
      "> 101 | 0.29659292101860046 | 0.8405172413793103\n",
      "> 102 | 0.2903013229370117 | 0.8470242042440318\n",
      "> 103 | 0.2948981523513794 | 0.8424651856763926\n",
      "> 104 | 0.3046908378601074 | 0.8363312334217506\n",
      "> 105 | 0.309370219707489 | 0.8420092838196287\n",
      "> 106 | 0.306838721036911 | 0.8452420424403183\n",
      "> 107 | 0.31119781732559204 | 0.8448690318302388\n",
      "> 108 | 0.3027680814266205 | 0.8408902519893899\n",
      "> 109 | 0.31408974528312683 | 0.8426724137931034\n",
      "> 110 | 0.3060048222541809 | 0.8433355437665783\n",
      "> 111 | 0.31849950551986694 | 0.8419263925729443\n",
      "> 112 | 0.2956429123878479 | 0.8408902519893899\n",
      "> 113 | 0.33033716678619385 | 0.8327669098143236\n",
      "> 114 | 0.31399327516555786 | 0.8351293103448276\n",
      "> 115 | 0.2936520576477051 | 0.8419678381962865\n",
      "> 116 | 0.30169379711151123 | 0.8426309681697612\n",
      "> 117 | 0.2905847728252411 | 0.84184350132626\n",
      "> 118 | 0.30101943016052246 | 0.8430454244031831\n",
      "> 119 | 0.30757108330726624 | 0.8426724137931034\n",
      "> 120 | 0.29238614439964294 | 0.846816976127321\n",
      "> 121 | 0.28869280219078064 | 0.8453663793103449\n",
      "> 122 | 0.31080812215805054 | 0.8466926392572944\n",
      "> 123 | 0.307739794254303 | 0.8461538461538461\n",
      "> 124 | 0.2936897277832031 | 0.8458222811671088\n",
      "> 125 | 0.29642295837402344 | 0.8445374668435013\n",
      "> 126 | 0.3288978338241577 | 0.8327669098143236\n",
      "> 127 | 0.3131151795387268 | 0.8353365384615384\n",
      "> 128 | 0.3075034022331238 | 0.8436256631299734\n",
      "> 129 | 0.30388763546943665 | 0.8473143236074271\n",
      "> 130 | 0.2929658591747284 | 0.8454907161803713\n",
      "> 131 | 0.3094061315059662 | 0.8451591511936339\n",
      "> 132 | 0.3055885434150696 | 0.8426724137931034\n",
      "> 133 | 0.29462429881095886 | 0.8461952917771883\n",
      "> 134 | 0.31349438428878784 | 0.8466511936339522\n",
      "> 135 | 0.31384462118148804 | 0.8456150530503979\n",
      "> 136 | 0.2928730249404907 | 0.8433769893899205\n",
      "> 137 | 0.28897160291671753 | 0.8464854111405835\n",
      "> 138 | 0.30614805221557617 | 0.8459466180371353\n",
      "> 139 | 0.30117201805114746 | 0.8455321618037135\n",
      "> 140 | 0.311273992061615 | 0.8432112068965517\n",
      "> 141 | 0.30158185958862305 | 0.8473143236074271\n",
      "> 142 | 0.3068285882472992 | 0.8451177055702918\n",
      "> 143 | 0.29306676983833313 | 0.8461952917771883\n",
      "> 144 | 0.2943757176399231 | 0.8449519230769231\n",
      "> 145 | 0.29861894249916077 | 0.8425066312997347\n",
      "> 146 | 0.3254550099372864 | 0.8434598806366047\n",
      "> 147 | 0.29067206382751465 | 0.8364970159151194\n",
      "> 148 | 0.2965541481971741 | 0.8435842175066313\n",
      "> 149 | 0.30458539724349976 | 0.8478531167108754\n",
      "> 150 | 0.3137039542198181 | 0.8420507294429709\n",
      "> 151 | 0.29514080286026 | 0.8457808355437666\n",
      "> 152 | 0.29731637239456177 | 0.8474386604774535\n",
      "> 153 | 0.30459436774253845 | 0.8423822944297082\n",
      "> 154 | 0.28762611746788025 | 0.8447446949602122\n",
      "> 155 | 0.30730047821998596 | 0.8445374668435013\n",
      "> 156 | 0.30867546796798706 | 0.8465268567639257\n",
      "> 157 | 0.2928055226802826 | 0.8469413129973475\n",
      "> 158 | 0.3290896713733673 | 0.8341760610079576\n",
      "> 159 | 0.31565797328948975 | 0.8352950928381963\n",
      "> 160 | 0.3066164255142212 | 0.8434184350132626\n",
      "> 161 | 0.30746763944625854 | 0.8463610742705571\n",
      "> 162 | 0.28670984506607056 | 0.8468998673740054\n",
      "> 163 | 0.29771894216537476 | 0.8487649204244032\n",
      "> 164 | 0.3105890154838562 | 0.8471070954907162\n",
      "> 165 | 0.2842349708080292 | 0.8438328912466844\n",
      "> 166 | 0.3072255253791809 | 0.8483504641909814\n",
      "> 167 | 0.28918564319610596 | 0.8418020557029178\n",
      "> 168 | 0.2961442470550537 | 0.848723474801061\n",
      "> 169 | 0.30724674463272095 | 0.8462367374005305\n",
      "> 170 | 0.2909051179885864 | 0.8483504641909814\n",
      "> 171 | 0.314302921295166 | 0.8458222811671088\n",
      "> 172 | 0.2946743369102478 | 0.8447861405835544\n",
      "> 173 | 0.31339573860168457 | 0.8424237400530504\n",
      "> 174 | 0.31305742263793945 | 0.8449104774535809\n",
      "> 175 | 0.312477707862854 | 0.8447446949602122\n",
      "> 176 | 0.30264416337013245 | 0.8444131299734748\n",
      "> 177 | 0.31280517578125 | 0.8426309681697612\n",
      "> 178 | 0.30281904339790344 | 0.8444131299734748\n",
      "> 179 | 0.31217557191848755 | 0.8426309681697612\n",
      "> 180 | 0.3112425208091736 | 0.8437085543766578\n",
      "> 181 | 0.29478931427001953 | 0.8462367374005305\n",
      "> 182 | 0.31621742248535156 | 0.8361240053050398\n",
      "> 183 | 0.2915831208229065 | 0.8428381962864722\n",
      "> 184 | 0.28419703245162964 | 0.8466926392572944\n",
      "> 185 | 0.2840709090232849 | 0.8432526525198939\n",
      "> 186 | 0.3083409070968628 | 0.8468998673740054\n",
      "> 187 | 0.29382285475730896 | 0.8469413129973475\n",
      "> 188 | 0.29698309302330017 | 0.8452005968169761\n",
      "> 189 | 0.2859639823436737 | 0.8459466180371353\n",
      "> 190 | 0.29584020376205444 | 0.8452005968169761\n",
      "> 191 | 0.2900868356227875 | 0.8466511936339522\n",
      "> 192 | 0.32802659273147583 | 0.8345905172413793\n",
      "> 193 | 0.3134844899177551 | 0.8357924403183024\n",
      "> 194 | 0.30796802043914795 | 0.8439572281167109\n",
      "> 195 | 0.30597299337387085 | 0.8464854111405835\n",
      "> 196 | 0.28557053208351135 | 0.8457808355437666\n",
      "> 197 | 0.283300518989563 | 0.8490135941644562\n",
      "> 198 | 0.28509101271629333 | 0.8500497347480106\n",
      "> 199 | 0.3093264102935791 | 0.8462367374005305\n",
      "> 200 | 0.3040257394313812 | 0.8460295092838196\n",
      "> Class Acc\n",
      "> 0.8297872340425532\n",
      "> DP | DI | DEOPP\n",
      "> 0.7860314697027206 | 0.9142525047063828 | 0.939537525177002\n",
      "> Confusion Matrix \n",
      "TN: 3925.0 | FP: 593.0 \n",
      "FN: 431.0 | TP: 1067.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1565.0 | FP: 102.0 \n",
      "FN: 72.0 | TP: 140.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2360.0 | FP: 491.0 \n",
      "FN: 359.0 | TP: 927.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.3882751166820526 | 0.7657078912466844\n",
      "> 2 | 0.3911265730857849 | 0.8118783156498673\n",
      "> 3 | 0.39347174763679504 | 0.8273789787798409\n",
      "> 4 | 0.3386576473712921 | 0.8274204244031831\n",
      "> 5 | 0.347187876701355 | 0.830487400530504\n",
      "> 6 | 0.33378928899765015 | 0.8265086206896551\n",
      "> 7 | 0.365893691778183 | 0.8252238063660478\n",
      "> 8 | 0.3630523085594177 | 0.8309433023872679\n",
      "> 9 | 0.3241446018218994 | 0.8299900530503979\n",
      "> 10 | 0.42222434282302856 | 0.8318136604774535\n",
      "> 11 | 0.3776119351387024 | 0.8231929708222812\n",
      "> 12 | 0.3099239468574524 | 0.831191976127321\n",
      "> 13 | 0.33585864305496216 | 0.82907824933687\n",
      "> 14 | 0.3018547296524048 | 0.8343832891246684\n",
      "> 15 | 0.37438905239105225 | 0.8275033156498673\n",
      "> 16 | 0.34225133061408997 | 0.8276690981432361\n",
      "> 17 | 0.30041420459747314 | 0.8353365384615384\n",
      "> 18 | 0.36504611372947693 | 0.8284565649867374\n",
      "> 19 | 0.33269864320755005 | 0.8310261936339522\n",
      "> 20 | 0.30380529165267944 | 0.8338444960212201\n",
      "> 21 | 0.30134326219558716 | 0.8400613395225465\n",
      "> 22 | 0.2945544719696045 | 0.8393982095490716\n",
      "> 23 | 0.32687243819236755 | 0.8368285809018567\n",
      "> 24 | 0.3008618950843811 | 0.8356266578249337\n",
      "> 25 | 0.3260415494441986 | 0.8319794429708223\n",
      "> 26 | 0.3010311424732208 | 0.8364970159151194\n",
      "> 27 | 0.33437591791152954 | 0.8378647214854111\n",
      "> 28 | 0.302618145942688 | 0.8359996684350133\n",
      "> 29 | 0.3236303925514221 | 0.8310676392572944\n",
      "> 30 | 0.2973473370075226 | 0.837823275862069\n",
      "> 31 | 0.3027607500553131 | 0.8379061671087533\n",
      "> 32 | 0.3135007321834564 | 0.8398541114058355\n",
      "> 33 | 0.3426706790924072 | 0.8355852122015915\n",
      "> 34 | 0.3323158919811249 | 0.833554376657825\n",
      "> 35 | 0.32630038261413574 | 0.8369943633952255\n",
      "> 36 | 0.3656062185764313 | 0.8350049734748011\n",
      "> 37 | 0.33228468894958496 | 0.8301143899204244\n",
      "> 38 | 0.30611899495124817 | 0.8350464190981433\n",
      "> 39 | 0.30765610933303833 | 0.8407244694960212\n",
      "> 40 | 0.31613990664482117 | 0.8411803713527851\n",
      "> 41 | 0.33766302466392517 | 0.8309433023872679\n",
      "> 42 | 0.3120839297771454 | 0.8344661803713528\n",
      "> 43 | 0.2989777624607086 | 0.8455736074270557\n",
      "> 44 | 0.29794204235076904 | 0.8422994031830239\n",
      "> 45 | 0.30181440711021423 | 0.8398126657824934\n",
      "> 46 | 0.3459439277648926 | 0.8319379973474801\n",
      "> 47 | 0.3238271474838257 | 0.8318965517241379\n",
      "> 48 | 0.30589622259140015 | 0.8347977453580901\n",
      "> 49 | 0.3129117488861084 | 0.8435013262599469\n",
      "> 50 | 0.30092981457710266 | 0.8457393899204244\n",
      "> 51 | 0.3188168406486511 | 0.8438328912466844\n",
      "> 52 | 0.29805389046669006 | 0.8396883289124668\n",
      "> 53 | 0.3435716927051544 | 0.8323110079575597\n",
      "> 54 | 0.3220714032649994 | 0.8333885941644562\n",
      "> 55 | 0.302541047334671 | 0.8365384615384616\n",
      "> 56 | 0.29643386602401733 | 0.8476873342175066\n",
      "> 57 | 0.3087082505226135 | 0.846112400530504\n",
      "> 58 | 0.32428300380706787 | 0.8324353448275862\n",
      "> 59 | 0.2977660894393921 | 0.8363726790450928\n",
      "> 60 | 0.3009134531021118 | 0.8466511936339522\n",
      "> 61 | 0.29431530833244324 | 0.8428381962864722\n",
      "> 62 | 0.2976570129394531 | 0.8465683023872679\n",
      "> 63 | 0.2983085513114929 | 0.84565649867374\n",
      "> 64 | 0.30042892694473267 | 0.8435427718832891\n",
      "> 65 | 0.3393920660018921 | 0.8323524535809018\n",
      "> 66 | 0.31939905881881714 | 0.8345490716180372\n",
      "> 67 | 0.29687732458114624 | 0.8374088196286472\n",
      "> 68 | 0.2968798279762268 | 0.8465268567639257\n",
      "> 69 | 0.3135567307472229 | 0.8438328912466844\n",
      "> 70 | 0.2954620122909546 | 0.8442887931034483\n",
      "> 71 | 0.3276565372943878 | 0.8328083554376657\n",
      "> 72 | 0.3091464638710022 | 0.8348391909814323\n",
      "> 73 | 0.2928928732872009 | 0.8451591511936339\n",
      "> 74 | 0.3003489375114441 | 0.8469827586206897\n",
      "> 75 | 0.30934834480285645 | 0.839481100795756\n",
      "> 76 | 0.29756978154182434 | 0.8434184350132626\n",
      "> 77 | 0.3147426247596741 | 0.8401856763925729\n",
      "> 78 | 0.2975875437259674 | 0.8395225464190982\n",
      "> 79 | 0.34623077511787415 | 0.8382377320954907\n",
      "> 80 | 0.3217821717262268 | 0.8332642572944298\n",
      "> 81 | 0.30569377541542053 | 0.8369114721485411\n",
      "> 82 | 0.2992869019508362 | 0.8464439655172413\n",
      "> 83 | 0.29311424493789673 | 0.8472314323607427\n",
      "> 84 | 0.2967512905597687 | 0.8430868700265252\n",
      "> 85 | 0.30181884765625 | 0.847770225464191\n",
      "> 86 | 0.3193711042404175 | 0.8435013262599469\n",
      "> 87 | 0.2955193519592285 | 0.840683023872679\n",
      "> 88 | 0.30032116174697876 | 0.8448275862068966\n",
      "> 89 | 0.3168453574180603 | 0.8414290450928382\n",
      "> 90 | 0.2912535071372986 | 0.84184350132626\n",
      "> 91 | 0.3096695840358734 | 0.8405172413793103\n",
      "> 92 | 0.3212658166885376 | 0.8423408488063661\n",
      "> 93 | 0.3035566806793213 | 0.8391495358090186\n",
      "> 94 | 0.32387396693229675 | 0.841387599469496\n",
      "> 95 | 0.29497912526130676 | 0.8415948275862069\n",
      "> 96 | 0.294195294380188 | 0.8443302387267905\n",
      "> 97 | 0.31869834661483765 | 0.8394396551724138\n",
      "> 98 | 0.31293922662734985 | 0.8378647214854111\n",
      "> 99 | 0.31371939182281494 | 0.8442059018567639\n",
      "> 100 | 0.3139076828956604 | 0.8410974801061009\n",
      "> 101 | 0.29659292101860046 | 0.8405172413793103\n",
      "> 102 | 0.2903013229370117 | 0.8470242042440318\n",
      "> 103 | 0.2948981523513794 | 0.8424651856763926\n",
      "> 104 | 0.3046908378601074 | 0.8363312334217506\n",
      "> 105 | 0.309370219707489 | 0.8420092838196287\n",
      "> 106 | 0.306838721036911 | 0.8452420424403183\n",
      "> 107 | 0.31119781732559204 | 0.8448690318302388\n",
      "> 108 | 0.3027680814266205 | 0.8408902519893899\n",
      "> 109 | 0.31408974528312683 | 0.8426724137931034\n",
      "> 110 | 0.3060048222541809 | 0.8433355437665783\n",
      "> 111 | 0.31849950551986694 | 0.8419263925729443\n",
      "> 112 | 0.2956429123878479 | 0.8408902519893899\n",
      "> 113 | 0.33033716678619385 | 0.8327669098143236\n",
      "> 114 | 0.31399327516555786 | 0.8351293103448276\n",
      "> 115 | 0.2936520576477051 | 0.8419678381962865\n",
      "> 116 | 0.30169379711151123 | 0.8426309681697612\n",
      "> 117 | 0.2905847728252411 | 0.84184350132626\n",
      "> 118 | 0.30101943016052246 | 0.8430454244031831\n",
      "> 119 | 0.30757108330726624 | 0.8426724137931034\n",
      "> 120 | 0.29238614439964294 | 0.846816976127321\n",
      "> 121 | 0.28869280219078064 | 0.8453663793103449\n",
      "> 122 | 0.31080812215805054 | 0.8466926392572944\n",
      "> 123 | 0.307739794254303 | 0.8461538461538461\n",
      "> 124 | 0.2936897277832031 | 0.8458222811671088\n",
      "> 125 | 0.29642295837402344 | 0.8445374668435013\n",
      "> 126 | 0.3288978338241577 | 0.8327669098143236\n",
      "> 127 | 0.3131151795387268 | 0.8353365384615384\n",
      "> 128 | 0.3075034022331238 | 0.8436256631299734\n",
      "> 129 | 0.30388763546943665 | 0.8473143236074271\n",
      "> 130 | 0.2929658591747284 | 0.8454907161803713\n",
      "> 131 | 0.3094061315059662 | 0.8451591511936339\n",
      "> 132 | 0.3055885434150696 | 0.8426724137931034\n",
      "> 133 | 0.29462429881095886 | 0.8461952917771883\n",
      "> 134 | 0.31349438428878784 | 0.8466511936339522\n",
      "> 135 | 0.31384462118148804 | 0.8456150530503979\n",
      "> 136 | 0.2928730249404907 | 0.8433769893899205\n",
      "> 137 | 0.28897160291671753 | 0.8464854111405835\n",
      "> 138 | 0.30614805221557617 | 0.8459466180371353\n",
      "> 139 | 0.30117201805114746 | 0.8455321618037135\n",
      "> 140 | 0.311273992061615 | 0.8432112068965517\n",
      "> 141 | 0.30158185958862305 | 0.8473143236074271\n",
      "> 142 | 0.3068285882472992 | 0.8451177055702918\n",
      "> 143 | 0.29306676983833313 | 0.8461952917771883\n",
      "> 144 | 0.2943757176399231 | 0.8449519230769231\n",
      "> 145 | 0.29861894249916077 | 0.8425066312997347\n",
      "> 146 | 0.3254550099372864 | 0.8434598806366047\n",
      "> 147 | 0.29067206382751465 | 0.8364970159151194\n",
      "> 148 | 0.2965541481971741 | 0.8435842175066313\n",
      "> 149 | 0.30458539724349976 | 0.8478531167108754\n",
      "> 150 | 0.3137039542198181 | 0.8420507294429709\n",
      "> 151 | 0.29514080286026 | 0.8457808355437666\n",
      "> 152 | 0.29731637239456177 | 0.8474386604774535\n",
      "> 153 | 0.30459436774253845 | 0.8423822944297082\n",
      "> 154 | 0.28762611746788025 | 0.8447446949602122\n",
      "> 155 | 0.30730047821998596 | 0.8445374668435013\n",
      "> 156 | 0.30867546796798706 | 0.8465268567639257\n",
      "> 157 | 0.2928055226802826 | 0.8469413129973475\n",
      "> 158 | 0.3290896713733673 | 0.8341760610079576\n",
      "> 159 | 0.31565797328948975 | 0.8352950928381963\n",
      "> 160 | 0.3066164255142212 | 0.8434184350132626\n",
      "> 161 | 0.30746763944625854 | 0.8463610742705571\n",
      "> 162 | 0.28670984506607056 | 0.8468998673740054\n",
      "> 163 | 0.29771894216537476 | 0.8487649204244032\n",
      "> 164 | 0.3105890154838562 | 0.8471070954907162\n",
      "> 165 | 0.2842349708080292 | 0.8438328912466844\n",
      "> 166 | 0.3072255253791809 | 0.8483504641909814\n",
      "> 167 | 0.28918564319610596 | 0.8418020557029178\n",
      "> 168 | 0.2961442470550537 | 0.848723474801061\n",
      "> 169 | 0.30724674463272095 | 0.8462367374005305\n",
      "> 170 | 0.2909051179885864 | 0.8483504641909814\n",
      "> 171 | 0.314302921295166 | 0.8458222811671088\n",
      "> 172 | 0.2946743369102478 | 0.8447861405835544\n",
      "> 173 | 0.31339573860168457 | 0.8424237400530504\n",
      "> 174 | 0.31305742263793945 | 0.8449104774535809\n",
      "> 175 | 0.312477707862854 | 0.8447446949602122\n",
      "> 176 | 0.30264416337013245 | 0.8444131299734748\n",
      "> 177 | 0.31280517578125 | 0.8426309681697612\n",
      "> 178 | 0.30281904339790344 | 0.8444131299734748\n",
      "> 179 | 0.31217557191848755 | 0.8426309681697612\n",
      "> 180 | 0.3112425208091736 | 0.8437085543766578\n",
      "> 181 | 0.29478931427001953 | 0.8462367374005305\n",
      "> 182 | 0.31621742248535156 | 0.8361240053050398\n",
      "> 183 | 0.2915831208229065 | 0.8428381962864722\n",
      "> 184 | 0.28419703245162964 | 0.8466926392572944\n",
      "> 185 | 0.2840709090232849 | 0.8432526525198939\n",
      "> 186 | 0.3083409070968628 | 0.8468998673740054\n",
      "> 187 | 0.29382285475730896 | 0.8469413129973475\n",
      "> 188 | 0.29698309302330017 | 0.8452005968169761\n",
      "> 189 | 0.2859639823436737 | 0.8459466180371353\n",
      "> 190 | 0.29584020376205444 | 0.8452005968169761\n",
      "> 191 | 0.2900868356227875 | 0.8466511936339522\n",
      "> 192 | 0.32802659273147583 | 0.8345905172413793\n",
      "> 193 | 0.3134844899177551 | 0.8357924403183024\n",
      "> 194 | 0.30796802043914795 | 0.8439572281167109\n",
      "> 195 | 0.30597299337387085 | 0.8464854111405835\n",
      "> 196 | 0.28557053208351135 | 0.8457808355437666\n",
      "> 197 | 0.283300518989563 | 0.8490135941644562\n",
      "> 198 | 0.28509101271629333 | 0.8500497347480106\n",
      "> 199 | 0.3093264102935791 | 0.8462367374005305\n",
      "> 200 | 0.3040257394313812 | 0.8460295092838196\n",
      "> Class Acc\n",
      "> 0.8297872340425532\n",
      "> DP | DI | DEOPP\n",
      "> 0.7860314697027206 | 0.9142525047063828 | 0.939537525177002\n",
      "> Confusion Matrix \n",
      "TN: 3925.0 | FP: 593.0 \n",
      "FN: 431.0 | TP: 1067.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1565.0 | FP: 102.0 \n",
      "FN: 72.0 | TP: 140.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2360.0 | FP: 491.0 \n",
      "FN: 359.0 | TP: 927.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.3882751166820526 | 0.7657078912466844\n",
      "> 2 | 0.3911265730857849 | 0.8118783156498673\n",
      "> 3 | 0.39347174763679504 | 0.8273789787798409\n",
      "> 4 | 0.3386576473712921 | 0.8274204244031831\n",
      "> 5 | 0.347187876701355 | 0.830487400530504\n",
      "> 6 | 0.33378928899765015 | 0.8265086206896551\n",
      "> 7 | 0.365893691778183 | 0.8252238063660478\n",
      "> 8 | 0.3630523085594177 | 0.8309433023872679\n",
      "> 9 | 0.3241446018218994 | 0.8299900530503979\n",
      "> 10 | 0.42222434282302856 | 0.8318136604774535\n",
      "> 11 | 0.3776119351387024 | 0.8231929708222812\n",
      "> 12 | 0.3099239468574524 | 0.831191976127321\n",
      "> 13 | 0.33585864305496216 | 0.82907824933687\n",
      "> 14 | 0.3018547296524048 | 0.8343832891246684\n",
      "> 15 | 0.37438905239105225 | 0.8275033156498673\n",
      "> 16 | 0.34225133061408997 | 0.8276690981432361\n",
      "> 17 | 0.30041420459747314 | 0.8353365384615384\n",
      "> 18 | 0.36504611372947693 | 0.8284565649867374\n",
      "> 19 | 0.33269864320755005 | 0.8310261936339522\n",
      "> 20 | 0.30380529165267944 | 0.8338444960212201\n",
      "> 21 | 0.30134326219558716 | 0.8400613395225465\n",
      "> 22 | 0.2945544719696045 | 0.8393982095490716\n",
      "> 23 | 0.32687243819236755 | 0.8368285809018567\n",
      "> 24 | 0.3008618950843811 | 0.8356266578249337\n",
      "> 25 | 0.3260415494441986 | 0.8319794429708223\n",
      "> 26 | 0.3010311424732208 | 0.8364970159151194\n",
      "> 27 | 0.33437591791152954 | 0.8378647214854111\n",
      "> 28 | 0.302618145942688 | 0.8359996684350133\n",
      "> 29 | 0.3236303925514221 | 0.8310676392572944\n",
      "> 30 | 0.2973473370075226 | 0.837823275862069\n",
      "> 31 | 0.3027607500553131 | 0.8379061671087533\n",
      "> 32 | 0.3135007321834564 | 0.8398541114058355\n",
      "> 33 | 0.3426706790924072 | 0.8355852122015915\n",
      "> 34 | 0.3323158919811249 | 0.833554376657825\n",
      "> 35 | 0.32630038261413574 | 0.8369943633952255\n",
      "> 36 | 0.3656062185764313 | 0.8350049734748011\n",
      "> 37 | 0.33228468894958496 | 0.8301143899204244\n",
      "> 38 | 0.30611899495124817 | 0.8350464190981433\n",
      "> 39 | 0.30765610933303833 | 0.8407244694960212\n",
      "> 40 | 0.31613990664482117 | 0.8411803713527851\n",
      "> 41 | 0.33766302466392517 | 0.8309433023872679\n",
      "> 42 | 0.3120839297771454 | 0.8344661803713528\n",
      "> 43 | 0.2989777624607086 | 0.8455736074270557\n",
      "> 44 | 0.29794204235076904 | 0.8422994031830239\n",
      "> 45 | 0.30181440711021423 | 0.8398126657824934\n",
      "> 46 | 0.3459439277648926 | 0.8319379973474801\n",
      "> 47 | 0.3238271474838257 | 0.8318965517241379\n",
      "> 48 | 0.30589622259140015 | 0.8347977453580901\n",
      "> 49 | 0.3129117488861084 | 0.8435013262599469\n",
      "> 50 | 0.30092981457710266 | 0.8457393899204244\n",
      "> 51 | 0.3188168406486511 | 0.8438328912466844\n",
      "> 52 | 0.29805389046669006 | 0.8396883289124668\n",
      "> 53 | 0.3435716927051544 | 0.8323110079575597\n",
      "> 54 | 0.3220714032649994 | 0.8333885941644562\n",
      "> 55 | 0.302541047334671 | 0.8365384615384616\n",
      "> 56 | 0.29643386602401733 | 0.8476873342175066\n",
      "> 57 | 0.3087082505226135 | 0.846112400530504\n",
      "> 58 | 0.32428300380706787 | 0.8324353448275862\n",
      "> 59 | 0.2977660894393921 | 0.8363726790450928\n",
      "> 60 | 0.3009134531021118 | 0.8466511936339522\n",
      "> 61 | 0.29431530833244324 | 0.8428381962864722\n",
      "> 62 | 0.2976570129394531 | 0.8465683023872679\n",
      "> 63 | 0.2983085513114929 | 0.84565649867374\n",
      "> 64 | 0.30042892694473267 | 0.8435427718832891\n",
      "> 65 | 0.3393920660018921 | 0.8323524535809018\n",
      "> 66 | 0.31939905881881714 | 0.8345490716180372\n",
      "> 67 | 0.29687732458114624 | 0.8374088196286472\n",
      "> 68 | 0.2968798279762268 | 0.8465268567639257\n",
      "> 69 | 0.3135567307472229 | 0.8438328912466844\n",
      "> 70 | 0.2954620122909546 | 0.8442887931034483\n",
      "> 71 | 0.3276565372943878 | 0.8328083554376657\n",
      "> 72 | 0.3091464638710022 | 0.8348391909814323\n",
      "> 73 | 0.2928928732872009 | 0.8451591511936339\n",
      "> 74 | 0.3003489375114441 | 0.8469827586206897\n",
      "> 75 | 0.30934834480285645 | 0.839481100795756\n",
      "> 76 | 0.29756978154182434 | 0.8434184350132626\n",
      "> 77 | 0.3147426247596741 | 0.8401856763925729\n",
      "> 78 | 0.2975875437259674 | 0.8395225464190982\n",
      "> 79 | 0.34623077511787415 | 0.8382377320954907\n",
      "> 80 | 0.3217821717262268 | 0.8332642572944298\n",
      "> 81 | 0.30569377541542053 | 0.8369114721485411\n",
      "> 82 | 0.2992869019508362 | 0.8464439655172413\n",
      "> 83 | 0.29311424493789673 | 0.8472314323607427\n",
      "> 84 | 0.2967512905597687 | 0.8430868700265252\n",
      "> 85 | 0.30181884765625 | 0.847770225464191\n",
      "> 86 | 0.3193711042404175 | 0.8435013262599469\n",
      "> 87 | 0.2955193519592285 | 0.840683023872679\n",
      "> 88 | 0.30032116174697876 | 0.8448275862068966\n",
      "> 89 | 0.3168453574180603 | 0.8414290450928382\n",
      "> 90 | 0.2912535071372986 | 0.84184350132626\n",
      "> 91 | 0.3096695840358734 | 0.8405172413793103\n",
      "> 92 | 0.3212658166885376 | 0.8423408488063661\n",
      "> 93 | 0.3035566806793213 | 0.8391495358090186\n",
      "> 94 | 0.32387396693229675 | 0.841387599469496\n",
      "> 95 | 0.29497912526130676 | 0.8415948275862069\n",
      "> 96 | 0.294195294380188 | 0.8443302387267905\n",
      "> 97 | 0.31869834661483765 | 0.8394396551724138\n",
      "> 98 | 0.31293922662734985 | 0.8378647214854111\n",
      "> 99 | 0.31371939182281494 | 0.8442059018567639\n",
      "> 100 | 0.3139076828956604 | 0.8410974801061009\n",
      "> 101 | 0.29659292101860046 | 0.8405172413793103\n",
      "> 102 | 0.2903013229370117 | 0.8470242042440318\n",
      "> 103 | 0.2948981523513794 | 0.8424651856763926\n",
      "> 104 | 0.3046908378601074 | 0.8363312334217506\n",
      "> 105 | 0.309370219707489 | 0.8420092838196287\n",
      "> 106 | 0.306838721036911 | 0.8452420424403183\n",
      "> 107 | 0.31119781732559204 | 0.8448690318302388\n",
      "> 108 | 0.3027680814266205 | 0.8408902519893899\n",
      "> 109 | 0.31408974528312683 | 0.8426724137931034\n",
      "> 110 | 0.3060048222541809 | 0.8433355437665783\n",
      "> 111 | 0.31849950551986694 | 0.8419263925729443\n",
      "> 112 | 0.2956429123878479 | 0.8408902519893899\n",
      "> 113 | 0.33033716678619385 | 0.8327669098143236\n",
      "> 114 | 0.31399327516555786 | 0.8351293103448276\n",
      "> 115 | 0.2936520576477051 | 0.8419678381962865\n",
      "> 116 | 0.30169379711151123 | 0.8426309681697612\n",
      "> 117 | 0.2905847728252411 | 0.84184350132626\n",
      "> 118 | 0.30101943016052246 | 0.8430454244031831\n",
      "> 119 | 0.30757108330726624 | 0.8426724137931034\n",
      "> 120 | 0.29238614439964294 | 0.846816976127321\n",
      "> 121 | 0.28869280219078064 | 0.8453663793103449\n",
      "> 122 | 0.31080812215805054 | 0.8466926392572944\n",
      "> 123 | 0.307739794254303 | 0.8461538461538461\n",
      "> 124 | 0.2936897277832031 | 0.8458222811671088\n",
      "> 125 | 0.29642295837402344 | 0.8445374668435013\n",
      "> 126 | 0.3288978338241577 | 0.8327669098143236\n",
      "> 127 | 0.3131151795387268 | 0.8353365384615384\n",
      "> 128 | 0.3075034022331238 | 0.8436256631299734\n",
      "> 129 | 0.30388763546943665 | 0.8473143236074271\n",
      "> 130 | 0.2929658591747284 | 0.8454907161803713\n",
      "> 131 | 0.3094061315059662 | 0.8451591511936339\n",
      "> 132 | 0.3055885434150696 | 0.8426724137931034\n",
      "> 133 | 0.29462429881095886 | 0.8461952917771883\n",
      "> 134 | 0.31349438428878784 | 0.8466511936339522\n",
      "> 135 | 0.31384462118148804 | 0.8456150530503979\n",
      "> 136 | 0.2928730249404907 | 0.8433769893899205\n",
      "> 137 | 0.28897160291671753 | 0.8464854111405835\n",
      "> 138 | 0.30614805221557617 | 0.8459466180371353\n",
      "> 139 | 0.30117201805114746 | 0.8455321618037135\n",
      "> 140 | 0.311273992061615 | 0.8432112068965517\n",
      "> 141 | 0.30158185958862305 | 0.8473143236074271\n",
      "> 142 | 0.3068285882472992 | 0.8451177055702918\n",
      "> 143 | 0.29306676983833313 | 0.8461952917771883\n",
      "> 144 | 0.2943757176399231 | 0.8449519230769231\n",
      "> 145 | 0.29861894249916077 | 0.8425066312997347\n",
      "> 146 | 0.3254550099372864 | 0.8434598806366047\n",
      "> 147 | 0.29067206382751465 | 0.8364970159151194\n",
      "> 148 | 0.2965541481971741 | 0.8435842175066313\n",
      "> 149 | 0.30458539724349976 | 0.8478531167108754\n",
      "> 150 | 0.3137039542198181 | 0.8420507294429709\n",
      "> 151 | 0.29514080286026 | 0.8457808355437666\n",
      "> 152 | 0.29731637239456177 | 0.8474386604774535\n",
      "> 153 | 0.30459436774253845 | 0.8423822944297082\n",
      "> 154 | 0.28762611746788025 | 0.8447446949602122\n",
      "> 155 | 0.30730047821998596 | 0.8445374668435013\n",
      "> 156 | 0.30867546796798706 | 0.8465268567639257\n",
      "> 157 | 0.2928055226802826 | 0.8469413129973475\n",
      "> 158 | 0.3290896713733673 | 0.8341760610079576\n",
      "> 159 | 0.31565797328948975 | 0.8352950928381963\n",
      "> 160 | 0.3066164255142212 | 0.8434184350132626\n",
      "> 161 | 0.30746763944625854 | 0.8463610742705571\n",
      "> 162 | 0.28670984506607056 | 0.8468998673740054\n",
      "> 163 | 0.29771894216537476 | 0.8487649204244032\n",
      "> 164 | 0.3105890154838562 | 0.8471070954907162\n",
      "> 165 | 0.2842349708080292 | 0.8438328912466844\n",
      "> 166 | 0.3072255253791809 | 0.8483504641909814\n",
      "> 167 | 0.28918564319610596 | 0.8418020557029178\n",
      "> 168 | 0.2961442470550537 | 0.848723474801061\n",
      "> 169 | 0.30724674463272095 | 0.8462367374005305\n",
      "> 170 | 0.2909051179885864 | 0.8483504641909814\n",
      "> 171 | 0.314302921295166 | 0.8458222811671088\n",
      "> 172 | 0.2946743369102478 | 0.8447861405835544\n",
      "> 173 | 0.31339573860168457 | 0.8424237400530504\n",
      "> 174 | 0.31305742263793945 | 0.8449104774535809\n",
      "> 175 | 0.312477707862854 | 0.8447446949602122\n",
      "> 176 | 0.30264416337013245 | 0.8444131299734748\n",
      "> 177 | 0.31280517578125 | 0.8426309681697612\n",
      "> 178 | 0.30281904339790344 | 0.8444131299734748\n",
      "> 179 | 0.31217557191848755 | 0.8426309681697612\n",
      "> 180 | 0.3112425208091736 | 0.8437085543766578\n",
      "> 181 | 0.29478931427001953 | 0.8462367374005305\n",
      "> 182 | 0.31621742248535156 | 0.8361240053050398\n",
      "> 183 | 0.2915831208229065 | 0.8428381962864722\n",
      "> 184 | 0.28419703245162964 | 0.8466926392572944\n",
      "> 185 | 0.2840709090232849 | 0.8432526525198939\n",
      "> 186 | 0.3083409070968628 | 0.8468998673740054\n",
      "> 187 | 0.29382285475730896 | 0.8469413129973475\n",
      "> 188 | 0.29698309302330017 | 0.8452005968169761\n",
      "> 189 | 0.2859639823436737 | 0.8459466180371353\n",
      "> 190 | 0.29584020376205444 | 0.8452005968169761\n",
      "> 191 | 0.2900868356227875 | 0.8466511936339522\n",
      "> 192 | 0.32802659273147583 | 0.8345905172413793\n",
      "> 193 | 0.3134844899177551 | 0.8357924403183024\n",
      "> 194 | 0.30796802043914795 | 0.8439572281167109\n",
      "> 195 | 0.30597299337387085 | 0.8464854111405835\n",
      "> 196 | 0.28557053208351135 | 0.8457808355437666\n",
      "> 197 | 0.283300518989563 | 0.8490135941644562\n",
      "> 198 | 0.28509101271629333 | 0.8500497347480106\n",
      "> 199 | 0.3093264102935791 | 0.8462367374005305\n",
      "> 200 | 0.3040257394313812 | 0.8460295092838196\n",
      "> Class Acc\n",
      "> 0.8297872340425532\n",
      "> DP | DI | DEOPP\n",
      "> 0.7860314697027206 | 0.9142525047063828 | 0.939537525177002\n",
      "> Confusion Matrix \n",
      "TN: 3925.0 | FP: 593.0 \n",
      "FN: 431.0 | TP: 1067.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1565.0 | FP: 102.0 \n",
      "FN: 72.0 | TP: 140.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2360.0 | FP: 491.0 \n",
      "FN: 359.0 | TP: 927.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.3882751166820526 | 0.7657078912466844\n",
      "> 2 | 0.3911265730857849 | 0.8118783156498673\n",
      "> 3 | 0.39347174763679504 | 0.8273789787798409\n",
      "> 4 | 0.3386576473712921 | 0.8274204244031831\n",
      "> 5 | 0.347187876701355 | 0.830487400530504\n",
      "> 6 | 0.33378928899765015 | 0.8265086206896551\n",
      "> 7 | 0.365893691778183 | 0.8252238063660478\n",
      "> 8 | 0.3630523085594177 | 0.8309433023872679\n",
      "> 9 | 0.3241446018218994 | 0.8299900530503979\n",
      "> 10 | 0.42222434282302856 | 0.8318136604774535\n",
      "> 11 | 0.3776119351387024 | 0.8231929708222812\n",
      "> 12 | 0.3099239468574524 | 0.831191976127321\n",
      "> 13 | 0.33585864305496216 | 0.82907824933687\n",
      "> 14 | 0.3018547296524048 | 0.8343832891246684\n",
      "> 15 | 0.37438905239105225 | 0.8275033156498673\n",
      "> 16 | 0.34225133061408997 | 0.8276690981432361\n",
      "> 17 | 0.30041420459747314 | 0.8353365384615384\n",
      "> 18 | 0.36504611372947693 | 0.8284565649867374\n",
      "> 19 | 0.33269864320755005 | 0.8310261936339522\n",
      "> 20 | 0.30380529165267944 | 0.8338444960212201\n",
      "> 21 | 0.30134326219558716 | 0.8400613395225465\n",
      "> 22 | 0.2945544719696045 | 0.8393982095490716\n",
      "> 23 | 0.32687243819236755 | 0.8368285809018567\n",
      "> 24 | 0.3008618950843811 | 0.8356266578249337\n",
      "> 25 | 0.3260415494441986 | 0.8319794429708223\n",
      "> 26 | 0.3010311424732208 | 0.8364970159151194\n",
      "> 27 | 0.33437591791152954 | 0.8378647214854111\n",
      "> 28 | 0.302618145942688 | 0.8359996684350133\n",
      "> 29 | 0.3236303925514221 | 0.8310676392572944\n",
      "> 30 | 0.2973473370075226 | 0.837823275862069\n",
      "> 31 | 0.3027607500553131 | 0.8379061671087533\n",
      "> 32 | 0.3135007321834564 | 0.8398541114058355\n",
      "> 33 | 0.3426706790924072 | 0.8355852122015915\n",
      "> 34 | 0.3323158919811249 | 0.833554376657825\n",
      "> 35 | 0.32630038261413574 | 0.8369943633952255\n",
      "> 36 | 0.3656062185764313 | 0.8350049734748011\n",
      "> 37 | 0.33228468894958496 | 0.8301143899204244\n",
      "> 38 | 0.30611899495124817 | 0.8350464190981433\n",
      "> 39 | 0.30765610933303833 | 0.8407244694960212\n",
      "> 40 | 0.31613990664482117 | 0.8411803713527851\n",
      "> 41 | 0.33766302466392517 | 0.8309433023872679\n",
      "> 42 | 0.3120839297771454 | 0.8344661803713528\n",
      "> 43 | 0.2989777624607086 | 0.8455736074270557\n",
      "> 44 | 0.29794204235076904 | 0.8422994031830239\n",
      "> 45 | 0.30181440711021423 | 0.8398126657824934\n",
      "> 46 | 0.3459439277648926 | 0.8319379973474801\n",
      "> 47 | 0.3238271474838257 | 0.8318965517241379\n",
      "> 48 | 0.30589622259140015 | 0.8347977453580901\n",
      "> 49 | 0.3129117488861084 | 0.8435013262599469\n",
      "> 50 | 0.30092981457710266 | 0.8457393899204244\n",
      "> 51 | 0.3188168406486511 | 0.8438328912466844\n",
      "> 52 | 0.29805389046669006 | 0.8396883289124668\n",
      "> 53 | 0.3435716927051544 | 0.8323110079575597\n",
      "> 54 | 0.3220714032649994 | 0.8333885941644562\n",
      "> 55 | 0.302541047334671 | 0.8365384615384616\n",
      "> 56 | 0.29643386602401733 | 0.8476873342175066\n",
      "> 57 | 0.3087082505226135 | 0.846112400530504\n",
      "> 58 | 0.32428300380706787 | 0.8324353448275862\n",
      "> 59 | 0.2977660894393921 | 0.8363726790450928\n",
      "> 60 | 0.3009134531021118 | 0.8466511936339522\n",
      "> 61 | 0.29431530833244324 | 0.8428381962864722\n",
      "> 62 | 0.2976570129394531 | 0.8465683023872679\n",
      "> 63 | 0.2983085513114929 | 0.84565649867374\n",
      "> 64 | 0.30042892694473267 | 0.8435427718832891\n",
      "> 65 | 0.3393920660018921 | 0.8323524535809018\n",
      "> 66 | 0.31939905881881714 | 0.8345490716180372\n",
      "> 67 | 0.29687732458114624 | 0.8374088196286472\n",
      "> 68 | 0.2968798279762268 | 0.8465268567639257\n",
      "> 69 | 0.3135567307472229 | 0.8438328912466844\n",
      "> 70 | 0.2954620122909546 | 0.8442887931034483\n",
      "> 71 | 0.3276565372943878 | 0.8328083554376657\n",
      "> 72 | 0.3091464638710022 | 0.8348391909814323\n",
      "> 73 | 0.2928928732872009 | 0.8451591511936339\n",
      "> 74 | 0.3003489375114441 | 0.8469827586206897\n",
      "> 75 | 0.30934834480285645 | 0.839481100795756\n",
      "> 76 | 0.29756978154182434 | 0.8434184350132626\n",
      "> 77 | 0.3147426247596741 | 0.8401856763925729\n",
      "> 78 | 0.2975875437259674 | 0.8395225464190982\n",
      "> 79 | 0.34623077511787415 | 0.8382377320954907\n",
      "> 80 | 0.3217821717262268 | 0.8332642572944298\n",
      "> 81 | 0.30569377541542053 | 0.8369114721485411\n",
      "> 82 | 0.2992869019508362 | 0.8464439655172413\n",
      "> 83 | 0.29311424493789673 | 0.8472314323607427\n",
      "> 84 | 0.2967512905597687 | 0.8430868700265252\n",
      "> 85 | 0.30181884765625 | 0.847770225464191\n",
      "> 86 | 0.3193711042404175 | 0.8435013262599469\n",
      "> 87 | 0.2955193519592285 | 0.840683023872679\n",
      "> 88 | 0.30032116174697876 | 0.8448275862068966\n",
      "> 89 | 0.3168453574180603 | 0.8414290450928382\n",
      "> 90 | 0.2912535071372986 | 0.84184350132626\n",
      "> 91 | 0.3096695840358734 | 0.8405172413793103\n",
      "> 92 | 0.3212658166885376 | 0.8423408488063661\n",
      "> 93 | 0.3035566806793213 | 0.8391495358090186\n",
      "> 94 | 0.32387396693229675 | 0.841387599469496\n",
      "> 95 | 0.29497912526130676 | 0.8415948275862069\n",
      "> 96 | 0.294195294380188 | 0.8443302387267905\n",
      "> 97 | 0.31869834661483765 | 0.8394396551724138\n",
      "> 98 | 0.31293922662734985 | 0.8378647214854111\n",
      "> 99 | 0.31371939182281494 | 0.8442059018567639\n",
      "> 100 | 0.3139076828956604 | 0.8410974801061009\n",
      "> 101 | 0.29659292101860046 | 0.8405172413793103\n",
      "> 102 | 0.2903013229370117 | 0.8470242042440318\n",
      "> 103 | 0.2948981523513794 | 0.8424651856763926\n",
      "> 104 | 0.3046908378601074 | 0.8363312334217506\n",
      "> 105 | 0.309370219707489 | 0.8420092838196287\n",
      "> 106 | 0.306838721036911 | 0.8452420424403183\n",
      "> 107 | 0.31119781732559204 | 0.8448690318302388\n",
      "> 108 | 0.3027680814266205 | 0.8408902519893899\n",
      "> 109 | 0.31408974528312683 | 0.8426724137931034\n",
      "> 110 | 0.3060048222541809 | 0.8433355437665783\n",
      "> 111 | 0.31849950551986694 | 0.8419263925729443\n",
      "> 112 | 0.2956429123878479 | 0.8408902519893899\n",
      "> 113 | 0.33033716678619385 | 0.8327669098143236\n",
      "> 114 | 0.31399327516555786 | 0.8351293103448276\n",
      "> 115 | 0.2936520576477051 | 0.8419678381962865\n",
      "> 116 | 0.30169379711151123 | 0.8426309681697612\n",
      "> 117 | 0.2905847728252411 | 0.84184350132626\n",
      "> 118 | 0.30101943016052246 | 0.8430454244031831\n",
      "> 119 | 0.30757108330726624 | 0.8426724137931034\n",
      "> 120 | 0.29238614439964294 | 0.846816976127321\n",
      "> 121 | 0.28869280219078064 | 0.8453663793103449\n",
      "> 122 | 0.31080812215805054 | 0.8466926392572944\n",
      "> 123 | 0.307739794254303 | 0.8461538461538461\n",
      "> 124 | 0.2936897277832031 | 0.8458222811671088\n",
      "> 125 | 0.29642295837402344 | 0.8445374668435013\n",
      "> 126 | 0.3288978338241577 | 0.8327669098143236\n",
      "> 127 | 0.3131151795387268 | 0.8353365384615384\n",
      "> 128 | 0.3075034022331238 | 0.8436256631299734\n",
      "> 129 | 0.30388763546943665 | 0.8473143236074271\n",
      "> 130 | 0.2929658591747284 | 0.8454907161803713\n",
      "> 131 | 0.3094061315059662 | 0.8451591511936339\n",
      "> 132 | 0.3055885434150696 | 0.8426724137931034\n",
      "> 133 | 0.29462429881095886 | 0.8461952917771883\n",
      "> 134 | 0.31349438428878784 | 0.8466511936339522\n",
      "> 135 | 0.31384462118148804 | 0.8456150530503979\n",
      "> 136 | 0.2928730249404907 | 0.8433769893899205\n",
      "> 137 | 0.28897160291671753 | 0.8464854111405835\n",
      "> 138 | 0.30614805221557617 | 0.8459466180371353\n",
      "> 139 | 0.30117201805114746 | 0.8455321618037135\n",
      "> 140 | 0.311273992061615 | 0.8432112068965517\n",
      "> 141 | 0.30158185958862305 | 0.8473143236074271\n",
      "> 142 | 0.3068285882472992 | 0.8451177055702918\n",
      "> 143 | 0.29306676983833313 | 0.8461952917771883\n",
      "> 144 | 0.2943757176399231 | 0.8449519230769231\n",
      "> 145 | 0.29861894249916077 | 0.8425066312997347\n",
      "> 146 | 0.3254550099372864 | 0.8434598806366047\n",
      "> 147 | 0.29067206382751465 | 0.8364970159151194\n",
      "> 148 | 0.2965541481971741 | 0.8435842175066313\n",
      "> 149 | 0.30458539724349976 | 0.8478531167108754\n",
      "> 150 | 0.3137039542198181 | 0.8420507294429709\n",
      "> 151 | 0.29514080286026 | 0.8457808355437666\n",
      "> 152 | 0.29731637239456177 | 0.8474386604774535\n",
      "> 153 | 0.30459436774253845 | 0.8423822944297082\n",
      "> 154 | 0.28762611746788025 | 0.8447446949602122\n",
      "> 155 | 0.30730047821998596 | 0.8445374668435013\n",
      "> 156 | 0.30867546796798706 | 0.8465268567639257\n",
      "> 157 | 0.2928055226802826 | 0.8469413129973475\n",
      "> 158 | 0.3290896713733673 | 0.8341760610079576\n",
      "> 159 | 0.31565797328948975 | 0.8352950928381963\n",
      "> 160 | 0.3066164255142212 | 0.8434184350132626\n",
      "> 161 | 0.30746763944625854 | 0.8463610742705571\n",
      "> 162 | 0.28670984506607056 | 0.8468998673740054\n",
      "> 163 | 0.29771894216537476 | 0.8487649204244032\n",
      "> 164 | 0.3105890154838562 | 0.8471070954907162\n",
      "> 165 | 0.2842349708080292 | 0.8438328912466844\n",
      "> 166 | 0.3072255253791809 | 0.8483504641909814\n",
      "> 167 | 0.28918564319610596 | 0.8418020557029178\n",
      "> 168 | 0.2961442470550537 | 0.848723474801061\n",
      "> 169 | 0.30724674463272095 | 0.8462367374005305\n",
      "> 170 | 0.2909051179885864 | 0.8483504641909814\n",
      "> 171 | 0.314302921295166 | 0.8458222811671088\n",
      "> 172 | 0.2946743369102478 | 0.8447861405835544\n",
      "> 173 | 0.31339573860168457 | 0.8424237400530504\n",
      "> 174 | 0.31305742263793945 | 0.8449104774535809\n",
      "> 175 | 0.312477707862854 | 0.8447446949602122\n",
      "> 176 | 0.30264416337013245 | 0.8444131299734748\n",
      "> 177 | 0.31280517578125 | 0.8426309681697612\n",
      "> 178 | 0.30281904339790344 | 0.8444131299734748\n",
      "> 179 | 0.31217557191848755 | 0.8426309681697612\n",
      "> 180 | 0.3112425208091736 | 0.8437085543766578\n",
      "> 181 | 0.29478931427001953 | 0.8462367374005305\n",
      "> 182 | 0.31621742248535156 | 0.8361240053050398\n",
      "> 183 | 0.2915831208229065 | 0.8428381962864722\n",
      "> 184 | 0.28419703245162964 | 0.8466926392572944\n",
      "> 185 | 0.2840709090232849 | 0.8432526525198939\n",
      "> 186 | 0.3083409070968628 | 0.8468998673740054\n",
      "> 187 | 0.29382285475730896 | 0.8469413129973475\n",
      "> 188 | 0.29698309302330017 | 0.8452005968169761\n",
      "> 189 | 0.2859639823436737 | 0.8459466180371353\n",
      "> 190 | 0.29584020376205444 | 0.8452005968169761\n",
      "> 191 | 0.2900868356227875 | 0.8466511936339522\n",
      "> 192 | 0.32802659273147583 | 0.8345905172413793\n",
      "> 193 | 0.3134844899177551 | 0.8357924403183024\n",
      "> 194 | 0.30796802043914795 | 0.8439572281167109\n",
      "> 195 | 0.30597299337387085 | 0.8464854111405835\n",
      "> 196 | 0.28557053208351135 | 0.8457808355437666\n",
      "> 197 | 0.283300518989563 | 0.8490135941644562\n",
      "> 198 | 0.28509101271629333 | 0.8500497347480106\n",
      "> 199 | 0.3093264102935791 | 0.8462367374005305\n",
      "> 200 | 0.3040257394313812 | 0.8460295092838196\n",
      "> Class Acc\n",
      "> 0.8297872340425532\n",
      "> DP | DI | DEOPP\n",
      "> 0.7860314697027206 | 0.9142525047063828 | 0.939537525177002\n",
      "> Confusion Matrix \n",
      "TN: 3925.0 | FP: 593.0 \n",
      "FN: 431.0 | TP: 1067.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1565.0 | FP: 102.0 \n",
      "FN: 72.0 | TP: 140.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2360.0 | FP: 491.0 \n",
      "FN: 359.0 | TP: 927.0\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving into DF then CSV"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "result_df = pd.DataFrame(results, columns=header)\n",
    "result_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  model_name  clas_acc        dp   deqodds    deqopp  trade_dp  trade_deqodds  \\\n",
       "0   UnfairNN  0.829787  0.786031  0.914253  0.939538  0.807317       0.869975   \n",
       "1   UnfairNN  0.829787  0.786031  0.914253  0.939538  0.807317       0.869975   \n",
       "2   UnfairNN  0.829787  0.786031  0.914253  0.939538  0.807317       0.869975   \n",
       "3   UnfairNN  0.829787  0.786031  0.914253  0.939538  0.807317       0.869975   \n",
       "4   UnfairNN  0.829787  0.786031  0.914253  0.939538  0.807317       0.869975   \n",
       "\n",
       "   trade_deqopp   TN_a0  FP_a0  FN_a0  TP_a0   TN_a1  FP_a1  FN_a1  TP_a1  \n",
       "0      0.881259  1565.0  102.0   72.0  140.0  2360.0  491.0  359.0  927.0  \n",
       "1      0.881259  1565.0  102.0   72.0  140.0  2360.0  491.0  359.0  927.0  \n",
       "2      0.881259  1565.0  102.0   72.0  140.0  2360.0  491.0  359.0  927.0  \n",
       "3      0.881259  1565.0  102.0   72.0  140.0  2360.0  491.0  359.0  927.0  \n",
       "4      0.881259  1565.0  102.0   72.0  140.0  2360.0  491.0  359.0  927.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>clas_acc</th>\n",
       "      <th>dp</th>\n",
       "      <th>deqodds</th>\n",
       "      <th>deqopp</th>\n",
       "      <th>trade_dp</th>\n",
       "      <th>trade_deqodds</th>\n",
       "      <th>trade_deqopp</th>\n",
       "      <th>TN_a0</th>\n",
       "      <th>FP_a0</th>\n",
       "      <th>FN_a0</th>\n",
       "      <th>TP_a0</th>\n",
       "      <th>TN_a1</th>\n",
       "      <th>FP_a1</th>\n",
       "      <th>FN_a1</th>\n",
       "      <th>TP_a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UnfairNN</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.786031</td>\n",
       "      <td>0.914253</td>\n",
       "      <td>0.939538</td>\n",
       "      <td>0.807317</td>\n",
       "      <td>0.869975</td>\n",
       "      <td>0.881259</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>927.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UnfairNN</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.786031</td>\n",
       "      <td>0.914253</td>\n",
       "      <td>0.939538</td>\n",
       "      <td>0.807317</td>\n",
       "      <td>0.869975</td>\n",
       "      <td>0.881259</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>927.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UnfairNN</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.786031</td>\n",
       "      <td>0.914253</td>\n",
       "      <td>0.939538</td>\n",
       "      <td>0.807317</td>\n",
       "      <td>0.869975</td>\n",
       "      <td>0.881259</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>927.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UnfairNN</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.786031</td>\n",
       "      <td>0.914253</td>\n",
       "      <td>0.939538</td>\n",
       "      <td>0.807317</td>\n",
       "      <td>0.869975</td>\n",
       "      <td>0.881259</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>927.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UnfairNN</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.786031</td>\n",
       "      <td>0.914253</td>\n",
       "      <td>0.939538</td>\n",
       "      <td>0.807317</td>\n",
       "      <td>0.869975</td>\n",
       "      <td>0.881259</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>927.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "result_df.to_csv('results/validation_unfair_nn-200.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}