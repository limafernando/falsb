{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import libs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from math import sqrt, isnan\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "from util import metrics\n",
    "from util.load_data import load_data\n",
    "from util.evaluation import compute_tradeoff\n",
    "\n",
    "from zhang.models import UnfairLogisticRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "batch_size = 64\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "opt = Adam(learning_rate=lr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "header = \"model_name\", \"clas_acc\", \"dp\", \"deqodds\", \"deqopp\", \"trade_dp\", \"trade_deqodds\", \"trade_deqopp\", \"TN_a0\", \"FP_a0\", \"FN_a0\", \"TP_a0\", \"TN_a1\", \"FP_a1\", \"FN_a1\", \"TP_a1\"\n",
    "results = []\n",
    "\n",
    "test_loop = 5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x_train, y_train, a_train = load_data('adult', 'train')\n",
    "raw_data = (x_train, y_train, a_train)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "xdim = x_train.shape[1]\n",
    "ydim = y_train.shape[1]\n",
    "adim = a_train.shape[1]\n",
    "zdim = 8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "train_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 113), (64, 1), (64, 1)), types: (tf.float64, tf.float64, tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "x_valid, y_valid, a_valid = load_data('adult', 'valid')\n",
    "\n",
    "valid_data = Dataset.from_tensor_slices((x_valid, y_valid, a_valid))\n",
    "valid_data = valid_data.batch(batch_size, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "x_test, y_test, a_test = load_data('adult', 'test')\n",
    "\n",
    "test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "test_data = test_data.batch(batch_size, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def train(model, X, Y, A, optimizer):\n",
    "    clas_vars = [model.clas.W, model.b]\n",
    "    \n",
    "    with tf.GradientTape() as clas_tape:\n",
    "        \n",
    "        model(X, Y, A)        \n",
    "        clas_loss = model.clas_loss\n",
    "\n",
    "    dWLp = clas_tape.gradient(clas_loss, clas_vars)\n",
    "    optimizer.apply_gradients(zip(dWLp, clas_vars))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def train_loop(model, train_dataset, epochs, opt=None):\n",
    "    \n",
    "    print(\"> Epoch | Class Loss | Class Acc\")\n",
    "\n",
    "    if opt is not None:\n",
    "        optimizer = opt\n",
    "        decay4epoch = False\n",
    "    else:\n",
    "        decay4epoch = True\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        Y_hat = None\n",
    "        batch_count = 1\n",
    "\n",
    "        if decay4epoch:\n",
    "            lr = 0.001/(epoch+1)\n",
    "            optimizer = Adam(learning_rate=lr)\n",
    "        \n",
    "        for X, Y, A in train_dataset:\n",
    "            \n",
    "            r = train(model, X, Y, A, optimizer)\n",
    "            if r:\n",
    "                print('parou')\n",
    "                print(model.clas_loss)\n",
    "                break\n",
    "\n",
    "            if batch_count == 1:\n",
    "                Y_hat = model.Y_hat\n",
    "                batch_count += 1\n",
    "            else:\n",
    "                Y_hat = tf.concat([Y_hat, model.Y_hat], 0)\n",
    "\n",
    "        clas_loss = model.clas_loss\n",
    "        clas_acc = metrics.accuracy(raw_data[1], tf.math.round(Y_hat))\n",
    "\n",
    "        \n",
    "    \n",
    "        print(\"> {} | {} | {}\".format(\n",
    "            epoch+1, \n",
    "            clas_loss, \n",
    "            clas_acc))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "\n",
    "def evaluation(model, valid_data):\n",
    "    Y_hat = None\n",
    "    batch_count = 1\n",
    "    \n",
    "    for X, Y, A in valid_data:\n",
    "        \n",
    "        model(X, Y, A)\n",
    "        \n",
    "        if batch_count == 1:\n",
    "            Y_hat = model.Y_hat\n",
    "            batch_count += 1\n",
    "        else:\n",
    "            Y_hat = tf.concat([Y_hat, model.Y_hat], 0)\n",
    "    \n",
    "    return Y_hat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def compute_metrics(Y, Y_hat, A):\n",
    "    Y_hat = tf.math.round(Y_hat)\n",
    "    \n",
    "    clas_acc = metrics.accuracy(Y, Y_hat)\n",
    "\n",
    "    print(\"> Class Acc\")\n",
    "    print(\"> {}\".format(clas_acc))\n",
    "\n",
    "    dp = metrics.DP(Y_hat.numpy(), A)\n",
    "    deqodds = metrics.DEqOdds(Y, Y_hat.numpy(), A)\n",
    "    deqopp = metrics.DEqOpp(Y, Y_hat.numpy(), A)\n",
    "\n",
    "    print(\"> DP | DI | DEOPP\")\n",
    "    print(\"> {} | {} | {}\".format(dp, deqodds, deqopp))\n",
    "\n",
    "    tp = metrics.TP(Y, Y_hat.numpy())\n",
    "    tn = metrics.TN(Y, Y_hat.numpy())\n",
    "    fp = metrics.FP(Y, Y_hat.numpy())\n",
    "    fn = metrics.FN(Y, Y_hat.numpy())\n",
    "\n",
    "    print('> Confusion Matrix \\n' +\n",
    "                'TN: {} | FP: {} \\n'.format(tn, fp) +\n",
    "                'FN: {} | TP: {}'.format(fn, tp))\n",
    "\n",
    "    confusion_matrix = np.array([[tn, fp],\n",
    "                                [fn, tp]])\n",
    "\n",
    "    m = [metrics.TN, metrics.FP, metrics.FN, metrics.TP]\n",
    "    metrics_a0 = [0, 0, 0, 0]\n",
    "    metrics_a1 = [0, 0, 0, 0]\n",
    "    for i in range(len(m)):\n",
    "        metrics_a0[i] = metrics.subgroup(m[i], A, Y, Y_hat.numpy())\n",
    "        metrics_a1[i] = metrics.subgroup(m[i], 1 - A, Y, Y_hat.numpy())\n",
    "\n",
    "    print('> Confusion Matrix for A = 0 \\n' +\n",
    "            'TN: {} | FP: {} \\n'.format(metrics_a0[0], metrics_a0[1]) +\n",
    "            'FN: {} | TP: {}'.format(metrics_a0[2], metrics_a0[3]))\n",
    "\n",
    "    print('> Confusion Matrix for A = 1 \\n' +\n",
    "            'TN: {} | FP: {} \\n'.format(metrics_a1[0], metrics_a1[1]) +\n",
    "            'FN: {} | TP: {}'.format(metrics_a1[2], metrics_a1[3]))\n",
    "\n",
    "    confusion_matrix = np.array([[tn, fp],\n",
    "                                [fn, tp]])\n",
    "\n",
    "    return clas_acc, confusion_matrix, dp, deqodds, deqopp, metrics_a0, metrics_a1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "for i in range(test_loop):\n",
    "\n",
    "    model = UnfairLogisticRegression(xdim, batch_size)\n",
    "    ret = train_loop(model, train_data, epochs)\n",
    "    Y_hat = evaluation(model, valid_data)\n",
    "    \n",
    "    clas_acc, confusion_matrix, dp, deqodds, deqopp, metrics_a0, metrics_a1  = compute_metrics(y_valid, Y_hat, a_valid)\n",
    "    \n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    \n",
    "    tradeoff = []\n",
    "    \n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "    \n",
    "    result = ['UnfairLR-decay', clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5004817843437195 | 0.7487566312997347\n",
      "> 2 | 0.4250468909740448 | 0.7940981432360743\n",
      "> 3 | 0.38422220945358276 | 0.8143236074270557\n",
      "> 4 | 0.36236947774887085 | 0.8235245358090186\n",
      "> 5 | 0.35435330867767334 | 0.8309433023872679\n",
      "> 6 | 0.3480228781700134 | 0.8362068965517242\n",
      "> 7 | 0.3438219428062439 | 0.8391495358090186\n",
      "> 8 | 0.34053272008895874 | 0.8399370026525199\n",
      "> 9 | 0.33758699893951416 | 0.8413461538461539\n",
      "> 10 | 0.3348346948623657 | 0.8418849469496021\n",
      "> 11 | 0.33249279856681824 | 0.8425895225464191\n",
      "> 12 | 0.3304925858974457 | 0.8430039787798409\n",
      "> 13 | 0.3288639783859253 | 0.8434184350132626\n",
      "> 14 | 0.3274797797203064 | 0.8440401193633953\n",
      "> 15 | 0.32632455229759216 | 0.8445789124668435\n",
      "> 16 | 0.3253505527973175 | 0.8450348143236074\n",
      "> 17 | 0.32451677322387695 | 0.8451591511936339\n",
      "> 18 | 0.3237929940223694 | 0.8454078249336869\n",
      "> 19 | 0.323161780834198 | 0.8455736074270557\n",
      "> 20 | 0.32261425256729126 | 0.8460709549071618\n",
      "> 21 | 0.3222368061542511 | 0.8459880636604775\n",
      "> 22 | 0.3218652009963989 | 0.8459466180371353\n",
      "> 23 | 0.32156968116760254 | 0.8460295092838196\n",
      "> 24 | 0.32134681940078735 | 0.8460709549071618\n",
      "> 25 | 0.3213326334953308 | 0.8464439655172413\n",
      "> 26 | 0.32126009464263916 | 0.8466511936339522\n",
      "> 27 | 0.3213502764701843 | 0.8467755305039788\n",
      "> 28 | 0.3213960528373718 | 0.846816976127321\n",
      "> 29 | 0.32156500220298767 | 0.8468584217506632\n",
      "> 30 | 0.3217525780200958 | 0.8467755305039788\n",
      "> 31 | 0.32191404700279236 | 0.8468998673740054\n",
      "> 32 | 0.3221426010131836 | 0.8468584217506632\n",
      "> 33 | 0.3223761320114136 | 0.8469827586206897\n",
      "> 34 | 0.3226112723350525 | 0.8471070954907162\n",
      "> 35 | 0.3228435218334198 | 0.8472728779840849\n",
      "> 36 | 0.32306960225105286 | 0.8472314323607427\n",
      "> 37 | 0.3232862055301666 | 0.8475629973474801\n",
      "> 38 | 0.3235009014606476 | 0.8476458885941645\n",
      "> 39 | 0.32368916273117065 | 0.847770225464191\n",
      "> 40 | 0.32386183738708496 | 0.8478116710875332\n",
      "> 41 | 0.3240176737308502 | 0.8481432360742706\n",
      "> 42 | 0.3241559863090515 | 0.8481846816976127\n",
      "> 43 | 0.32427653670310974 | 0.8481017904509284\n",
      "> 44 | 0.32437941431999207 | 0.8480603448275862\n",
      "> 45 | 0.32446491718292236 | 0.8482261273209549\n",
      "> 46 | 0.32453370094299316 | 0.8485162466843501\n",
      "> 47 | 0.32458627223968506 | 0.8486820291777188\n",
      "> 48 | 0.3246217370033264 | 0.8486405835543767\n",
      "> 49 | 0.32464492321014404 | 0.8490135941644562\n",
      "> 50 | 0.3246545195579529 | 0.848930702917772\n",
      "> Class Acc\n",
      "> 0.847406914893617\n",
      "> DP | DI | DEOPP\n",
      "> 0.8008753657341003 | 0.8773041144013405 | 0.8418380618095398\n",
      "> Confusion Matrix \n",
      "TN: 4188.0 | FP: 330.0 \n",
      "FN: 588.0 | TP: 910.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1637.0 | FP: 30.0 \n",
      "FN: 112.0 | TP: 100.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2551.0 | FP: 300.0 \n",
      "FN: 476.0 | TP: 810.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5004817843437195 | 0.7487566312997347\n",
      "> 2 | 0.4250468909740448 | 0.7940981432360743\n",
      "> 3 | 0.38422220945358276 | 0.8143236074270557\n",
      "> 4 | 0.36236947774887085 | 0.8235245358090186\n",
      "> 5 | 0.35435330867767334 | 0.8309433023872679\n",
      "> 6 | 0.3480228781700134 | 0.8362068965517242\n",
      "> 7 | 0.3438219428062439 | 0.8391495358090186\n",
      "> 8 | 0.34053272008895874 | 0.8399370026525199\n",
      "> 9 | 0.33758699893951416 | 0.8413461538461539\n",
      "> 10 | 0.3348346948623657 | 0.8418849469496021\n",
      "> 11 | 0.33249279856681824 | 0.8425895225464191\n",
      "> 12 | 0.3304925858974457 | 0.8430039787798409\n",
      "> 13 | 0.3288639783859253 | 0.8434184350132626\n",
      "> 14 | 0.3274797797203064 | 0.8440401193633953\n",
      "> 15 | 0.32632455229759216 | 0.8445789124668435\n",
      "> 16 | 0.3253505527973175 | 0.8450348143236074\n",
      "> 17 | 0.32451677322387695 | 0.8451591511936339\n",
      "> 18 | 0.3237929940223694 | 0.8454078249336869\n",
      "> 19 | 0.323161780834198 | 0.8455736074270557\n",
      "> 20 | 0.32261425256729126 | 0.8460709549071618\n",
      "> 21 | 0.3222368061542511 | 0.8459880636604775\n",
      "> 22 | 0.3218652009963989 | 0.8459466180371353\n",
      "> 23 | 0.32156968116760254 | 0.8460295092838196\n",
      "> 24 | 0.32134681940078735 | 0.8460709549071618\n",
      "> 25 | 0.3213326334953308 | 0.8464439655172413\n",
      "> 26 | 0.32126009464263916 | 0.8466511936339522\n",
      "> 27 | 0.3213502764701843 | 0.8467755305039788\n",
      "> 28 | 0.3213960528373718 | 0.846816976127321\n",
      "> 29 | 0.32156500220298767 | 0.8468584217506632\n",
      "> 30 | 0.3217525780200958 | 0.8467755305039788\n",
      "> 31 | 0.32191404700279236 | 0.8468998673740054\n",
      "> 32 | 0.3221426010131836 | 0.8468584217506632\n",
      "> 33 | 0.3223761320114136 | 0.8469827586206897\n",
      "> 34 | 0.3226112723350525 | 0.8471070954907162\n",
      "> 35 | 0.3228435218334198 | 0.8472728779840849\n",
      "> 36 | 0.32306960225105286 | 0.8472314323607427\n",
      "> 37 | 0.3232862055301666 | 0.8475629973474801\n",
      "> 38 | 0.3235009014606476 | 0.8476458885941645\n",
      "> 39 | 0.32368916273117065 | 0.847770225464191\n",
      "> 40 | 0.32386183738708496 | 0.8478116710875332\n",
      "> 41 | 0.3240176737308502 | 0.8481432360742706\n",
      "> 42 | 0.3241559863090515 | 0.8481846816976127\n",
      "> 43 | 0.32427653670310974 | 0.8481017904509284\n",
      "> 44 | 0.32437941431999207 | 0.8480603448275862\n",
      "> 45 | 0.32446491718292236 | 0.8482261273209549\n",
      "> 46 | 0.32453370094299316 | 0.8485162466843501\n",
      "> 47 | 0.32458627223968506 | 0.8486820291777188\n",
      "> 48 | 0.3246217370033264 | 0.8486405835543767\n",
      "> 49 | 0.32464492321014404 | 0.8490135941644562\n",
      "> 50 | 0.3246545195579529 | 0.848930702917772\n",
      "> Class Acc\n",
      "> 0.847406914893617\n",
      "> DP | DI | DEOPP\n",
      "> 0.8008753657341003 | 0.8773041144013405 | 0.8418380618095398\n",
      "> Confusion Matrix \n",
      "TN: 4188.0 | FP: 330.0 \n",
      "FN: 588.0 | TP: 910.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1637.0 | FP: 30.0 \n",
      "FN: 112.0 | TP: 100.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2551.0 | FP: 300.0 \n",
      "FN: 476.0 | TP: 810.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5004817843437195 | 0.7487566312997347\n",
      "> 2 | 0.4250468909740448 | 0.7940981432360743\n",
      "> 3 | 0.38422220945358276 | 0.8143236074270557\n",
      "> 4 | 0.36236947774887085 | 0.8235245358090186\n",
      "> 5 | 0.35435330867767334 | 0.8309433023872679\n",
      "> 6 | 0.3480228781700134 | 0.8362068965517242\n",
      "> 7 | 0.3438219428062439 | 0.8391495358090186\n",
      "> 8 | 0.34053272008895874 | 0.8399370026525199\n",
      "> 9 | 0.33758699893951416 | 0.8413461538461539\n",
      "> 10 | 0.3348346948623657 | 0.8418849469496021\n",
      "> 11 | 0.33249279856681824 | 0.8425895225464191\n",
      "> 12 | 0.3304925858974457 | 0.8430039787798409\n",
      "> 13 | 0.3288639783859253 | 0.8434184350132626\n",
      "> 14 | 0.3274797797203064 | 0.8440401193633953\n",
      "> 15 | 0.32632455229759216 | 0.8445789124668435\n",
      "> 16 | 0.3253505527973175 | 0.8450348143236074\n",
      "> 17 | 0.32451677322387695 | 0.8451591511936339\n",
      "> 18 | 0.3237929940223694 | 0.8454078249336869\n",
      "> 19 | 0.323161780834198 | 0.8455736074270557\n",
      "> 20 | 0.32261425256729126 | 0.8460709549071618\n",
      "> 21 | 0.3222368061542511 | 0.8459880636604775\n",
      "> 22 | 0.3218652009963989 | 0.8459466180371353\n",
      "> 23 | 0.32156968116760254 | 0.8460295092838196\n",
      "> 24 | 0.32134681940078735 | 0.8460709549071618\n",
      "> 25 | 0.3213326334953308 | 0.8464439655172413\n",
      "> 26 | 0.32126009464263916 | 0.8466511936339522\n",
      "> 27 | 0.3213502764701843 | 0.8467755305039788\n",
      "> 28 | 0.3213960528373718 | 0.846816976127321\n",
      "> 29 | 0.32156500220298767 | 0.8468584217506632\n",
      "> 30 | 0.3217525780200958 | 0.8467755305039788\n",
      "> 31 | 0.32191404700279236 | 0.8468998673740054\n",
      "> 32 | 0.3221426010131836 | 0.8468584217506632\n",
      "> 33 | 0.3223761320114136 | 0.8469827586206897\n",
      "> 34 | 0.3226112723350525 | 0.8471070954907162\n",
      "> 35 | 0.3228435218334198 | 0.8472728779840849\n",
      "> 36 | 0.32306960225105286 | 0.8472314323607427\n",
      "> 37 | 0.3232862055301666 | 0.8475629973474801\n",
      "> 38 | 0.3235009014606476 | 0.8476458885941645\n",
      "> 39 | 0.32368916273117065 | 0.847770225464191\n",
      "> 40 | 0.32386183738708496 | 0.8478116710875332\n",
      "> 41 | 0.3240176737308502 | 0.8481432360742706\n",
      "> 42 | 0.3241559863090515 | 0.8481846816976127\n",
      "> 43 | 0.32427653670310974 | 0.8481017904509284\n",
      "> 44 | 0.32437941431999207 | 0.8480603448275862\n",
      "> 45 | 0.32446491718292236 | 0.8482261273209549\n",
      "> 46 | 0.32453370094299316 | 0.8485162466843501\n",
      "> 47 | 0.32458627223968506 | 0.8486820291777188\n",
      "> 48 | 0.3246217370033264 | 0.8486405835543767\n",
      "> 49 | 0.32464492321014404 | 0.8490135941644562\n",
      "> 50 | 0.3246545195579529 | 0.848930702917772\n",
      "> Class Acc\n",
      "> 0.847406914893617\n",
      "> DP | DI | DEOPP\n",
      "> 0.8008753657341003 | 0.8773041144013405 | 0.8418380618095398\n",
      "> Confusion Matrix \n",
      "TN: 4188.0 | FP: 330.0 \n",
      "FN: 588.0 | TP: 910.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1637.0 | FP: 30.0 \n",
      "FN: 112.0 | TP: 100.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2551.0 | FP: 300.0 \n",
      "FN: 476.0 | TP: 810.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5004817843437195 | 0.7487566312997347\n",
      "> 2 | 0.4250468909740448 | 0.7940981432360743\n",
      "> 3 | 0.38422220945358276 | 0.8143236074270557\n",
      "> 4 | 0.36236947774887085 | 0.8235245358090186\n",
      "> 5 | 0.35435330867767334 | 0.8309433023872679\n",
      "> 6 | 0.3480228781700134 | 0.8362068965517242\n",
      "> 7 | 0.3438219428062439 | 0.8391495358090186\n",
      "> 8 | 0.34053272008895874 | 0.8399370026525199\n",
      "> 9 | 0.33758699893951416 | 0.8413461538461539\n",
      "> 10 | 0.3348346948623657 | 0.8418849469496021\n",
      "> 11 | 0.33249279856681824 | 0.8425895225464191\n",
      "> 12 | 0.3304925858974457 | 0.8430039787798409\n",
      "> 13 | 0.3288639783859253 | 0.8434184350132626\n",
      "> 14 | 0.3274797797203064 | 0.8440401193633953\n",
      "> 15 | 0.32632455229759216 | 0.8445789124668435\n",
      "> 16 | 0.3253505527973175 | 0.8450348143236074\n",
      "> 17 | 0.32451677322387695 | 0.8451591511936339\n",
      "> 18 | 0.3237929940223694 | 0.8454078249336869\n",
      "> 19 | 0.323161780834198 | 0.8455736074270557\n",
      "> 20 | 0.32261425256729126 | 0.8460709549071618\n",
      "> 21 | 0.3222368061542511 | 0.8459880636604775\n",
      "> 22 | 0.3218652009963989 | 0.8459466180371353\n",
      "> 23 | 0.32156968116760254 | 0.8460295092838196\n",
      "> 24 | 0.32134681940078735 | 0.8460709549071618\n",
      "> 25 | 0.3213326334953308 | 0.8464439655172413\n",
      "> 26 | 0.32126009464263916 | 0.8466511936339522\n",
      "> 27 | 0.3213502764701843 | 0.8467755305039788\n",
      "> 28 | 0.3213960528373718 | 0.846816976127321\n",
      "> 29 | 0.32156500220298767 | 0.8468584217506632\n",
      "> 30 | 0.3217525780200958 | 0.8467755305039788\n",
      "> 31 | 0.32191404700279236 | 0.8468998673740054\n",
      "> 32 | 0.3221426010131836 | 0.8468584217506632\n",
      "> 33 | 0.3223761320114136 | 0.8469827586206897\n",
      "> 34 | 0.3226112723350525 | 0.8471070954907162\n",
      "> 35 | 0.3228435218334198 | 0.8472728779840849\n",
      "> 36 | 0.32306960225105286 | 0.8472314323607427\n",
      "> 37 | 0.3232862055301666 | 0.8475629973474801\n",
      "> 38 | 0.3235009014606476 | 0.8476458885941645\n",
      "> 39 | 0.32368916273117065 | 0.847770225464191\n",
      "> 40 | 0.32386183738708496 | 0.8478116710875332\n",
      "> 41 | 0.3240176737308502 | 0.8481432360742706\n",
      "> 42 | 0.3241559863090515 | 0.8481846816976127\n",
      "> 43 | 0.32427653670310974 | 0.8481017904509284\n",
      "> 44 | 0.32437941431999207 | 0.8480603448275862\n",
      "> 45 | 0.32446491718292236 | 0.8482261273209549\n",
      "> 46 | 0.32453370094299316 | 0.8485162466843501\n",
      "> 47 | 0.32458627223968506 | 0.8486820291777188\n",
      "> 48 | 0.3246217370033264 | 0.8486405835543767\n",
      "> 49 | 0.32464492321014404 | 0.8490135941644562\n",
      "> 50 | 0.3246545195579529 | 0.848930702917772\n",
      "> Class Acc\n",
      "> 0.847406914893617\n",
      "> DP | DI | DEOPP\n",
      "> 0.8008753657341003 | 0.8773041144013405 | 0.8418380618095398\n",
      "> Confusion Matrix \n",
      "TN: 4188.0 | FP: 330.0 \n",
      "FN: 588.0 | TP: 910.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1637.0 | FP: 30.0 \n",
      "FN: 112.0 | TP: 100.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2551.0 | FP: 300.0 \n",
      "FN: 476.0 | TP: 810.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5004817843437195 | 0.7487566312997347\n",
      "> 2 | 0.4250468909740448 | 0.7940981432360743\n",
      "> 3 | 0.38422220945358276 | 0.8143236074270557\n",
      "> 4 | 0.36236947774887085 | 0.8235245358090186\n",
      "> 5 | 0.35435330867767334 | 0.8309433023872679\n",
      "> 6 | 0.3480228781700134 | 0.8362068965517242\n",
      "> 7 | 0.3438219428062439 | 0.8391495358090186\n",
      "> 8 | 0.34053272008895874 | 0.8399370026525199\n",
      "> 9 | 0.33758699893951416 | 0.8413461538461539\n",
      "> 10 | 0.3348346948623657 | 0.8418849469496021\n",
      "> 11 | 0.33249279856681824 | 0.8425895225464191\n",
      "> 12 | 0.3304925858974457 | 0.8430039787798409\n",
      "> 13 | 0.3288639783859253 | 0.8434184350132626\n",
      "> 14 | 0.3274797797203064 | 0.8440401193633953\n",
      "> 15 | 0.32632455229759216 | 0.8445789124668435\n",
      "> 16 | 0.3253505527973175 | 0.8450348143236074\n",
      "> 17 | 0.32451677322387695 | 0.8451591511936339\n",
      "> 18 | 0.3237929940223694 | 0.8454078249336869\n",
      "> 19 | 0.323161780834198 | 0.8455736074270557\n",
      "> 20 | 0.32261425256729126 | 0.8460709549071618\n",
      "> 21 | 0.3222368061542511 | 0.8459880636604775\n",
      "> 22 | 0.3218652009963989 | 0.8459466180371353\n",
      "> 23 | 0.32156968116760254 | 0.8460295092838196\n",
      "> 24 | 0.32134681940078735 | 0.8460709549071618\n",
      "> 25 | 0.3213326334953308 | 0.8464439655172413\n",
      "> 26 | 0.32126009464263916 | 0.8466511936339522\n",
      "> 27 | 0.3213502764701843 | 0.8467755305039788\n",
      "> 28 | 0.3213960528373718 | 0.846816976127321\n",
      "> 29 | 0.32156500220298767 | 0.8468584217506632\n",
      "> 30 | 0.3217525780200958 | 0.8467755305039788\n",
      "> 31 | 0.32191404700279236 | 0.8468998673740054\n",
      "> 32 | 0.3221426010131836 | 0.8468584217506632\n",
      "> 33 | 0.3223761320114136 | 0.8469827586206897\n",
      "> 34 | 0.3226112723350525 | 0.8471070954907162\n",
      "> 35 | 0.3228435218334198 | 0.8472728779840849\n",
      "> 36 | 0.32306960225105286 | 0.8472314323607427\n",
      "> 37 | 0.3232862055301666 | 0.8475629973474801\n",
      "> 38 | 0.3235009014606476 | 0.8476458885941645\n",
      "> 39 | 0.32368916273117065 | 0.847770225464191\n",
      "> 40 | 0.32386183738708496 | 0.8478116710875332\n",
      "> 41 | 0.3240176737308502 | 0.8481432360742706\n",
      "> 42 | 0.3241559863090515 | 0.8481846816976127\n",
      "> 43 | 0.32427653670310974 | 0.8481017904509284\n",
      "> 44 | 0.32437941431999207 | 0.8480603448275862\n",
      "> 45 | 0.32446491718292236 | 0.8482261273209549\n",
      "> 46 | 0.32453370094299316 | 0.8485162466843501\n",
      "> 47 | 0.32458627223968506 | 0.8486820291777188\n",
      "> 48 | 0.3246217370033264 | 0.8486405835543767\n",
      "> 49 | 0.32464492321014404 | 0.8490135941644562\n",
      "> 50 | 0.3246545195579529 | 0.848930702917772\n",
      "> Class Acc\n",
      "> 0.847406914893617\n",
      "> DP | DI | DEOPP\n",
      "> 0.8008753657341003 | 0.8773041144013405 | 0.8418380618095398\n",
      "> Confusion Matrix \n",
      "TN: 4188.0 | FP: 330.0 \n",
      "FN: 588.0 | TP: 910.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1637.0 | FP: 30.0 \n",
      "FN: 112.0 | TP: 100.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2551.0 | FP: 300.0 \n",
      "FN: 476.0 | TP: 810.0\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sns.heatmap(confusion_matrix, annot=True, fmt='g')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4f9bb608e0>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "for i in range(test_loop):\n",
    "    opt = Adam(learning_rate=lr)\n",
    "\n",
    "    model = UnfairLogisticRegression(xdim, batch_size)\n",
    "    ret = train_loop(model, train_data, epochs, opt)\n",
    "    Y_hat = evaluation(model, valid_data)\n",
    "    \n",
    "    clas_acc, confusion_matrix, dp, deqodds, deqopp, metrics_a0, metrics_a1  = compute_metrics(y_valid, Y_hat, a_valid)\n",
    "    \n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    \n",
    "    tradeoff = []\n",
    "    \n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "    \n",
    "    result = ['UnfairLR', clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5004817843437195 | 0.7487566312997347\n",
      "> 2 | 0.43046826124191284 | 0.7967092175066313\n",
      "> 3 | 0.4265057444572449 | 0.8164787798408488\n",
      "> 4 | 0.3977872431278229 | 0.828125\n",
      "> 5 | 0.3788052797317505 | 0.8338030503978779\n",
      "> 6 | 0.3722444176673889 | 0.8389837533156499\n",
      "> 7 | 0.35932254791259766 | 0.8396468832891246\n",
      "> 8 | 0.365489661693573 | 0.8395639920424403\n",
      "> 9 | 0.35043689608573914 | 0.841387599469496\n",
      "> 10 | 0.35543712973594666 | 0.8425480769230769\n",
      "> 11 | 0.34435534477233887 | 0.8445374668435013\n",
      "> 12 | 0.9974395036697388 | 0.8433355437665783\n",
      "> 13 | 0.33965668082237244 | 0.839729774535809\n",
      "> 14 | 0.531301736831665 | 0.8430868700265252\n",
      "> 15 | 0.3296221196651459 | 0.8480603448275862\n",
      "> 16 | 0.9852874875068665 | 0.8441644562334217\n",
      "> 17 | 0.9825006127357483 | 0.8331399204244032\n",
      "> 18 | 0.33471840620040894 | 0.8358338859416445\n",
      "> 19 | 0.32079070806503296 | 0.8455321618037135\n",
      "> 20 | 0.31524109840393066 | 0.8468998673740054\n",
      "> 21 | 0.3298489451408386 | 0.8467755305039788\n",
      "> 22 | 0.33171743154525757 | 0.847065649867374\n",
      "> 23 | 0.3064884841442108 | 0.8462781830238727\n",
      "> 24 | 0.30767932534217834 | 0.846816976127321\n",
      "> 25 | 0.3207545280456543 | 0.8449519230769231\n",
      "> 26 | 0.3529583513736725 | 0.8448690318302388\n",
      "> 27 | 0.3010445237159729 | 0.8455736074270557\n",
      "> 28 | 0.30363672971725464 | 0.8446618037135278\n",
      "> 29 | 0.37016624212265015 | 0.8454078249336869\n",
      "> 30 | 0.36420905590057373 | 0.8449519230769231\n",
      "> 31 | 0.3649325966835022 | 0.8450762599469496\n",
      "> 32 | 0.3665430247783661 | 0.8449104774535809\n",
      "> 33 | 0.36618772149086 | 0.8447861405835544\n",
      "> 34 | 0.3656221032142639 | 0.8447446949602122\n",
      "> 35 | 0.36475372314453125 | 0.84470324933687\n",
      "> 36 | 0.36340659856796265 | 0.8447861405835544\n",
      "> 37 | 0.3622782528400421 | 0.8448690318302388\n",
      "> 38 | 0.35984015464782715 | 0.8450348143236074\n",
      "> 39 | 0.3587883412837982 | 0.8452834880636605\n",
      "> 40 | 0.6900540590286255 | 0.8452005968169761\n",
      "> 41 | 0.6427038311958313 | 0.8457393899204244\n",
      "> 42 | 0.55645751953125 | 0.8443302387267905\n",
      "> 43 | 0.5696516036987305 | 0.8460709549071618\n",
      "> 44 | 0.5468385815620422 | 0.8456979442970822\n",
      "> 45 | 0.570908784866333 | 0.845863726790451\n",
      "> 46 | 0.5315980315208435 | 0.84565649867374\n",
      "> 47 | 0.5473337173461914 | 0.8454907161803713\n",
      "> 48 | 0.29311901330947876 | 0.8459466180371353\n",
      "> 49 | 0.3044631779193878 | 0.8473972148541113\n",
      "> 50 | 0.9594934582710266 | 0.8436671087533156\n",
      "> Class Acc\n",
      "> 0.8159906914893618\n",
      "> DP | DI | DEOPP\n",
      "> 0.8258117437362671 | 0.940455324947834 | 0.9690425097942352\n",
      "> Confusion Matrix \n",
      "TN: 3993.0 | FP: 525.0 \n",
      "FN: 582.0 | TP: 916.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1566.0 | FP: 101.0 \n",
      "FN: 88.0 | TP: 124.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2427.0 | FP: 424.0 \n",
      "FN: 494.0 | TP: 792.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5004817843437195 | 0.7487566312997347\n",
      "> 2 | 0.43046826124191284 | 0.7967092175066313\n",
      "> 3 | 0.4265057444572449 | 0.8164787798408488\n",
      "> 4 | 0.3977872431278229 | 0.828125\n",
      "> 5 | 0.3788052797317505 | 0.8338030503978779\n",
      "> 6 | 0.3722444176673889 | 0.8389837533156499\n",
      "> 7 | 0.35932254791259766 | 0.8396468832891246\n",
      "> 8 | 0.365489661693573 | 0.8395639920424403\n",
      "> 9 | 0.35043689608573914 | 0.841387599469496\n",
      "> 10 | 0.35543712973594666 | 0.8425480769230769\n",
      "> 11 | 0.34435534477233887 | 0.8445374668435013\n",
      "> 12 | 0.9974395036697388 | 0.8433355437665783\n",
      "> 13 | 0.33965668082237244 | 0.839729774535809\n",
      "> 14 | 0.531301736831665 | 0.8430868700265252\n",
      "> 15 | 0.3296221196651459 | 0.8480603448275862\n",
      "> 16 | 0.9852874875068665 | 0.8441644562334217\n",
      "> 17 | 0.9825006127357483 | 0.8331399204244032\n",
      "> 18 | 0.33471840620040894 | 0.8358338859416445\n",
      "> 19 | 0.32079070806503296 | 0.8455321618037135\n",
      "> 20 | 0.31524109840393066 | 0.8468998673740054\n",
      "> 21 | 0.3298489451408386 | 0.8467755305039788\n",
      "> 22 | 0.33171743154525757 | 0.847065649867374\n",
      "> 23 | 0.3064884841442108 | 0.8462781830238727\n",
      "> 24 | 0.30767932534217834 | 0.846816976127321\n",
      "> 25 | 0.3207545280456543 | 0.8449519230769231\n",
      "> 26 | 0.3529583513736725 | 0.8448690318302388\n",
      "> 27 | 0.3010445237159729 | 0.8455736074270557\n",
      "> 28 | 0.30363672971725464 | 0.8446618037135278\n",
      "> 29 | 0.37016624212265015 | 0.8454078249336869\n",
      "> 30 | 0.36420905590057373 | 0.8449519230769231\n",
      "> 31 | 0.3649325966835022 | 0.8450762599469496\n",
      "> 32 | 0.3665430247783661 | 0.8449104774535809\n",
      "> 33 | 0.36618772149086 | 0.8447861405835544\n",
      "> 34 | 0.3656221032142639 | 0.8447446949602122\n",
      "> 35 | 0.36475372314453125 | 0.84470324933687\n",
      "> 36 | 0.36340659856796265 | 0.8447861405835544\n",
      "> 37 | 0.3622782528400421 | 0.8448690318302388\n",
      "> 38 | 0.35984015464782715 | 0.8450348143236074\n",
      "> 39 | 0.3587883412837982 | 0.8452834880636605\n",
      "> 40 | 0.6900540590286255 | 0.8452005968169761\n",
      "> 41 | 0.6427038311958313 | 0.8457393899204244\n",
      "> 42 | 0.55645751953125 | 0.8443302387267905\n",
      "> 43 | 0.5696516036987305 | 0.8460709549071618\n",
      "> 44 | 0.5468385815620422 | 0.8456979442970822\n",
      "> 45 | 0.570908784866333 | 0.845863726790451\n",
      "> 46 | 0.5315980315208435 | 0.84565649867374\n",
      "> 47 | 0.5473337173461914 | 0.8454907161803713\n",
      "> 48 | 0.29311901330947876 | 0.8459466180371353\n",
      "> 49 | 0.3044631779193878 | 0.8473972148541113\n",
      "> 50 | 0.9594934582710266 | 0.8436671087533156\n",
      "> Class Acc\n",
      "> 0.8159906914893618\n",
      "> DP | DI | DEOPP\n",
      "> 0.8258117437362671 | 0.940455324947834 | 0.9690425097942352\n",
      "> Confusion Matrix \n",
      "TN: 3993.0 | FP: 525.0 \n",
      "FN: 582.0 | TP: 916.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1566.0 | FP: 101.0 \n",
      "FN: 88.0 | TP: 124.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2427.0 | FP: 424.0 \n",
      "FN: 494.0 | TP: 792.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5004817843437195 | 0.7487566312997347\n",
      "> 2 | 0.43046826124191284 | 0.7967092175066313\n",
      "> 3 | 0.4265057444572449 | 0.8164787798408488\n",
      "> 4 | 0.3977872431278229 | 0.828125\n",
      "> 5 | 0.3788052797317505 | 0.8338030503978779\n",
      "> 6 | 0.3722444176673889 | 0.8389837533156499\n",
      "> 7 | 0.35932254791259766 | 0.8396468832891246\n",
      "> 8 | 0.365489661693573 | 0.8395639920424403\n",
      "> 9 | 0.35043689608573914 | 0.841387599469496\n",
      "> 10 | 0.35543712973594666 | 0.8425480769230769\n",
      "> 11 | 0.34435534477233887 | 0.8445374668435013\n",
      "> 12 | 0.9974395036697388 | 0.8433355437665783\n",
      "> 13 | 0.33965668082237244 | 0.839729774535809\n",
      "> 14 | 0.531301736831665 | 0.8430868700265252\n",
      "> 15 | 0.3296221196651459 | 0.8480603448275862\n",
      "> 16 | 0.9852874875068665 | 0.8441644562334217\n",
      "> 17 | 0.9825006127357483 | 0.8331399204244032\n",
      "> 18 | 0.33471840620040894 | 0.8358338859416445\n",
      "> 19 | 0.32079070806503296 | 0.8455321618037135\n",
      "> 20 | 0.31524109840393066 | 0.8468998673740054\n",
      "> 21 | 0.3298489451408386 | 0.8467755305039788\n",
      "> 22 | 0.33171743154525757 | 0.847065649867374\n",
      "> 23 | 0.3064884841442108 | 0.8462781830238727\n",
      "> 24 | 0.30767932534217834 | 0.846816976127321\n",
      "> 25 | 0.3207545280456543 | 0.8449519230769231\n",
      "> 26 | 0.3529583513736725 | 0.8448690318302388\n",
      "> 27 | 0.3010445237159729 | 0.8455736074270557\n",
      "> 28 | 0.30363672971725464 | 0.8446618037135278\n",
      "> 29 | 0.37016624212265015 | 0.8454078249336869\n",
      "> 30 | 0.36420905590057373 | 0.8449519230769231\n",
      "> 31 | 0.3649325966835022 | 0.8450762599469496\n",
      "> 32 | 0.3665430247783661 | 0.8449104774535809\n",
      "> 33 | 0.36618772149086 | 0.8447861405835544\n",
      "> 34 | 0.3656221032142639 | 0.8447446949602122\n",
      "> 35 | 0.36475372314453125 | 0.84470324933687\n",
      "> 36 | 0.36340659856796265 | 0.8447861405835544\n",
      "> 37 | 0.3622782528400421 | 0.8448690318302388\n",
      "> 38 | 0.35984015464782715 | 0.8450348143236074\n",
      "> 39 | 0.3587883412837982 | 0.8452834880636605\n",
      "> 40 | 0.6900540590286255 | 0.8452005968169761\n",
      "> 41 | 0.6427038311958313 | 0.8457393899204244\n",
      "> 42 | 0.55645751953125 | 0.8443302387267905\n",
      "> 43 | 0.5696516036987305 | 0.8460709549071618\n",
      "> 44 | 0.5468385815620422 | 0.8456979442970822\n",
      "> 45 | 0.570908784866333 | 0.845863726790451\n",
      "> 46 | 0.5315980315208435 | 0.84565649867374\n",
      "> 47 | 0.5473337173461914 | 0.8454907161803713\n",
      "> 48 | 0.29311901330947876 | 0.8459466180371353\n",
      "> 49 | 0.3044631779193878 | 0.8473972148541113\n",
      "> 50 | 0.9594934582710266 | 0.8436671087533156\n",
      "> Class Acc\n",
      "> 0.8159906914893618\n",
      "> DP | DI | DEOPP\n",
      "> 0.8258117437362671 | 0.940455324947834 | 0.9690425097942352\n",
      "> Confusion Matrix \n",
      "TN: 3993.0 | FP: 525.0 \n",
      "FN: 582.0 | TP: 916.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1566.0 | FP: 101.0 \n",
      "FN: 88.0 | TP: 124.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2427.0 | FP: 424.0 \n",
      "FN: 494.0 | TP: 792.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5004817843437195 | 0.7487566312997347\n",
      "> 2 | 0.43046826124191284 | 0.7967092175066313\n",
      "> 3 | 0.4265057444572449 | 0.8164787798408488\n",
      "> 4 | 0.3977872431278229 | 0.828125\n",
      "> 5 | 0.3788052797317505 | 0.8338030503978779\n",
      "> 6 | 0.3722444176673889 | 0.8389837533156499\n",
      "> 7 | 0.35932254791259766 | 0.8396468832891246\n",
      "> 8 | 0.365489661693573 | 0.8395639920424403\n",
      "> 9 | 0.35043689608573914 | 0.841387599469496\n",
      "> 10 | 0.35543712973594666 | 0.8425480769230769\n",
      "> 11 | 0.34435534477233887 | 0.8445374668435013\n",
      "> 12 | 0.9974395036697388 | 0.8433355437665783\n",
      "> 13 | 0.33965668082237244 | 0.839729774535809\n",
      "> 14 | 0.531301736831665 | 0.8430868700265252\n",
      "> 15 | 0.3296221196651459 | 0.8480603448275862\n",
      "> 16 | 0.9852874875068665 | 0.8441644562334217\n",
      "> 17 | 0.9825006127357483 | 0.8331399204244032\n",
      "> 18 | 0.33471840620040894 | 0.8358338859416445\n",
      "> 19 | 0.32079070806503296 | 0.8455321618037135\n",
      "> 20 | 0.31524109840393066 | 0.8468998673740054\n",
      "> 21 | 0.3298489451408386 | 0.8467755305039788\n",
      "> 22 | 0.33171743154525757 | 0.847065649867374\n",
      "> 23 | 0.3064884841442108 | 0.8462781830238727\n",
      "> 24 | 0.30767932534217834 | 0.846816976127321\n",
      "> 25 | 0.3207545280456543 | 0.8449519230769231\n",
      "> 26 | 0.3529583513736725 | 0.8448690318302388\n",
      "> 27 | 0.3010445237159729 | 0.8455736074270557\n",
      "> 28 | 0.30363672971725464 | 0.8446618037135278\n",
      "> 29 | 0.37016624212265015 | 0.8454078249336869\n",
      "> 30 | 0.36420905590057373 | 0.8449519230769231\n",
      "> 31 | 0.3649325966835022 | 0.8450762599469496\n",
      "> 32 | 0.3665430247783661 | 0.8449104774535809\n",
      "> 33 | 0.36618772149086 | 0.8447861405835544\n",
      "> 34 | 0.3656221032142639 | 0.8447446949602122\n",
      "> 35 | 0.36475372314453125 | 0.84470324933687\n",
      "> 36 | 0.36340659856796265 | 0.8447861405835544\n",
      "> 37 | 0.3622782528400421 | 0.8448690318302388\n",
      "> 38 | 0.35984015464782715 | 0.8450348143236074\n",
      "> 39 | 0.3587883412837982 | 0.8452834880636605\n",
      "> 40 | 0.6900540590286255 | 0.8452005968169761\n",
      "> 41 | 0.6427038311958313 | 0.8457393899204244\n",
      "> 42 | 0.55645751953125 | 0.8443302387267905\n",
      "> 43 | 0.5696516036987305 | 0.8460709549071618\n",
      "> 44 | 0.5468385815620422 | 0.8456979442970822\n",
      "> 45 | 0.570908784866333 | 0.845863726790451\n",
      "> 46 | 0.5315980315208435 | 0.84565649867374\n",
      "> 47 | 0.5473337173461914 | 0.8454907161803713\n",
      "> 48 | 0.29311901330947876 | 0.8459466180371353\n",
      "> 49 | 0.3044631779193878 | 0.8473972148541113\n",
      "> 50 | 0.9594934582710266 | 0.8436671087533156\n",
      "> Class Acc\n",
      "> 0.8159906914893618\n",
      "> DP | DI | DEOPP\n",
      "> 0.8258117437362671 | 0.940455324947834 | 0.9690425097942352\n",
      "> Confusion Matrix \n",
      "TN: 3993.0 | FP: 525.0 \n",
      "FN: 582.0 | TP: 916.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1566.0 | FP: 101.0 \n",
      "FN: 88.0 | TP: 124.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2427.0 | FP: 424.0 \n",
      "FN: 494.0 | TP: 792.0\n",
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5004817843437195 | 0.7487566312997347\n",
      "> 2 | 0.43046826124191284 | 0.7967092175066313\n",
      "> 3 | 0.4265057444572449 | 0.8164787798408488\n",
      "> 4 | 0.3977872431278229 | 0.828125\n",
      "> 5 | 0.3788052797317505 | 0.8338030503978779\n",
      "> 6 | 0.3722444176673889 | 0.8389837533156499\n",
      "> 7 | 0.35932254791259766 | 0.8396468832891246\n",
      "> 8 | 0.365489661693573 | 0.8395639920424403\n",
      "> 9 | 0.35043689608573914 | 0.841387599469496\n",
      "> 10 | 0.35543712973594666 | 0.8425480769230769\n",
      "> 11 | 0.34435534477233887 | 0.8445374668435013\n",
      "> 12 | 0.9974395036697388 | 0.8433355437665783\n",
      "> 13 | 0.33965668082237244 | 0.839729774535809\n",
      "> 14 | 0.531301736831665 | 0.8430868700265252\n",
      "> 15 | 0.3296221196651459 | 0.8480603448275862\n",
      "> 16 | 0.9852874875068665 | 0.8441644562334217\n",
      "> 17 | 0.9825006127357483 | 0.8331399204244032\n",
      "> 18 | 0.33471840620040894 | 0.8358338859416445\n",
      "> 19 | 0.32079070806503296 | 0.8455321618037135\n",
      "> 20 | 0.31524109840393066 | 0.8468998673740054\n",
      "> 21 | 0.3298489451408386 | 0.8467755305039788\n",
      "> 22 | 0.33171743154525757 | 0.847065649867374\n",
      "> 23 | 0.3064884841442108 | 0.8462781830238727\n",
      "> 24 | 0.30767932534217834 | 0.846816976127321\n",
      "> 25 | 0.3207545280456543 | 0.8449519230769231\n",
      "> 26 | 0.3529583513736725 | 0.8448690318302388\n",
      "> 27 | 0.3010445237159729 | 0.8455736074270557\n",
      "> 28 | 0.30363672971725464 | 0.8446618037135278\n",
      "> 29 | 0.37016624212265015 | 0.8454078249336869\n",
      "> 30 | 0.36420905590057373 | 0.8449519230769231\n",
      "> 31 | 0.3649325966835022 | 0.8450762599469496\n",
      "> 32 | 0.3665430247783661 | 0.8449104774535809\n",
      "> 33 | 0.36618772149086 | 0.8447861405835544\n",
      "> 34 | 0.3656221032142639 | 0.8447446949602122\n",
      "> 35 | 0.36475372314453125 | 0.84470324933687\n",
      "> 36 | 0.36340659856796265 | 0.8447861405835544\n",
      "> 37 | 0.3622782528400421 | 0.8448690318302388\n",
      "> 38 | 0.35984015464782715 | 0.8450348143236074\n",
      "> 39 | 0.3587883412837982 | 0.8452834880636605\n",
      "> 40 | 0.6900540590286255 | 0.8452005968169761\n",
      "> 41 | 0.6427038311958313 | 0.8457393899204244\n",
      "> 42 | 0.55645751953125 | 0.8443302387267905\n",
      "> 43 | 0.5696516036987305 | 0.8460709549071618\n",
      "> 44 | 0.5468385815620422 | 0.8456979442970822\n",
      "> 45 | 0.570908784866333 | 0.845863726790451\n",
      "> 46 | 0.5315980315208435 | 0.84565649867374\n",
      "> 47 | 0.5473337173461914 | 0.8454907161803713\n",
      "> 48 | 0.29311901330947876 | 0.8459466180371353\n",
      "> 49 | 0.3044631779193878 | 0.8473972148541113\n",
      "> 50 | 0.9594934582710266 | 0.8436671087533156\n",
      "> Class Acc\n",
      "> 0.8159906914893618\n",
      "> DP | DI | DEOPP\n",
      "> 0.8258117437362671 | 0.940455324947834 | 0.9690425097942352\n",
      "> Confusion Matrix \n",
      "TN: 3993.0 | FP: 525.0 \n",
      "FN: 582.0 | TP: 916.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 1566.0 | FP: 101.0 \n",
      "FN: 88.0 | TP: 124.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 2427.0 | FP: 424.0 \n",
      "FN: 494.0 | TP: 792.0\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "sns.heatmap(confusion_matrix, annot=True, fmt='g')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4ea5e5c2e0>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving into DF then CSV"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "result_df = pd.DataFrame(results, columns=header)\n",
    "result_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       model_name  clas_acc        dp   deqodds    deqopp  trade_dp  \\\n",
       "0  UnfairLR-decay  0.847407  0.800875  0.877304  0.841838  0.823484   \n",
       "1  UnfairLR-decay  0.847407  0.800875  0.877304  0.841838  0.823484   \n",
       "2  UnfairLR-decay  0.847407  0.800875  0.877304  0.841838  0.823484   \n",
       "3  UnfairLR-decay  0.847407  0.800875  0.877304  0.841838  0.823484   \n",
       "4  UnfairLR-decay  0.847407  0.800875  0.877304  0.841838  0.823484   \n",
       "5        UnfairLR  0.815991  0.825812  0.940455  0.969043  0.820872   \n",
       "6        UnfairLR  0.815991  0.825812  0.940455  0.969043  0.820872   \n",
       "7        UnfairLR  0.815991  0.825812  0.940455  0.969043  0.820872   \n",
       "8        UnfairLR  0.815991  0.825812  0.940455  0.969043  0.820872   \n",
       "9        UnfairLR  0.815991  0.825812  0.940455  0.969043  0.820872   \n",
       "\n",
       "   trade_deqodds  trade_deqopp   TN_a0  FP_a0  FN_a0  TP_a0   TN_a1  FP_a1  \\\n",
       "0       0.862096      0.844613  1637.0   30.0  112.0  100.0  2551.0  300.0   \n",
       "1       0.862096      0.844613  1637.0   30.0  112.0  100.0  2551.0  300.0   \n",
       "2       0.862096      0.844613  1637.0   30.0  112.0  100.0  2551.0  300.0   \n",
       "3       0.862096      0.844613  1637.0   30.0  112.0  100.0  2551.0  300.0   \n",
       "4       0.862096      0.844613  1637.0   30.0  112.0  100.0  2551.0  300.0   \n",
       "5       0.873813      0.885955  1566.0  101.0   88.0  124.0  2427.0  424.0   \n",
       "6       0.873813      0.885955  1566.0  101.0   88.0  124.0  2427.0  424.0   \n",
       "7       0.873813      0.885955  1566.0  101.0   88.0  124.0  2427.0  424.0   \n",
       "8       0.873813      0.885955  1566.0  101.0   88.0  124.0  2427.0  424.0   \n",
       "9       0.873813      0.885955  1566.0  101.0   88.0  124.0  2427.0  424.0   \n",
       "\n",
       "   FN_a1  TP_a1  \n",
       "0  476.0  810.0  \n",
       "1  476.0  810.0  \n",
       "2  476.0  810.0  \n",
       "3  476.0  810.0  \n",
       "4  476.0  810.0  \n",
       "5  494.0  792.0  \n",
       "6  494.0  792.0  \n",
       "7  494.0  792.0  \n",
       "8  494.0  792.0  \n",
       "9  494.0  792.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>clas_acc</th>\n",
       "      <th>dp</th>\n",
       "      <th>deqodds</th>\n",
       "      <th>deqopp</th>\n",
       "      <th>trade_dp</th>\n",
       "      <th>trade_deqodds</th>\n",
       "      <th>trade_deqopp</th>\n",
       "      <th>TN_a0</th>\n",
       "      <th>FP_a0</th>\n",
       "      <th>FN_a0</th>\n",
       "      <th>TP_a0</th>\n",
       "      <th>TN_a1</th>\n",
       "      <th>FP_a1</th>\n",
       "      <th>FN_a1</th>\n",
       "      <th>TP_a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>0.847407</td>\n",
       "      <td>0.800875</td>\n",
       "      <td>0.877304</td>\n",
       "      <td>0.841838</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>0.862096</td>\n",
       "      <td>0.844613</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2551.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>0.847407</td>\n",
       "      <td>0.800875</td>\n",
       "      <td>0.877304</td>\n",
       "      <td>0.841838</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>0.862096</td>\n",
       "      <td>0.844613</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2551.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>0.847407</td>\n",
       "      <td>0.800875</td>\n",
       "      <td>0.877304</td>\n",
       "      <td>0.841838</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>0.862096</td>\n",
       "      <td>0.844613</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2551.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>0.847407</td>\n",
       "      <td>0.800875</td>\n",
       "      <td>0.877304</td>\n",
       "      <td>0.841838</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>0.862096</td>\n",
       "      <td>0.844613</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2551.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>0.847407</td>\n",
       "      <td>0.800875</td>\n",
       "      <td>0.877304</td>\n",
       "      <td>0.841838</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>0.862096</td>\n",
       "      <td>0.844613</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2551.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>0.815991</td>\n",
       "      <td>0.825812</td>\n",
       "      <td>0.940455</td>\n",
       "      <td>0.969043</td>\n",
       "      <td>0.820872</td>\n",
       "      <td>0.873813</td>\n",
       "      <td>0.885955</td>\n",
       "      <td>1566.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>2427.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>792.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>0.815991</td>\n",
       "      <td>0.825812</td>\n",
       "      <td>0.940455</td>\n",
       "      <td>0.969043</td>\n",
       "      <td>0.820872</td>\n",
       "      <td>0.873813</td>\n",
       "      <td>0.885955</td>\n",
       "      <td>1566.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>2427.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>792.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>0.815991</td>\n",
       "      <td>0.825812</td>\n",
       "      <td>0.940455</td>\n",
       "      <td>0.969043</td>\n",
       "      <td>0.820872</td>\n",
       "      <td>0.873813</td>\n",
       "      <td>0.885955</td>\n",
       "      <td>1566.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>2427.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>792.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>0.815991</td>\n",
       "      <td>0.825812</td>\n",
       "      <td>0.940455</td>\n",
       "      <td>0.969043</td>\n",
       "      <td>0.820872</td>\n",
       "      <td>0.873813</td>\n",
       "      <td>0.885955</td>\n",
       "      <td>1566.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>2427.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>792.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>0.815991</td>\n",
       "      <td>0.825812</td>\n",
       "      <td>0.940455</td>\n",
       "      <td>0.969043</td>\n",
       "      <td>0.820872</td>\n",
       "      <td>0.873813</td>\n",
       "      <td>0.885955</td>\n",
       "      <td>1566.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>2427.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>792.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "result_df.to_csv('results/validation_unfair_lr-50.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('falsb': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "34ca74ed6235dfc7dda926bb3adb31e801e3d02679121d5b444ee035e270bd57"
   }
  },
  "interpreter": {
   "hash": "34ca74ed6235dfc7dda926bb3adb31e801e3d02679121d5b444ee035e270bd57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}