{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing file \n",
    "### where we evaluate Zhang's models using the test set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from math import sqrt, isnan\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "from util import metrics\n",
    "from util.load_data import load_data\n",
    "from util.evaluation import *\n",
    "\n",
    "from fairgan.models import *\n",
    "from fairgan.learning import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "opt = Adam(learning_rate=lr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "test_loop = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x_train, y_train, a_train = load_data('adult', 'train')\n",
    "raw_data = (x_train, y_train, a_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "xdim = x_train.shape[1]\n",
    "ydim = y_train.shape[1]\n",
    "adim = a_train.shape[1]\n",
    "zdim = 8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "train_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 113), (64, 1), (64, 1)), types: (tf.float64, tf.float64, tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "x_valid, y_valid, a_valid = load_data('adult', 'valid')\n",
    "\n",
    "valid_data = Dataset.from_tensor_slices((x_valid, y_valid, a_valid))\n",
    "valid_data = valid_data.batch(batch_size, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "x_test, y_test, a_test = load_data('adult', 'test')\n",
    "\n",
    "test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "test_data = test_data.batch(batch_size, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "num_batchs = y_test.shape[0]//batch_size\n",
    "num_batchs *= batch_size\n",
    "y_test = y_test[:num_batchs]\n",
    "a_test = a_test[:num_batchs]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Result file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "header = \"model_name\", \"clas_acc\", \"dp\", \"deqodds\", \"deqopp\", \"trade_dp\", \"trade_deqodds\", \"trade_deqopp\", \"TN_a0\", \"FP_a0\", \"FN_a0\", \"TP_a0\", \"TN_a1\", \"FP_a1\", \"FN_a1\", \"TP_a1\"\n",
    "results = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing loop\n",
    "#### Each model is evalueted 5 times\n",
    "#### In the end of each iteration we save the result"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Zhang for DP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "fairdef = 'DemPar'\n",
    "\n",
    "for i in range(test_loop):\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    model = FairLogisticRegression(xdim, batch_size, fairdef)\n",
    "    zhang_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "    Y_hat, A_hat = evaluation(model, test_data)\n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1 = compute_metrics(y_test, Y_hat, a_test, A_hat)\n",
    "\n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    tradeoff = []\n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "    result = ['Zhang4DP', clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> Epoch | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.41985824704170227 | 0.7958990931510925 | 0.7547662466843501 | 0.3273375331564987\n",
      "> 2 | 1.0354886054992676 | 0.6687623262405396 | 0.8144479442970822 | 0.3868534482758621\n",
      "> 3 | 1.0318939685821533 | 0.6110285520553589 | 0.8206647877984085 | 0.6441064323607427\n",
      "> 4 | 0.6960029006004333 | 0.5837156772613525 | 0.8297413793103449 | 0.7141495358090186\n",
      "> 5 | 0.9805105328559875 | 0.5663491487503052 | 0.8360825596816976 | 0.7120772546419099\n",
      "> 6 | 0.3829643130302429 | 0.5573411583900452 | 0.8416777188328912 | 0.6932609416445623\n",
      "> 7 | 0.3636026978492737 | 0.5365790128707886 | 0.8407659151193634 | 0.684888925729443\n",
      "> 8 | 0.7166281938552856 | 0.5501534938812256 | 0.8445789124668435 | 0.6815318302387268\n",
      "> 9 | 1.0129873752593994 | 0.5469249486923218 | 0.8423822944297082 | 0.6807858090185677\n",
      "> 10 | 1.0095266103744507 | 0.54656583070755 | 0.8320208885941645 | 0.6812002652519894\n",
      "> 11 | 1.0033175945281982 | 0.5453625321388245 | 0.8367871352785146 | 0.6815318302387268\n",
      "> 12 | 1.0015054941177368 | 0.5462006330490112 | 0.8321866710875332 | 0.6822778514588859\n",
      "> 13 | 0.36519503593444824 | 0.521156907081604 | 0.834507625994695 | 0.6839771220159151\n",
      "> 14 | 0.9968184232711792 | 0.5463130474090576 | 0.8433355437665783 | 0.6850961538461539\n",
      "> 15 | 0.9956006407737732 | 0.5473375916481018 | 0.8325596816976127 | 0.6879559018567639\n",
      "> 16 | 0.4005972146987915 | 0.5262917280197144 | 0.8335129310344828 | 0.6911886604774535\n",
      "> 17 | 0.325905978679657 | 0.5238138437271118 | 0.8432940981432361 | 0.6918517904509284\n",
      "> 18 | 0.990240216255188 | 0.5483875870704651 | 0.8413047082228117 | 0.6929708222811671\n",
      "> 19 | 0.9896870255470276 | 0.5494670867919922 | 0.8328912466843501 | 0.6942141909814323\n",
      "> 20 | 0.8911620378494263 | 0.5508488416671753 | 0.8331399204244032 | 0.6973640583554377\n",
      "> 21 | 0.3309088945388794 | 0.5319581627845764 | 0.8459880636604775 | 0.6976541777188329\n",
      "> 22 | 0.3991861939430237 | 0.5229882001876831 | 0.8422994031830239 | 0.6969496021220158\n",
      "> 23 | 0.33992743492126465 | 0.536028265953064 | 0.8486405835543767 | 0.6970739389920424\n",
      "> 24 | 0.33810025453567505 | 0.5349802374839783 | 0.8453663793103449 | 0.6972811671087533\n",
      "> 25 | 0.339204341173172 | 0.5356348752975464 | 0.8476458885941645 | 0.6975298408488064\n",
      "> 26 | 0.3173942267894745 | 0.5231751799583435 | 0.8432940981432361 | 0.6972811671087533\n",
      "> 27 | 0.3160659670829773 | 0.5224118232727051 | 0.8461538461538461 | 0.6976541777188329\n",
      "> 28 | 0.40826451778411865 | 0.55583655834198 | 0.8458222811671088 | 0.6978614058355438\n",
      "> 29 | 0.31328344345092773 | 0.5253969430923462 | 0.8374502652519894 | 0.6985659814323608\n",
      "> 30 | 0.3212812840938568 | 0.5207537412643433 | 0.846816976127321 | 0.6992705570291777\n",
      "> 31 | 0.3328772187232971 | 0.5373739004135132 | 0.8504227453580901 | 0.6992705570291777\n",
      "> 32 | 0.3349609971046448 | 0.5385283827781677 | 0.8476458885941645 | 0.6994777851458887\n",
      "> 33 | 0.33827924728393555 | 0.520930290222168 | 0.8421336206896551 | 0.6996435676392573\n",
      "> 34 | 0.31738150119781494 | 0.5215505361557007 | 0.8456150530503979 | 0.6995192307692308\n",
      "> 35 | 0.31103062629699707 | 0.5242234468460083 | 0.8481017904509284 | 0.6994363395225465\n",
      "> 36 | 0.3138848543167114 | 0.5283282995223999 | 0.848930702917772 | 0.6994363395225465\n",
      "> 37 | 0.9783401489257812 | 0.5569949746131897 | 0.8445374668435013 | 0.7000165782493368\n",
      "> 38 | 0.9777476787567139 | 0.5567870140075684 | 0.8333885941644562 | 0.7010941644562334\n",
      "> 39 | 0.3252325654029846 | 0.5403410196304321 | 0.838776525198939 | 0.7010941644562334\n",
      "> 40 | 0.3183467388153076 | 0.5358732342720032 | 0.8490550397877984 | 0.7021303050397878\n",
      "> 41 | 0.8333129286766052 | 0.5594573020935059 | 0.8451591511936339 | 0.7012599469496021\n",
      "> 42 | 0.33154451847076416 | 0.5235429406166077 | 0.8428796419098143 | 0.7008454907161803\n",
      "> 43 | 0.32804566621780396 | 0.5408968329429626 | 0.8498425066312998 | 0.700513925729443\n",
      "> 44 | 0.3270394802093506 | 0.5405080914497375 | 0.8470242042440318 | 0.7000165782493368\n",
      "> 45 | 0.31874117255210876 | 0.5363761782646179 | 0.8478945623342176 | 0.6997679045092838\n",
      "> 46 | 0.3087531328201294 | 0.5251009464263916 | 0.8457393899204244 | 0.6998507957559681\n",
      "> 47 | 0.32607340812683105 | 0.5402535796165466 | 0.8484333554376657 | 0.6999336870026525\n",
      "> 48 | 0.757219672203064 | 0.5603572130203247 | 0.8420092838196287 | 0.6999751326259946\n",
      "> 49 | 0.33932238817214966 | 0.5240384340286255 | 0.8462367374005305 | 0.6996850132625995\n",
      "> 50 | 0.3158995509147644 | 0.5244752764701843 | 0.8466511936339522 | 0.7003895888594165\n",
      "> 51 | 0.39306640625 | 0.5282729864120483 | 0.8461538461538461 | 0.6993948938992043\n",
      "> 52 | 0.3368811011314392 | 0.5247230529785156 | 0.8477287798408488 | 0.7003066976127321\n",
      "> 53 | 0.39074981212615967 | 0.5284693241119385 | 0.8459051724137931 | 0.6995192307692308\n",
      "> 54 | 0.5318112373352051 | 0.5505987405776978 | 0.8466511936339522 | 0.6997264588859416\n",
      "> 55 | 0.9717371463775635 | 0.5604583024978638 | 0.8436256631299734 | 0.6999336870026525\n",
      "> 56 | 0.9717501401901245 | 0.560080349445343 | 0.833554376657825 | 0.6998507957559681\n",
      "> 57 | 0.6027535796165466 | 0.5529572367668152 | 0.8370772546419099 | 0.7006797082228117\n",
      "> 58 | 0.3582077622413635 | 0.5283061861991882 | 0.8466511936339522 | 0.7008869363395225\n",
      "> 59 | 0.3017675280570984 | 0.5334659814834595 | 0.8465268567639257 | 0.6996021220159151\n",
      "> 60 | 0.34970375895500183 | 0.5277832746505737 | 0.8467340848806366 | 0.698607427055703\n",
      "> 61 | 0.3060814142227173 | 0.5272479057312012 | 0.8476458885941645 | 0.6992291114058355\n",
      "> 62 | 0.4028754234313965 | 0.531636118888855 | 0.8473143236074271 | 0.6985659814323608\n",
      "> 63 | 0.3809223175048828 | 0.5285542011260986 | 0.8461952917771883 | 0.6987732095490716\n",
      "> 64 | 0.3188038468360901 | 0.5259290933609009 | 0.8488063660477454 | 0.6991876657824934\n",
      "> 65 | 0.300523966550827 | 0.5290219783782959 | 0.8453249336870027 | 0.6985245358090186\n",
      "> 66 | 0.30535072088241577 | 0.526587188243866 | 0.848018899204244 | 0.6988146551724138\n",
      "> 67 | 0.4027184247970581 | 0.5305166244506836 | 0.8446618037135278 | 0.6974883952254642\n",
      "> 68 | 0.82305908203125 | 0.5622483491897583 | 0.8450348143236074 | 0.6986488726790451\n",
      "> 69 | 0.33129042387008667 | 0.5258368253707886 | 0.847065649867374 | 0.6990633289124668\n",
      "> 70 | 0.3336842656135559 | 0.5264932513237 | 0.8478531167108754 | 0.6981929708222812\n",
      "> 71 | 0.5324713587760925 | 0.5510367155075073 | 0.8462367374005305 | 0.6981100795755968\n",
      "> 72 | 0.30311745405197144 | 0.5349859595298767 | 0.8457393899204244 | 0.6972397214854111\n",
      "> 73 | 0.3346188962459564 | 0.5272153615951538 | 0.848972148541114 | 0.6974055039787799\n",
      "> 74 | 0.9679959416389465 | 0.5616322755813599 | 0.8441644562334217 | 0.696991047745358\n",
      "> 75 | 0.9382991194725037 | 0.5613042116165161 | 0.8338030503978779 | 0.6978614058355438\n",
      "> 76 | 0.3058388829231262 | 0.5396300554275513 | 0.8437085543766578 | 0.6994363395225465\n",
      "> 77 | 0.33057069778442383 | 0.5279884338378906 | 0.8471070954907162 | 0.6978199602122016\n",
      "> 78 | 0.3180561065673828 | 0.5272729396820068 | 0.8463610742705571 | 0.6979857427055702\n",
      "> 79 | 0.3161037564277649 | 0.5274423360824585 | 0.8486820291777188 | 0.6981929708222812\n",
      "> 80 | 0.3191918134689331 | 0.5434778928756714 | 0.849925397877984 | 0.6972811671087533\n",
      "> 81 | 0.9692378044128418 | 0.5612024068832397 | 0.8420092838196287 | 0.6978199602122016\n",
      "> 82 | 0.9419637322425842 | 0.5611406564712524 | 0.8334714854111406 | 0.6993534482758621\n",
      "> 83 | 0.30084913969039917 | 0.5302425026893616 | 0.84375 | 0.6994363395225465\n",
      "> 84 | 0.31953877210617065 | 0.5453904867172241 | 0.848972148541114 | 0.6982758620689655\n",
      "> 85 | 0.968616247177124 | 0.5616254806518555 | 0.8443302387267905 | 0.6982758620689655\n",
      "> 86 | 0.9137199521064758 | 0.5614639520645142 | 0.8389008620689655 | 0.6985245358090186\n",
      "> 87 | 0.9273849725723267 | 0.5609925985336304 | 0.8345905172413793 | 0.6990218832891246\n",
      "> 88 | 0.39080366492271423 | 0.5326846837997437 | 0.8434184350132626 | 0.6987317639257294\n",
      "> 89 | 0.3104311525821686 | 0.5291026830673218 | 0.8483504641909814 | 0.6978614058355438\n",
      "> 90 | 0.31939393281936646 | 0.5457038879394531 | 0.849883952254642 | 0.6976127320954908\n",
      "> 91 | 0.6579018831253052 | 0.5606815218925476 | 0.8439157824933687 | 0.697198275862069\n",
      "> 92 | 0.5706021785736084 | 0.5545127391815186 | 0.8449519230769231 | 0.697944297082228\n",
      "> 93 | 0.3049616515636444 | 0.5282796025276184 | 0.8455736074270557 | 0.6966594827586207\n",
      "> 94 | 0.33147114515304565 | 0.5280947089195251 | 0.8481017904509284 | 0.6959134615384616\n",
      "> 95 | 0.9669325947761536 | 0.5621592402458191 | 0.8444545755968169 | 0.6962035809018567\n",
      "> 96 | 0.3285447061061859 | 0.5291382670402527 | 0.8363312334217506 | 0.697198275862069\n",
      "> 97 | 0.3291519284248352 | 0.5289179086685181 | 0.8486820291777188 | 0.6970739389920424\n",
      "> 98 | 0.6286633014678955 | 0.5589810609817505 | 0.8455736074270557 | 0.6959963527851458\n",
      "> 99 | 0.302890807390213 | 0.5289567112922668 | 0.8464854111405835 | 0.6958305702917772\n",
      "> 100 | 0.3323171138763428 | 0.5284028053283691 | 0.8483090185676393 | 0.6957476790450928\n",
      "> Class Acc | Adv Acc\n",
      "> 0.8291223404255319 | 0.6900930851063829\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8164318799972534 | 0.9132892899215221 | 0.917105495929718\n",
      "> Confusion Matrix \n",
      "TN: 10271.0 | FP: 1071.0 \n",
      "FN: 1499.0 | TP: 2199.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 4182.0 | FP: 168.0 \n",
      "FN: 265.0 | TP: 292.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 6089.0 | FP: 903.0 \n",
      "FN: 1234.0 | TP: 1907.0\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Zhang for Eq Odds"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "fairdef = 'EqOdds'\n",
    "\n",
    "for i in range(test_loop):\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    model = FairLogisticRegression(xdim, batch_size, fairdef)\n",
    "    zhang_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "    Y_hat, A_hat = evaluation(model, test_data)\n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1 = compute_metrics(y_test, Y_hat, a_test, A_hat)\n",
    "\n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    tradeoff = []\n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "    result = ['Zhang4EqOdds', clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> Epoch | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.8880362510681152 | 0.7379366159439087 | 0.7536057692307692 | 0.3326840185676393\n",
      "> 2 | 0.3920368552207947 | 0.595535397529602 | 0.8192970822281167 | 0.5264008620689655\n",
      "> 3 | 1.030700445175171 | 0.5585260987281799 | 0.8283322281167109 | 0.7106266578249336\n",
      "> 4 | 1.032625675201416 | 0.5441094040870667 | 0.8251823607427056 | 0.7202834880636605\n",
      "> 5 | 0.4029202163219452 | 0.5363216400146484 | 0.8282907824933687 | 0.7244694960212201\n",
      "> 6 | 0.3809119462966919 | 0.5224566459655762 | 0.8337201591511936 | 0.7265417771883289\n",
      "> 7 | 0.7461276054382324 | 0.5393401384353638 | 0.8391909814323607 | 0.724883952254642\n",
      "> 8 | 0.3661111295223236 | 0.5114368796348572 | 0.8455736074270557 | 0.7207393899204244\n",
      "> 9 | 0.35401415824890137 | 0.5024999380111694 | 0.8430039787798409 | 0.7177138594164456\n",
      "> 10 | 0.36830073595046997 | 0.5135820508003235 | 0.8476873342175066 | 0.7158073607427056\n",
      "> 11 | 0.3678044080734253 | 0.5143375396728516 | 0.8447446949602122 | 0.7151442307692308\n",
      "> 12 | 1.0058189630508423 | 0.5467468500137329 | 0.8418020557029178 | 0.7141495358090186\n",
      "> 13 | 0.35647422075271606 | 0.5144082307815552 | 0.8406001326259946 | 0.7145225464190981\n",
      "> 14 | 0.3439192473888397 | 0.49991485476493835 | 0.8438328912466844 | 0.713569297082228\n",
      "> 15 | 0.3414986729621887 | 0.5090839862823486 | 0.8475215517241379 | 0.7134449602122016\n",
      "> 16 | 0.33517056703567505 | 0.4990723133087158 | 0.8441230106100795 | 0.7129476127320955\n",
      "> 17 | 0.33005326986312866 | 0.49991101026535034 | 0.8449519230769231 | 0.7139008620689655\n",
      "> 18 | 0.3484331965446472 | 0.5169965028762817 | 0.8470242042440318 | 0.7129890583554377\n",
      "> 19 | 0.3249432444572449 | 0.5017985105514526 | 0.8428381962864722 | 0.7131133952254642\n",
      "> 20 | 0.4066320061683655 | 0.5110423564910889 | 0.8451177055702918 | 0.7120772546419099\n",
      "> 21 | 0.3248916268348694 | 0.508747398853302 | 0.84565649867374 | 0.711662798408488\n",
      "> 22 | 0.986179769039154 | 0.5607666373252869 | 0.8453663793103449 | 0.7125746021220158\n",
      "> 23 | 0.4927810728549957 | 0.5353101491928101 | 0.8338859416445623 | 0.7126574933687002\n",
      "> 24 | 0.3376123905181885 | 0.5224918127059937 | 0.8494694960212201 | 0.7132377320954908\n",
      "> 25 | 0.9843710064888 | 0.5620855689048767 | 0.8423822944297082 | 0.7112897877984086\n",
      "> 26 | 0.9834587574005127 | 0.5616980791091919 | 0.833305702917772 | 0.710709549071618\n",
      "> 27 | 0.8921575546264648 | 0.5622913837432861 | 0.8336787135278515 | 0.7109582228116711\n",
      "> 28 | 0.9060701727867126 | 0.5620043873786926 | 0.84470324933687 | 0.7103779840848806\n",
      "> 29 | 0.7445081472396851 | 0.5626775026321411 | 0.8423408488063661 | 0.7088444960212201\n",
      "> 30 | 0.3520282506942749 | 0.5143773555755615 | 0.8428796419098143 | 0.7084300397877984\n",
      "> 31 | 0.33354219794273376 | 0.530523955821991 | 0.8464439655172413 | 0.7090102785145889\n",
      "> 32 | 0.34273889660835266 | 0.5113986730575562 | 0.8472314323607427 | 0.7078083554376657\n",
      "> 33 | 0.35019785165786743 | 0.5121192336082458 | 0.8469827586206897 | 0.707145225464191\n",
      "> 34 | 0.33584362268447876 | 0.5105902552604675 | 0.8473557692307692 | 0.7074767904509284\n",
      "> 35 | 0.31264814734458923 | 0.5100730657577515 | 0.8461538461538461 | 0.707145225464191\n",
      "> 36 | 0.3282182216644287 | 0.5253995656967163 | 0.8502984084880636 | 0.7073110079575597\n",
      "> 37 | 0.9628289937973022 | 0.5657663345336914 | 0.8420921750663131 | 0.7073110079575597\n",
      "> 38 | 0.30704760551452637 | 0.5138442516326904 | 0.8406415782493368 | 0.7067722148541113\n",
      "> 39 | 0.3183704614639282 | 0.5110899209976196 | 0.8468998673740054 | 0.7057775198938991\n",
      "> 40 | 0.3365342617034912 | 0.5115622878074646 | 0.8473557692307692 | 0.7065649867374005\n",
      "> 41 | 0.4026113748550415 | 0.5201534628868103 | 0.8453249336870027 | 0.7060676392572944\n",
      "> 42 | 0.31029531359672546 | 0.5180908441543579 | 0.8456979442970822 | 0.7056117374005305\n",
      "> 43 | 0.7406406998634338 | 0.5703634023666382 | 0.8462367374005305 | 0.7056946286472148\n",
      "> 44 | 0.3386968970298767 | 0.5123559832572937 | 0.8472314323607427 | 0.7058604111405835\n",
      "> 45 | 0.30587440729141235 | 0.5161396265029907 | 0.8465268567639257 | 0.7045755968169761\n",
      "> 46 | 0.3076465129852295 | 0.5174976587295532 | 0.8459051724137931 | 0.7047413793103448\n",
      "> 47 | 0.3393915295600891 | 0.5140525102615356 | 0.8481017904509284 | 0.7047413793103448\n",
      "> 48 | 0.459125280380249 | 0.5376874804496765 | 0.8469413129973475 | 0.7042854774535809\n",
      "> 49 | 0.3068755865097046 | 0.5194693803787231 | 0.846112400530504 | 0.7041611405835544\n",
      "> 50 | 0.3506159484386444 | 0.5164654850959778 | 0.8478531167108754 | 0.7035809018567639\n",
      "> 51 | 0.3130476474761963 | 0.5138490796089172 | 0.8466097480106101 | 0.7037466843501325\n",
      "> 52 | 0.3143853545188904 | 0.5159209966659546 | 0.8464025198938991 | 0.7032907824933687\n",
      "> 53 | 0.33673661947250366 | 0.514961838722229 | 0.8476044429708223 | 0.7036223474801061\n",
      "> 54 | 0.3054092526435852 | 0.5199003219604492 | 0.8464439655172413 | 0.7027105437665783\n",
      "> 55 | 0.3157402276992798 | 0.5144760608673096 | 0.8466926392572944 | 0.7033322281167109\n",
      "> 56 | 0.971360445022583 | 0.5695567727088928 | 0.8438328912466844 | 0.7032078912466844\n",
      "> 57 | 0.35567641258239746 | 0.5197632312774658 | 0.8357924403183024 | 0.7028348806366047\n",
      "> 58 | 0.3951594829559326 | 0.524337112903595 | 0.846816976127321 | 0.7038710212201591\n",
      "> 59 | 0.3018586039543152 | 0.5195636749267578 | 0.8474801061007957 | 0.7036637931034483\n",
      "> 60 | 0.561726450920105 | 0.5595826506614685 | 0.8466926392572944 | 0.7036637931034483\n",
      "> 61 | 0.9693256616592407 | 0.5686631798744202 | 0.8431283156498673 | 0.7036223474801061\n",
      "> 62 | 0.9695810675621033 | 0.5676355361938477 | 0.8336787135278515 | 0.7024618700265253\n",
      "> 63 | 0.32186511158943176 | 0.5204856991767883 | 0.8381962864721485 | 0.7028348806366047\n",
      "> 64 | 0.634662389755249 | 0.565963625907898 | 0.8453663793103449 | 0.703125\n",
      "> 65 | 0.4227975606918335 | 0.5337038636207581 | 0.8437914456233422 | 0.7026276525198939\n",
      "> 66 | 0.33921170234680176 | 0.5196588039398193 | 0.8476873342175066 | 0.7019230769230769\n",
      "> 67 | 0.7249054312705994 | 0.5694124698638916 | 0.8439157824933687 | 0.7024204244031831\n",
      "> 68 | 0.331428587436676 | 0.5178430080413818 | 0.8473972148541113 | 0.7016744031830239\n",
      "> 69 | 0.31439441442489624 | 0.5171557664871216 | 0.847770225464191 | 0.7021303050397878\n",
      "> 70 | 0.968842625617981 | 0.5677516460418701 | 0.8435427718832891 | 0.7018816312997347\n",
      "> 71 | 0.9689949750900269 | 0.5667181611061096 | 0.8337616047745358 | 0.7016744031830239\n",
      "> 72 | 0.29940855503082275 | 0.5267928242683411 | 0.8376574933687002 | 0.7011356100795756\n",
      "> 73 | 0.41022148728370667 | 0.529517650604248 | 0.8457808355437666 | 0.7015086206896552\n",
      "> 74 | 0.3168894350528717 | 0.5378932356834412 | 0.8464439655172413 | 0.7019230769230769\n",
      "> 75 | 0.3467766046524048 | 0.5204858779907227 | 0.8452005968169761 | 0.7014257294429709\n",
      "> 76 | 0.5225982666015625 | 0.5521318316459656 | 0.8459880636604775 | 0.70121850132626\n",
      "> 77 | 0.29980117082595825 | 0.5214360356330872 | 0.847770225464191 | 0.7003481432360743\n",
      "> 78 | 0.8281383514404297 | 0.5692871809005737 | 0.8443302387267905 | 0.700513925729443\n",
      "> 79 | 0.33219587802886963 | 0.5191900730133057 | 0.8474386604774535 | 0.7008454907161803\n",
      "> 80 | 0.30079710483551025 | 0.524215042591095 | 0.8465683023872679 | 0.7001823607427056\n",
      "> 81 | 0.3661584258079529 | 0.5222452282905579 | 0.8459466180371353 | 0.7001823607427056\n",
      "> 82 | 0.4225050210952759 | 0.5324733853340149 | 0.8486405835543767 | 0.7002652519893899\n",
      "> 83 | 0.30965811014175415 | 0.5182190537452698 | 0.8472728779840849 | 0.6998507957559681\n",
      "> 84 | 0.7254566550254822 | 0.5698326230049133 | 0.8460709549071618 | 0.6992705570291777\n",
      "> 85 | 0.3275323510169983 | 0.5179750919342041 | 0.8432940981432361 | 0.7003481432360743\n",
      "> 86 | 0.33142316341400146 | 0.5185139179229736 | 0.8488892572944298 | 0.6997679045092838\n",
      "> 87 | 0.6337335109710693 | 0.565515398979187 | 0.8463610742705571 | 0.6991462201591512\n",
      "> 88 | 0.3335287868976593 | 0.5188658833503723 | 0.8477287798408488 | 0.699809350132626\n",
      "> 89 | 0.4227171540260315 | 0.5326404571533203 | 0.848972148541114 | 0.699560676392573\n",
      "> 90 | 0.3162688612937927 | 0.5180529952049255 | 0.8481017904509284 | 0.6994777851458887\n",
      "> 91 | 0.31807368993759155 | 0.5324082374572754 | 0.8495523872679045 | 0.6998922413793103\n",
      "> 92 | 0.9692997932434082 | 0.5694035291671753 | 0.8423408488063661 | 0.6999336870026525\n",
      "> 93 | 0.31327691674232483 | 0.5340578556060791 | 0.838776525198939 | 0.7017987400530503\n",
      "> 94 | 0.3175647258758545 | 0.5350820422172546 | 0.8472728779840849 | 0.7015500663129973\n",
      "> 95 | 0.9682249426841736 | 0.5687751770019531 | 0.8442473474801061 | 0.7012599469496021\n",
      "> 96 | 0.32097822427749634 | 0.5201853513717651 | 0.8362897877984085 | 0.7004724801061009\n",
      "> 97 | 0.32112815976142883 | 0.5376864075660706 | 0.8483090185676393 | 0.7016744031830239\n",
      "> 98 | 0.32337188720703125 | 0.5183165669441223 | 0.8401027851458887 | 0.7004310344827587\n",
      "> 99 | 0.3030101954936981 | 0.5192937850952148 | 0.8471070954907162 | 0.7005968169761273\n",
      "> 100 | 0.3094140291213989 | 0.5191315412521362 | 0.8486820291777188 | 0.7003895888594165\n",
      "> Class Acc | Adv Acc\n",
      "> 0.8315159574468085 | 0.6914228723404255\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8142503798007965 | 0.9093906804919243 | 0.9098092615604401\n",
      "> Confusion Matrix \n",
      "TN: 10304.0 | FP: 1038.0 \n",
      "FN: 1496.0 | TP: 2202.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 4196.0 | FP: 154.0 \n",
      "FN: 268.0 | TP: 289.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 6108.0 | FP: 884.0 \n",
      "FN: 1228.0 | TP: 1913.0\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Zhang for Eq Opp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "fairdef = 'EqOpp'\n",
    "\n",
    "for i in range(test_loop):\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    model = FairLogisticRegression(xdim, batch_size, fairdef)\n",
    "    zhang_train(model, raw_data, train_data, epochs, opt)\n",
    "\n",
    "    Y_hat, A_hat = evaluation(model, test_data)\n",
    "    clas_acc, dp, deqodds, deqopp, confusion_matrix, metrics_a0, metrics_a1 = compute_metrics(y_test, Y_hat, a_test, A_hat)\n",
    "\n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    tradeoff = []\n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "\n",
    "    result = ['Zhang4EqOpp', clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> Epoch | Class Loss | Adv Loss | Class Acc | Adv Acc\n",
      "> 1 | 0.4192975163459778 | 0.2229214310646057 | 0.7571286472148542 | 0.32949270557029176\n",
      "> 2 | 0.3927159011363983 | 0.16524718701839447 | 0.8105106100795756 | 0.42100464190981435\n",
      "> 3 | 0.4386526942253113 | 0.12488310039043427 | 0.8232344164456233 | 0.5319545755968169\n",
      "> 4 | 1.0402820110321045 | 0.11261849105358124 | 0.8345905172413793 | 0.596816976127321\n",
      "> 5 | 1.0342415571212769 | 0.10793241858482361 | 0.8287881299734748 | 0.6348640583554377\n",
      "> 6 | 0.38009536266326904 | 0.11873888969421387 | 0.8367042440318302 | 0.648914124668435\n",
      "> 7 | 0.3784114420413971 | 0.11871960014104843 | 0.8429210875331565 | 0.6554625331564987\n",
      "> 8 | 1.0152990818023682 | 0.10325539112091064 | 0.8420092838196287 | 0.6622181697612732\n",
      "> 9 | 0.9706503748893738 | 0.10375283658504486 | 0.8322695623342176 | 0.6661140583554377\n",
      "> 10 | 0.5435814261436462 | 0.10506999492645264 | 0.8389837533156499 | 0.6591097480106101\n",
      "> 11 | 0.3388862609863281 | 0.10621694475412369 | 0.8471485411140584 | 0.6645805702917772\n",
      "> 12 | 0.3364219069480896 | 0.10638493299484253 | 0.8466926392572944 | 0.6632957559681698\n",
      "> 13 | 0.35486871004104614 | 0.10487990081310272 | 0.8463196286472149 | 0.663337201591512\n",
      "> 14 | 0.3501189053058624 | 0.10511623322963715 | 0.8451177055702918 | 0.6620523872679045\n",
      "> 15 | 0.32774606347084045 | 0.10810287296772003 | 0.8466097480106101 | 0.6608504641909814\n",
      "> 16 | 0.9918988943099976 | 0.1050071269273758 | 0.8456150530503979 | 0.662881299734748\n",
      "> 17 | 0.9897734522819519 | 0.10529831051826477 | 0.8334300397877984 | 0.6645805702917772\n",
      "> 18 | 0.3210720717906952 | 0.10749373584985733 | 0.8350049734748011 | 0.6641246684350133\n",
      "> 19 | 0.33139461278915405 | 0.11596731841564178 | 0.8483919098143236 | 0.6622596153846154\n",
      "> 20 | 0.3253134489059448 | 0.11381269246339798 | 0.8476873342175066 | 0.6632957559681698\n",
      "> 21 | 0.3415631055831909 | 0.12036262452602386 | 0.8486405835543767 | 0.6644147877984086\n",
      "> 22 | 0.3222901225090027 | 0.11343210935592651 | 0.8478116710875332 | 0.6654094827586207\n",
      "> 23 | 0.33472299575805664 | 0.11887237429618835 | 0.8481017904509284 | 0.6654094827586207\n",
      "> 24 | 0.32992231845855713 | 0.1176355853676796 | 0.8485576923076923 | 0.6655338196286472\n",
      "> 25 | 0.3400372266769409 | 0.12096886336803436 | 0.8482261273209549 | 0.6655752652519894\n",
      "> 26 | 0.32065051794052124 | 0.10649475455284119 | 0.8437914456233422 | 0.6653680371352786\n",
      "> 27 | 0.37451785802841187 | 0.12628228962421417 | 0.8456979442970822 | 0.6650779177718833\n",
      "> 28 | 0.474250853061676 | 0.10631562769412994 | 0.8468584217506632 | 0.6654094827586207\n",
      "> 29 | 0.3110429644584656 | 0.1071125715970993 | 0.8481017904509284 | 0.6661969496021221\n",
      "> 30 | 0.37628620862960815 | 0.10606035590171814 | 0.8461952917771883 | 0.6651608090185677\n",
      "> 31 | 0.6817762851715088 | 0.10642807930707932 | 0.8488063660477454 | 0.6666114058355438\n",
      "> 32 | 0.6566712260246277 | 0.10648515820503235 | 0.847065649867374 | 0.6659068302387268\n",
      "> 33 | 0.31183308362960815 | 0.10714162141084671 | 0.8483504641909814 | 0.6667771883289124\n",
      "> 34 | 0.7334039211273193 | 0.10607099533081055 | 0.8460295092838196 | 0.6651608090185677\n",
      "> 35 | 0.5266148447990417 | 0.106291264295578 | 0.8468998673740054 | 0.665492374005305\n",
      "> 36 | 0.9732280969619751 | 0.10597459971904755 | 0.8444960212201591 | 0.6659068302387268\n",
      "> 37 | 0.9729255437850952 | 0.10623545944690704 | 0.8339273872679045 | 0.6670258620689655\n",
      "> 38 | 0.32389020919799805 | 0.11857356131076813 | 0.8421750663129973 | 0.6651608090185677\n",
      "> 39 | 0.9721319079399109 | 0.10625636577606201 | 0.8453249336870027 | 0.6658239389920424\n",
      "> 40 | 0.3187866508960724 | 0.11730913817882538 | 0.8428796419098143 | 0.6669844164456233\n",
      "> 41 | 0.30741870403289795 | 0.10709818452596664 | 0.8466511936339522 | 0.6663627320954908\n",
      "> 42 | 0.9704262614250183 | 0.10607194900512695 | 0.8458222811671088 | 0.6677718832891246\n",
      "> 43 | 0.9708376526832581 | 0.10636108368635178 | 0.834300397877984 | 0.6694297082228117\n",
      "> 44 | 0.3029312491416931 | 0.10719437897205353 | 0.8366213527851458 | 0.6680620026525199\n",
      "> 45 | 0.3032359480857849 | 0.11122952401638031 | 0.8481846816976127 | 0.6683935676392573\n",
      "> 46 | 0.30691200494766235 | 0.10654899477958679 | 0.8459466180371353 | 0.6677304376657824\n",
      "> 47 | 0.3219846785068512 | 0.11848539859056473 | 0.8511273209549072 | 0.6684350132625995\n",
      "> 48 | 0.3185485601425171 | 0.1065296158194542 | 0.8456979442970822 | 0.6679376657824934\n",
      "> 49 | 0.30466383695602417 | 0.11232081055641174 | 0.8495938328912467 | 0.6687251326259946\n",
      "> 50 | 0.32829374074935913 | 0.12002864480018616 | 0.8490964854111406 | 0.6690981432360743\n",
      "> 51 | 0.3178289234638214 | 0.11728864908218384 | 0.8468584217506632 | 0.6691810344827587\n",
      "> 52 | 0.3213558793067932 | 0.11839642375707626 | 0.8500497347480106 | 0.6698027188328912\n",
      "> 53 | 0.30538809299468994 | 0.11259634792804718 | 0.8482261273209549 | 0.6704658488063661\n",
      "> 54 | 0.3112110495567322 | 0.11514309048652649 | 0.8479774535809018 | 0.6709631962864722\n",
      "> 55 | 0.323864221572876 | 0.1189839318394661 | 0.8493866047745358 | 0.67092175066313\n",
      "> 56 | 0.3203331530094147 | 0.11798063665628433 | 0.8466926392572944 | 0.6711289787798409\n",
      "> 57 | 0.9688532948493958 | 0.1064193844795227 | 0.8463196286472149 | 0.6723309018567639\n",
      "> 58 | 0.9688014984130859 | 0.10652092844247818 | 0.8338859416445623 | 0.6730354774535809\n",
      "> 59 | 0.9060822129249573 | 0.10661284625530243 | 0.834300397877984 | 0.6738229442970822\n",
      "> 60 | 0.3032526969909668 | 0.11201580613851547 | 0.837367374005305 | 0.6733670424403183\n",
      "> 61 | 0.30764496326446533 | 0.11375055462121964 | 0.8495523872679045 | 0.6741959549071618\n",
      "> 62 | 0.311102032661438 | 0.11498475074768066 | 0.8495523872679045 | 0.6724552387267905\n",
      "> 63 | 0.32684069871902466 | 0.11907394230365753 | 0.848930702917772 | 0.6722894562334217\n",
      "> 64 | 0.33761122822761536 | 0.10626325011253357 | 0.8480603448275862 | 0.6723309018567639\n",
      "> 65 | 0.3145095705986023 | 0.10631749033927917 | 0.8467755305039788 | 0.6730354774535809\n",
      "> 66 | 0.29938504099845886 | 0.10743632912635803 | 0.8457808355437666 | 0.6730769230769231\n",
      "> 67 | 0.692165732383728 | 0.10593096911907196 | 0.8455321618037135 | 0.6741130636604775\n",
      "> 68 | 0.44020846486091614 | 0.10616985708475113 | 0.8452005968169761 | 0.6746104111405835\n",
      "> 69 | 0.40773504972457886 | 0.10571114718914032 | 0.8486405835543767 | 0.6749834217506632\n",
      "> 70 | 0.3102579116821289 | 0.1060292050242424 | 0.8473557692307692 | 0.6755222148541113\n",
      "> 71 | 0.3285995125770569 | 0.10583901405334473 | 0.848723474801061 | 0.6758537798408488\n",
      "> 72 | 0.384660005569458 | 0.1060410737991333 | 0.8478531167108754 | 0.6770971485411141\n",
      "> 73 | 0.31688612699508667 | 0.10603664070367813 | 0.8478531167108754 | 0.6772214854111406\n",
      "> 74 | 0.5044139623641968 | 0.10593025386333466 | 0.8464854111405835 | 0.677055702917772\n",
      "> 75 | 0.29806530475616455 | 0.1092669814825058 | 0.8476458885941645 | 0.677304376657825\n",
      "> 76 | 0.7363159656524658 | 0.106046661734581 | 0.8469827586206897 | 0.6769728116710876\n",
      "> 77 | 0.34951168298721313 | 0.10490450263023376 | 0.8455321618037135 | 0.6763511273209549\n",
      "> 78 | 0.46958982944488525 | 0.10552172362804413 | 0.8498425066312998 | 0.6767241379310345\n",
      "> 79 | 0.6679192781448364 | 0.10568451881408691 | 0.8490135941644562 | 0.6759366710875332\n",
      "> 80 | 0.3032991290092468 | 0.10620977729558945 | 0.8478116710875332 | 0.6760195623342176\n",
      "> 81 | 0.3907557725906372 | 0.10565023869276047 | 0.8481846816976127 | 0.6757708885941645\n",
      "> 82 | 0.31958669424057007 | 0.10567200928926468 | 0.8479774535809018 | 0.6741545092838197\n",
      "> 83 | 0.325128972530365 | 0.10527565330266953 | 0.8476873342175066 | 0.6738229442970822\n",
      "> 84 | 0.30503731966018677 | 0.11188431829214096 | 0.8506714190981433 | 0.6744446286472148\n",
      "> 85 | 0.3186449408531189 | 0.11598365008831024 | 0.8502155172413793 | 0.6746104111405835\n",
      "> 86 | 0.32460200786590576 | 0.11714162677526474 | 0.8483504641909814 | 0.6730769230769231\n",
      "> 87 | 0.31897711753845215 | 0.11599143594503403 | 0.8479774535809018 | 0.6727868037135278\n",
      "> 88 | 0.3135441541671753 | 0.11464200913906097 | 0.8479360079575597 | 0.6719993368700266\n",
      "> 89 | 0.3243580758571625 | 0.11702840775251389 | 0.848930702917772 | 0.6716263262599469\n",
      "> 90 | 0.9658515453338623 | 0.10512338578701019 | 0.8450348143236074 | 0.6719164456233422\n",
      "> 91 | 0.9658978581428528 | 0.10522447526454926 | 0.8342175066312998 | 0.6710875331564987\n",
      "> 92 | 0.9663495421409607 | 0.10523104667663574 | 0.834507625994695 | 0.6713362068965517\n",
      "> 93 | 0.331909716129303 | 0.11773213744163513 | 0.8438743368700266 | 0.6707559681697612\n",
      "> 94 | 0.3232548236846924 | 0.11621914803981781 | 0.8446618037135278 | 0.6712533156498675\n",
      "> 95 | 0.3209570050239563 | 0.11572044342756271 | 0.8473143236074271 | 0.6711704244031831\n",
      "> 96 | 0.31762397289276123 | 0.11488474160432816 | 0.8485162466843501 | 0.6709631962864722\n",
      "> 97 | 0.31836867332458496 | 0.11489585041999817 | 0.8486820291777188 | 0.6709631962864722\n",
      "> 98 | 0.322651743888855 | 0.11559570580720901 | 0.8487649204244032 | 0.6712533156498675\n",
      "> 99 | 0.3187613785266876 | 0.11472184956073761 | 0.8485576923076923 | 0.6719578912466844\n",
      "> 100 | 0.3052004873752594 | 0.11091577261686325 | 0.8485991379310345 | 0.6717092175066313\n",
      "> Class Acc | Adv Acc\n",
      "> 0.8359042553191489 | 0.6712101063829787\n",
      "> DP | DEqOdds | DEqOpp\n",
      "> 0.8129555433988571 | 0.9016890861093998 | 0.8931557238101959\n",
      "> Confusion Matrix \n",
      "TN: 10404.0 | FP: 938.0 \n",
      "FN: 1530.0 | TP: 2168.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 4231.0 | FP: 119.0 \n",
      "FN: 281.0 | TP: 276.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 6173.0 | FP: 819.0 \n",
      "FN: 1249.0 | TP: 1892.0\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving into DF then CSV"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "result_df = pd.DataFrame(results, columns=header)\n",
    "result_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     model_name  clas_acc        dp   deqodds    deqopp  trade_dp  \\\n",
       "0      Zhang4DP  0.829122  0.816432  0.913289  0.917105  0.822728   \n",
       "1  Zhang4EqOdds  0.831516  0.814250  0.909391  0.909809  0.822793   \n",
       "2   Zhang4EqOpp  0.835904  0.812956  0.901689  0.893156  0.824270   \n",
       "\n",
       "   trade_deqodds  trade_deqopp   TN_a0  FP_a0  FN_a0  TP_a0   TN_a1  FP_a1  \\\n",
       "0       0.869173      0.870897  4182.0  168.0  265.0  292.0  6089.0  903.0   \n",
       "1       0.868712      0.868903  4196.0  154.0  268.0  289.0  6108.0  884.0   \n",
       "2       0.867551      0.863582  4231.0  119.0  281.0  276.0  6173.0  819.0   \n",
       "\n",
       "    FN_a1   TP_a1  \n",
       "0  1234.0  1907.0  \n",
       "1  1228.0  1913.0  \n",
       "2  1249.0  1892.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>clas_acc</th>\n",
       "      <th>dp</th>\n",
       "      <th>deqodds</th>\n",
       "      <th>deqopp</th>\n",
       "      <th>trade_dp</th>\n",
       "      <th>trade_deqodds</th>\n",
       "      <th>trade_deqopp</th>\n",
       "      <th>TN_a0</th>\n",
       "      <th>FP_a0</th>\n",
       "      <th>FN_a0</th>\n",
       "      <th>TP_a0</th>\n",
       "      <th>TN_a1</th>\n",
       "      <th>FP_a1</th>\n",
       "      <th>FN_a1</th>\n",
       "      <th>TP_a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zhang4DP</td>\n",
       "      <td>0.829122</td>\n",
       "      <td>0.816432</td>\n",
       "      <td>0.913289</td>\n",
       "      <td>0.917105</td>\n",
       "      <td>0.822728</td>\n",
       "      <td>0.869173</td>\n",
       "      <td>0.870897</td>\n",
       "      <td>4182.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>6089.0</td>\n",
       "      <td>903.0</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>1907.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zhang4EqOdds</td>\n",
       "      <td>0.831516</td>\n",
       "      <td>0.814250</td>\n",
       "      <td>0.909391</td>\n",
       "      <td>0.909809</td>\n",
       "      <td>0.822793</td>\n",
       "      <td>0.868712</td>\n",
       "      <td>0.868903</td>\n",
       "      <td>4196.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>6108.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>1228.0</td>\n",
       "      <td>1913.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zhang4EqOpp</td>\n",
       "      <td>0.835904</td>\n",
       "      <td>0.812956</td>\n",
       "      <td>0.901689</td>\n",
       "      <td>0.893156</td>\n",
       "      <td>0.824270</td>\n",
       "      <td>0.867551</td>\n",
       "      <td>0.863582</td>\n",
       "      <td>4231.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>6173.0</td>\n",
       "      <td>819.0</td>\n",
       "      <td>1249.0</td>\n",
       "      <td>1892.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#result_df.to_csv('results/test_fairgan-100.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34ca74ed6235dfc7dda926bb3adb31e801e3d02679121d5b444ee035e270bd57"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('falsb': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}