{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import libs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from math import sqrt, isnan\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "from util import metrics\n",
    "from util.load_data import load_data\n",
    "from util.evaluation import compute_tradeoff\n",
    "\n",
    "from zhang.models import UnfairLogisticRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "opt = Adam(learning_rate=lr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "header = \"model_name\", \"clas_acc\", \"dp\", \"deqodds\", \"deqopp\", \"trade_dp\", \"trade_deqodds\", \"trade_deqopp\", \"TN_a0\", \"FP_a0\", \"FN_a0\", \"TP_a0\", \"TN_a1\", \"FP_a1\", \"FN_a1\", \"TP_a1\"\n",
    "results = []\n",
    "\n",
    "test_loop = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x_train, y_train, a_train = load_data('adult', 'train')\n",
    "raw_data = (x_train, y_train, a_train)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "xdim = x_train.shape[1]\n",
    "ydim = y_train.shape[1]\n",
    "adim = a_train.shape[1]\n",
    "zdim = 8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_data = Dataset.from_tensor_slices((x_train, y_train, a_train))\n",
    "train_data = train_data.batch(batch_size, drop_remainder=True)\n",
    "train_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 113), (64, 1), (64, 1)), types: (tf.float64, tf.float64, tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "x_valid, y_valid, a_valid = load_data('adult', 'valid')\n",
    "\n",
    "valid_data = Dataset.from_tensor_slices((x_valid, y_valid, a_valid))\n",
    "valid_data = valid_data.batch(batch_size, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "x_test, y_test, a_test = load_data('adult', 'test')\n",
    "\n",
    "test_data = Dataset.from_tensor_slices((x_test, y_test, a_test))\n",
    "test_data = test_data.batch(batch_size, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "num_batchs = y_test.shape[0]//batch_size\n",
    "num_batchs *= batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "y_test = y_test[:num_batchs]\n",
    "a_test = a_test[:num_batchs]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def train(model, X, Y, A, optimizer):\n",
    "    clas_vars = [model.clas.W, model.b]\n",
    "    \n",
    "    with tf.GradientTape() as clas_tape:\n",
    "        \n",
    "        model(X, Y, A)        \n",
    "        clas_loss = model.clas_loss\n",
    "\n",
    "    dWLp = clas_tape.gradient(clas_loss, clas_vars)\n",
    "    optimizer.apply_gradients(zip(dWLp, clas_vars))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def train_loop(model, train_dataset, epochs, opt=None):\n",
    "    \n",
    "    print(\"> Epoch | Class Loss | Class Acc\")\n",
    "\n",
    "    if opt is not None:\n",
    "        optimizer = opt\n",
    "        decay4epoch = False\n",
    "    else:\n",
    "        decay4epoch = True\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        Y_hat = None\n",
    "        batch_count = 1\n",
    "\n",
    "        if decay4epoch:\n",
    "            lr = 0.001/(epoch+1)\n",
    "            optimizer = Adam(learning_rate=lr)\n",
    "        \n",
    "        for X, Y, A in train_dataset:\n",
    "            \n",
    "            r = train(model, X, Y, A, optimizer)\n",
    "            if r:\n",
    "                print('parou')\n",
    "                print(model.clas_loss)\n",
    "                break\n",
    "\n",
    "            if batch_count == 1:\n",
    "                Y_hat = model.Y_hat\n",
    "                batch_count += 1\n",
    "            else:\n",
    "                Y_hat = tf.concat([Y_hat, model.Y_hat], 0)\n",
    "\n",
    "        clas_loss = model.clas_loss\n",
    "        clas_acc = metrics.accuracy(raw_data[1], tf.math.round(Y_hat))\n",
    "\n",
    "        \n",
    "    \n",
    "        print(\"> {} | {} | {}\".format(\n",
    "            epoch+1, \n",
    "            clas_loss, \n",
    "            clas_acc))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "\n",
    "def evaluation(model, valid_data):\n",
    "    Y_hat = None\n",
    "    batch_count = 1\n",
    "    \n",
    "    for X, Y, A in valid_data:\n",
    "        \n",
    "        model(X, Y, A)\n",
    "        \n",
    "        if batch_count == 1:\n",
    "            Y_hat = model.Y_hat\n",
    "            batch_count += 1\n",
    "        else:\n",
    "            Y_hat = tf.concat([Y_hat, model.Y_hat], 0)\n",
    "    \n",
    "    return Y_hat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def compute_metrics(Y, Y_hat, A):\n",
    "    Y_hat = tf.math.round(Y_hat)\n",
    "    \n",
    "    clas_acc = metrics.accuracy(Y, Y_hat)\n",
    "\n",
    "    print(\"> Class Acc\")\n",
    "    print(\"> {}\".format(clas_acc))\n",
    "\n",
    "    dp = metrics.DP(Y_hat.numpy(), A)\n",
    "    deqodds = metrics.DEqOdds(Y, Y_hat.numpy(), A)\n",
    "    deqopp = metrics.DEqOpp(Y, Y_hat.numpy(), A)\n",
    "\n",
    "    print(\"> DP | DI | DEOPP\")\n",
    "    print(\"> {} | {} | {}\".format(dp, deqodds, deqopp))\n",
    "\n",
    "    tp = metrics.TP(Y, Y_hat.numpy())\n",
    "    tn = metrics.TN(Y, Y_hat.numpy())\n",
    "    fp = metrics.FP(Y, Y_hat.numpy())\n",
    "    fn = metrics.FN(Y, Y_hat.numpy())\n",
    "\n",
    "    print('> Confusion Matrix \\n' +\n",
    "                'TN: {} | FP: {} \\n'.format(tn, fp) +\n",
    "                'FN: {} | TP: {}'.format(fn, tp))\n",
    "\n",
    "    confusion_matrix = np.array([[tn, fp],\n",
    "                                [fn, tp]])\n",
    "\n",
    "    m = [metrics.TN, metrics.FP, metrics.FN, metrics.TP]\n",
    "    metrics_a0 = [0, 0, 0, 0]\n",
    "    metrics_a1 = [0, 0, 0, 0]\n",
    "    for i in range(len(m)):\n",
    "        metrics_a0[i] = metrics.subgroup(m[i], A, Y, Y_hat.numpy())\n",
    "        metrics_a1[i] = metrics.subgroup(m[i], 1 - A, Y, Y_hat.numpy())\n",
    "\n",
    "    print('> Confusion Matrix for A = 0 \\n' +\n",
    "            'TN: {} | FP: {} \\n'.format(metrics_a0[0], metrics_a0[1]) +\n",
    "            'FN: {} | TP: {}'.format(metrics_a0[2], metrics_a0[3]))\n",
    "\n",
    "    print('> Confusion Matrix for A = 1 \\n' +\n",
    "            'TN: {} | FP: {} \\n'.format(metrics_a1[0], metrics_a1[1]) +\n",
    "            'FN: {} | TP: {}'.format(metrics_a1[2], metrics_a1[3]))\n",
    "\n",
    "    confusion_matrix = np.array([[tn, fp],\n",
    "                                [fn, tp]])\n",
    "\n",
    "    return clas_acc, confusion_matrix, dp, deqodds, deqopp, metrics_a0, metrics_a1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "for i in range(test_loop):\n",
    "\n",
    "    model = UnfairLogisticRegression(xdim, batch_size)\n",
    "    ret = train_loop(model, train_data, epochs)\n",
    "    Y_hat = evaluation(model, test_data)\n",
    "    \n",
    "    clas_acc, confusion_matrix, dp, deqodds, deqopp, metrics_a0, metrics_a1  = compute_metrics(y_test, Y_hat, a_test)\n",
    "    \n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    \n",
    "    tradeoff = []\n",
    "    \n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "    \n",
    "    result = ['UnfairLR-decay', clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5004817843437195 | 0.7487566312997347\n",
      "> 2 | 0.4250468909740448 | 0.7940981432360743\n",
      "> 3 | 0.38422220945358276 | 0.8143236074270557\n",
      "> 4 | 0.36236947774887085 | 0.8235245358090186\n",
      "> 5 | 0.35435330867767334 | 0.8309433023872679\n",
      "> 6 | 0.3480228781700134 | 0.8362068965517242\n",
      "> 7 | 0.3438219428062439 | 0.8391495358090186\n",
      "> 8 | 0.34053272008895874 | 0.8399370026525199\n",
      "> 9 | 0.33758699893951416 | 0.8413461538461539\n",
      "> 10 | 0.3348346948623657 | 0.8418849469496021\n",
      "> 11 | 0.33249279856681824 | 0.8425895225464191\n",
      "> 12 | 0.3304925858974457 | 0.8430039787798409\n",
      "> 13 | 0.3288639783859253 | 0.8434184350132626\n",
      "> 14 | 0.3274797797203064 | 0.8440401193633953\n",
      "> 15 | 0.32632455229759216 | 0.8445789124668435\n",
      "> 16 | 0.3253505527973175 | 0.8450348143236074\n",
      "> 17 | 0.32451677322387695 | 0.8451591511936339\n",
      "> 18 | 0.3237929940223694 | 0.8454078249336869\n",
      "> 19 | 0.323161780834198 | 0.8455736074270557\n",
      "> 20 | 0.32261425256729126 | 0.8460709549071618\n",
      "> 21 | 0.3222368061542511 | 0.8459880636604775\n",
      "> 22 | 0.3218652009963989 | 0.8459466180371353\n",
      "> 23 | 0.32156968116760254 | 0.8460295092838196\n",
      "> 24 | 0.32134681940078735 | 0.8460709549071618\n",
      "> 25 | 0.3213326334953308 | 0.8464439655172413\n",
      "> 26 | 0.32126009464263916 | 0.8466511936339522\n",
      "> 27 | 0.3213502764701843 | 0.8467755305039788\n",
      "> 28 | 0.3213960528373718 | 0.846816976127321\n",
      "> 29 | 0.32156500220298767 | 0.8468584217506632\n",
      "> 30 | 0.3217525780200958 | 0.8467755305039788\n",
      "> 31 | 0.32191404700279236 | 0.8468998673740054\n",
      "> 32 | 0.3221426010131836 | 0.8468584217506632\n",
      "> 33 | 0.3223761320114136 | 0.8469827586206897\n",
      "> 34 | 0.3226112723350525 | 0.8471070954907162\n",
      "> 35 | 0.3228435218334198 | 0.8472728779840849\n",
      "> 36 | 0.32306960225105286 | 0.8472314323607427\n",
      "> 37 | 0.3232862055301666 | 0.8475629973474801\n",
      "> 38 | 0.3235009014606476 | 0.8476458885941645\n",
      "> 39 | 0.32368916273117065 | 0.847770225464191\n",
      "> 40 | 0.32386183738708496 | 0.8478116710875332\n",
      "> 41 | 0.3240176737308502 | 0.8481432360742706\n",
      "> 42 | 0.3241559863090515 | 0.8481846816976127\n",
      "> 43 | 0.32427653670310974 | 0.8481017904509284\n",
      "> 44 | 0.32437941431999207 | 0.8480603448275862\n",
      "> 45 | 0.32446491718292236 | 0.8482261273209549\n",
      "> 46 | 0.32453370094299316 | 0.8485162466843501\n",
      "> 47 | 0.32458627223968506 | 0.8486820291777188\n",
      "> 48 | 0.3246217370033264 | 0.8486405835543767\n",
      "> 49 | 0.32464492321014404 | 0.8490135941644562\n",
      "> 50 | 0.3246545195579529 | 0.848930702917772\n",
      "> 51 | 0.3246504068374634 | 0.8490964854111406\n",
      "> 52 | 0.32463592290878296 | 0.849179376657825\n",
      "> 53 | 0.3246098756790161 | 0.8492622679045093\n",
      "> 54 | 0.32457491755485535 | 0.849179376657825\n",
      "> 55 | 0.32453036308288574 | 0.8493451591511936\n",
      "> 56 | 0.32447782158851624 | 0.8494280503978779\n",
      "> 57 | 0.32441800832748413 | 0.8495109416445623\n",
      "> 58 | 0.32435208559036255 | 0.8494694960212201\n",
      "> 59 | 0.32427978515625 | 0.8495523872679045\n",
      "> 60 | 0.3242023289203644 | 0.8495523872679045\n",
      "> 61 | 0.3241201341152191 | 0.8495938328912467\n",
      "> 62 | 0.3240337669849396 | 0.849676724137931\n",
      "> 63 | 0.32394373416900635 | 0.8498010610079576\n",
      "> 64 | 0.32385045289993286 | 0.8497181697612732\n",
      "> 65 | 0.32375437021255493 | 0.8498425066312998\n",
      "> 66 | 0.32365599274635315 | 0.8498425066312998\n",
      "> 67 | 0.3235553503036499 | 0.8499668435013262\n",
      "> 68 | 0.32345306873321533 | 0.8500082891246684\n",
      "> 69 | 0.3233492970466614 | 0.8500082891246684\n",
      "> 70 | 0.32324427366256714 | 0.849925397877984\n",
      "> 71 | 0.3231383264064789 | 0.8499668435013262\n",
      "> 72 | 0.32303160429000854 | 0.8499668435013262\n",
      "> 73 | 0.32292431592941284 | 0.8499668435013262\n",
      "> 74 | 0.3228166401386261 | 0.8500497347480106\n",
      "> 75 | 0.3227088153362274 | 0.8500911803713528\n",
      "> 76 | 0.3226008713245392 | 0.8501740716180372\n",
      "> 77 | 0.3224930167198181 | 0.850132625994695\n",
      "> 78 | 0.322385311126709 | 0.8501740716180372\n",
      "> 79 | 0.32227787375450134 | 0.8500082891246684\n",
      "> 80 | 0.32217085361480713 | 0.8500911803713528\n",
      "> 81 | 0.32206428050994873 | 0.850132625994695\n",
      "> 82 | 0.32195866107940674 | 0.8501740716180372\n",
      "> 83 | 0.3218532204627991 | 0.8502569628647215\n",
      "> 84 | 0.3217484951019287 | 0.850381299734748\n",
      "> 85 | 0.3216444253921509 | 0.850381299734748\n",
      "> 86 | 0.3215411305427551 | 0.8504227453580901\n",
      "> 87 | 0.3214385509490967 | 0.8504227453580901\n",
      "> 88 | 0.32133686542510986 | 0.850381299734748\n",
      "> 89 | 0.3212360143661499 | 0.850381299734748\n",
      "> 90 | 0.32113611698150635 | 0.8503398541114058\n",
      "> 91 | 0.3210369944572449 | 0.8502984084880636\n",
      "> 92 | 0.3209388256072998 | 0.8502984084880636\n",
      "> 93 | 0.32084164023399353 | 0.8502984084880636\n",
      "> 94 | 0.32074588537216187 | 0.8502984084880636\n",
      "> 95 | 0.3206506371498108 | 0.8503398541114058\n",
      "> 96 | 0.3205562233924866 | 0.8504641909814323\n",
      "> 97 | 0.32046279311180115 | 0.8505056366047745\n",
      "> 98 | 0.32037031650543213 | 0.8505885278514589\n",
      "> 99 | 0.3202787935733795 | 0.8506299734748011\n",
      "> 100 | 0.3201882839202881 | 0.8505470822281167\n",
      "> Class Acc\n",
      "> 0.8488696808510638\n",
      "> DP | DI | DEOPP\n",
      "> 0.8031977713108063 | 0.8768594339489937 | 0.840693861246109\n",
      "> Confusion Matrix \n",
      "TN: 10567.0 | FP: 775.0 \n",
      "FN: 1498.0 | TP: 2200.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 4286.0 | FP: 64.0 \n",
      "FN: 301.0 | TP: 256.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 6281.0 | FP: 711.0 \n",
      "FN: 1197.0 | TP: 1944.0\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "sns.heatmap(confusion_matrix, annot=True, fmt='g')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5614d62c70>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "for i in range(test_loop):\n",
    "    opt = Adam(learning_rate=lr)\n",
    "\n",
    "    model = UnfairLogisticRegression(xdim, batch_size)\n",
    "    ret = train_loop(model, train_data, epochs, opt)\n",
    "    Y_hat = evaluation(model, test_data)\n",
    "    \n",
    "    clas_acc, confusion_matrix, dp, deqodds, deqopp, metrics_a0, metrics_a1  = compute_metrics(y_test, Y_hat, a_test)\n",
    "    \n",
    "    fair_metrics = (dp, deqodds, deqopp)\n",
    "    \n",
    "    tradeoff = []\n",
    "    \n",
    "    for fair_metric in fair_metrics:\n",
    "        tradeoff.append(compute_tradeoff(clas_acc, fair_metric))\n",
    "    \n",
    "    result = ['UnfairLR', clas_acc, dp, deqodds, deqopp, tradeoff[0], tradeoff[1], tradeoff[2]] + metrics_a0 + metrics_a1\n",
    "\n",
    "    results.append(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> Epoch | Class Loss | Class Acc\n",
      "> 1 | 0.5004817843437195 | 0.7487566312997347\n",
      "> 2 | 0.43046826124191284 | 0.7967092175066313\n",
      "> 3 | 0.4265057444572449 | 0.8164787798408488\n",
      "> 4 | 0.3977872431278229 | 0.828125\n",
      "> 5 | 0.3788052797317505 | 0.8338030503978779\n",
      "> 6 | 0.3722444176673889 | 0.8389837533156499\n",
      "> 7 | 0.35932254791259766 | 0.8396468832891246\n",
      "> 8 | 0.365489661693573 | 0.8395639920424403\n",
      "> 9 | 0.35043689608573914 | 0.841387599469496\n",
      "> 10 | 0.35543712973594666 | 0.8425480769230769\n",
      "> 11 | 0.34435534477233887 | 0.8445374668435013\n",
      "> 12 | 0.9974395036697388 | 0.8433355437665783\n",
      "> 13 | 0.33965668082237244 | 0.839729774535809\n",
      "> 14 | 0.531301736831665 | 0.8430868700265252\n",
      "> 15 | 0.3296221196651459 | 0.8480603448275862\n",
      "> 16 | 0.9852874875068665 | 0.8441644562334217\n",
      "> 17 | 0.9825006127357483 | 0.8331399204244032\n",
      "> 18 | 0.33471840620040894 | 0.8358338859416445\n",
      "> 19 | 0.32079070806503296 | 0.8455321618037135\n",
      "> 20 | 0.31524109840393066 | 0.8468998673740054\n",
      "> 21 | 0.3298489451408386 | 0.8467755305039788\n",
      "> 22 | 0.33171743154525757 | 0.847065649867374\n",
      "> 23 | 0.3064884841442108 | 0.8462781830238727\n",
      "> 24 | 0.30767932534217834 | 0.846816976127321\n",
      "> 25 | 0.3207545280456543 | 0.8449519230769231\n",
      "> 26 | 0.3529583513736725 | 0.8448690318302388\n",
      "> 27 | 0.3010445237159729 | 0.8455736074270557\n",
      "> 28 | 0.30363672971725464 | 0.8446618037135278\n",
      "> 29 | 0.37016624212265015 | 0.8454078249336869\n",
      "> 30 | 0.36420905590057373 | 0.8449519230769231\n",
      "> 31 | 0.3649325966835022 | 0.8450762599469496\n",
      "> 32 | 0.3665430247783661 | 0.8449104774535809\n",
      "> 33 | 0.36618772149086 | 0.8447861405835544\n",
      "> 34 | 0.3656221032142639 | 0.8447446949602122\n",
      "> 35 | 0.36475372314453125 | 0.84470324933687\n",
      "> 36 | 0.36340659856796265 | 0.8447861405835544\n",
      "> 37 | 0.3622782528400421 | 0.8448690318302388\n",
      "> 38 | 0.35984015464782715 | 0.8450348143236074\n",
      "> 39 | 0.3587883412837982 | 0.8452834880636605\n",
      "> 40 | 0.6900540590286255 | 0.8452005968169761\n",
      "> 41 | 0.6427038311958313 | 0.8457393899204244\n",
      "> 42 | 0.55645751953125 | 0.8443302387267905\n",
      "> 43 | 0.5696516036987305 | 0.8460709549071618\n",
      "> 44 | 0.5468385815620422 | 0.8456979442970822\n",
      "> 45 | 0.570908784866333 | 0.845863726790451\n",
      "> 46 | 0.5315980315208435 | 0.84565649867374\n",
      "> 47 | 0.5473337173461914 | 0.8454907161803713\n",
      "> 48 | 0.29311901330947876 | 0.8459466180371353\n",
      "> 49 | 0.3044631779193878 | 0.8473972148541113\n",
      "> 50 | 0.9594934582710266 | 0.8436671087533156\n",
      "> 51 | 0.9587709307670593 | 0.8329741379310345\n",
      "> 52 | 0.3100737929344177 | 0.8356681034482758\n",
      "> 53 | 0.29901498556137085 | 0.8406001326259946\n",
      "> 54 | 0.674165666103363 | 0.8455321618037135\n",
      "> 55 | 0.3126305341720581 | 0.8440815649867374\n",
      "> 56 | 0.3004017174243927 | 0.8454078249336869\n",
      "> 57 | 0.6395255923271179 | 0.8445374668435013\n",
      "> 58 | 0.5965968370437622 | 0.8466097480106101\n",
      "> 59 | 0.49927520751953125 | 0.8448275862068966\n",
      "> 60 | 0.6232485771179199 | 0.8457808355437666\n",
      "> 61 | 0.5064589381217957 | 0.84565649867374\n",
      "> 62 | 0.37210291624069214 | 0.8454492705570291\n",
      "> 63 | 0.9264265298843384 | 0.8430454244031831\n",
      "> 64 | 0.48951971530914307 | 0.8437085543766578\n",
      "> 65 | 0.3000060021877289 | 0.8465268567639257\n",
      "> 66 | 0.37913942337036133 | 0.8452005968169761\n",
      "> 67 | 0.2903483211994171 | 0.8459051724137931\n",
      "> 68 | 0.32009345293045044 | 0.8475629973474801\n",
      "> 69 | 0.29699257016181946 | 0.8486820291777188\n",
      "> 70 | 0.30239009857177734 | 0.8484333554376657\n",
      "> 71 | 0.3119325041770935 | 0.8468584217506632\n",
      "> 72 | 0.4825785160064697 | 0.8435427718832891\n",
      "> 73 | 0.956406831741333 | 0.8455321618037135\n",
      "> 74 | 0.28500881791114807 | 0.8362897877984085\n",
      "> 75 | 0.9548406600952148 | 0.8449933687002653\n",
      "> 76 | 0.9552003145217896 | 0.8331813660477454\n",
      "> 77 | 0.883773684501648 | 0.8338030503978779\n",
      "> 78 | 0.9543685913085938 | 0.8435842175066313\n",
      "> 79 | 0.95545494556427 | 0.8335129310344828\n",
      "> 80 | 0.9557806849479675 | 0.8337616047745358\n",
      "> 81 | 0.9543759822845459 | 0.8343418435013262\n",
      "> 82 | 0.9553747773170471 | 0.8340931697612732\n",
      "> 83 | 0.3003416657447815 | 0.8363312334217506\n",
      "> 84 | 0.9537608623504639 | 0.8410145888594165\n",
      "> 85 | 0.9548515677452087 | 0.8338444960212201\n",
      "> 86 | 0.9124616980552673 | 0.8343418435013262\n",
      "> 87 | 0.28961095213890076 | 0.8441230106100795\n",
      "> 88 | 0.49384069442749023 | 0.8449933687002653\n",
      "> 89 | 0.2957754135131836 | 0.8432940981432361\n",
      "> 90 | 0.9442521333694458 | 0.8367456896551724\n",
      "> 91 | 0.43822309374809265 | 0.8422165119363395\n",
      "> 92 | 0.28934764862060547 | 0.8462367374005305\n",
      "> 93 | 0.351669579744339 | 0.8474801061007957\n",
      "> 94 | 0.32269006967544556 | 0.8481846816976127\n",
      "> 95 | 0.35705673694610596 | 0.8453249336870027\n",
      "> 96 | 0.46844547986984253 | 0.8455736074270557\n",
      "> 97 | 0.40311330556869507 | 0.8462781830238727\n",
      "> 98 | 0.2937549650669098 | 0.8443716843501327\n",
      "> 99 | 0.3626859784126282 | 0.8472314323607427\n",
      "> 100 | 0.3477860987186432 | 0.8452005968169761\n",
      "> Class Acc\n",
      "> 0.8287898936170213\n",
      "> DP | DI | DEOPP\n",
      "> 0.8179635107517242 | 0.9148323759436607 | 0.9185824692249298\n",
      "> Confusion Matrix \n",
      "TN: 10264.0 | FP: 1078.0 \n",
      "FN: 1497.0 | TP: 2201.0\n",
      "> Confusion Matrix for A = 0 \n",
      "TN: 4175.0 | FP: 175.0 \n",
      "FN: 264.0 | TP: 293.0\n",
      "> Confusion Matrix for A = 1 \n",
      "TN: 6089.0 | FP: 903.0 \n",
      "FN: 1233.0 | TP: 1908.0\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "sns.heatmap(confusion_matrix, annot=True, fmt='g')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5526da1550>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving into DF then CSV"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "result_df = pd.DataFrame(results, columns=header)\n",
    "result_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       model_name  clas_acc        dp   deqodds    deqopp  trade_dp  \\\n",
       "0  UnfairLR-decay   0.84887  0.803198  0.876859  0.840694  0.825402   \n",
       "1        UnfairLR   0.82879  0.817964  0.914832  0.918582  0.823341   \n",
       "\n",
       "   trade_deqodds  trade_deqopp   TN_a0  FP_a0  FN_a0  TP_a0   TN_a1  FP_a1  \\\n",
       "0       0.862638      0.844762  4286.0   64.0  301.0  256.0  6281.0  711.0   \n",
       "1       0.869688      0.871379  4175.0  175.0  264.0  293.0  6089.0  903.0   \n",
       "\n",
       "    FN_a1   TP_a1  \n",
       "0  1197.0  1944.0  \n",
       "1  1233.0  1908.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>clas_acc</th>\n",
       "      <th>dp</th>\n",
       "      <th>deqodds</th>\n",
       "      <th>deqopp</th>\n",
       "      <th>trade_dp</th>\n",
       "      <th>trade_deqodds</th>\n",
       "      <th>trade_deqopp</th>\n",
       "      <th>TN_a0</th>\n",
       "      <th>FP_a0</th>\n",
       "      <th>FN_a0</th>\n",
       "      <th>TP_a0</th>\n",
       "      <th>TN_a1</th>\n",
       "      <th>FP_a1</th>\n",
       "      <th>FN_a1</th>\n",
       "      <th>TP_a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UnfairLR-decay</td>\n",
       "      <td>0.84887</td>\n",
       "      <td>0.803198</td>\n",
       "      <td>0.876859</td>\n",
       "      <td>0.840694</td>\n",
       "      <td>0.825402</td>\n",
       "      <td>0.862638</td>\n",
       "      <td>0.844762</td>\n",
       "      <td>4286.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6281.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>1944.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UnfairLR</td>\n",
       "      <td>0.82879</td>\n",
       "      <td>0.817964</td>\n",
       "      <td>0.914832</td>\n",
       "      <td>0.918582</td>\n",
       "      <td>0.823341</td>\n",
       "      <td>0.869688</td>\n",
       "      <td>0.871379</td>\n",
       "      <td>4175.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>6089.0</td>\n",
       "      <td>903.0</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>1908.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "result_df.to_csv('results/test_unfair_lr-100.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('falsb': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "34ca74ed6235dfc7dda926bb3adb31e801e3d02679121d5b444ee035e270bd57"
   }
  },
  "interpreter": {
   "hash": "34ca74ed6235dfc7dda926bb3adb31e801e3d02679121d5b444ee035e270bd57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}